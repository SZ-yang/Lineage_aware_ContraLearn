{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/titan_env_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import anndata as ad\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5893,) (641,)\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/Users/apple/Desktop/KB/data\"\n",
    "adata_train = ad.read_h5ad(input_dir+'/BiddyData/Biddy_train.h5ad')\n",
    "adata_test = ad.read_h5ad(input_dir+'/BiddyData/Biddy_test.h5ad')\n",
    "\n",
    "train_labels = adata_train.obs[\"clone_id\"].to_numpy()\n",
    "test_labels = adata_test.obs[\"clone_id\"].to_numpy()\n",
    "\n",
    "print(train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supUMAP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data matrix and labels\n",
    "train_data = adata_train.X\n",
    "test_data = adata_test.X\n",
    "\n",
    "# Initialize UMAP with a higher number of neighbors for supervised learning\n",
    "reducer = umap.UMAP(n_neighbors=15, n_components=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data with the labels\n",
    "X_train = reducer.fit_transform(train_data, y=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test embeddings\n",
    "X_test = reducer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5893, 10), (641, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1) Helpers: filter adata + embeddings together\n",
    "# -----------------------\n",
    "def filter_by_clone_future_size(\n",
    "    adata,\n",
    "    X,\n",
    "    day_key=\"reprogramming_day\",\n",
    "    lineage_key=\"clone_id\",\n",
    "    future_day=\"28\",\n",
    "    min_future_cells=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep all cells (all days) whose clone has >= min_future_cells at future_day (e.g., Day 28).\n",
    "    Filtering is computed within this adata only (no leakage across splits).\n",
    "    Returns filtered (adata_sub, X_sub) aligned by obs order.\n",
    "    \"\"\"\n",
    "    day = adata.obs[day_key].astype(str)\n",
    "    is_future = (day == str(future_day))\n",
    "\n",
    "    # Count future-day cells per clone\n",
    "    counts = adata.obs.loc[is_future, lineage_key].value_counts()\n",
    "\n",
    "    # Clones to keep\n",
    "    keep_clones = set(counts[counts >= int(min_future_cells)].index)\n",
    "\n",
    "    # Keep all cells from those clones (all days)\n",
    "    keep_mask = adata.obs[lineage_key].isin(keep_clones).to_numpy()\n",
    "    adata_sub = adata[keep_mask].copy()\n",
    "    X_sub = X[keep_mask]\n",
    "\n",
    "    return adata_sub, X_sub\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 2) Build Day12 inputs + lineage composition targets from Day28\n",
    "# -----------------------\n",
    "def build_targets_from_future(\n",
    "    X: np.ndarray,\n",
    "    adata,\n",
    "    early_day=\"12\",\n",
    "    future_day=\"28\",\n",
    "    lineage_key=\"clone_id\",\n",
    "    celltype_key=\"cell_type\",\n",
    "    terminal_types=(\"iEP\", \"Fibroblast\", \"Ambiguous\"),\n",
    "    alpha_smooth=1e-3,\n",
    "    drop_missing_future=True,\n",
    "):\n",
    "    terminal_types = list(terminal_types)\n",
    "    C = len(terminal_types)\n",
    "\n",
    "    # Future-day cells to compute lineage compositions\n",
    "    future_mask = (adata.obs[\"reprogramming_day\"].astype(str) == str(future_day))\n",
    "    adata_future = adata[future_mask].copy()\n",
    "\n",
    "    # clone_id -> probability vector over terminal_types\n",
    "    clone_to_probs = {}\n",
    "    for clone_id, df in adata_future.obs.groupby(lineage_key):\n",
    "        counts = np.array([(df[celltype_key] == ct).sum() for ct in terminal_types], dtype=float)\n",
    "        counts = counts + alpha_smooth\n",
    "        probs = counts / counts.sum()\n",
    "        clone_to_probs[clone_id] = probs\n",
    "\n",
    "    # Early-day cells as inputs\n",
    "    early_mask = (adata.obs[\"reprogramming_day\"].astype(str) == str(early_day))\n",
    "    early_idx = np.where(early_mask.values)[0]\n",
    "\n",
    "    X_early = X[early_idx]\n",
    "    clone_early = adata.obs.iloc[early_idx][lineage_key].to_numpy()\n",
    "\n",
    "    y_prob = np.zeros((X_early.shape[0], C), dtype=float)\n",
    "    keep = np.ones(X_early.shape[0], dtype=bool)\n",
    "\n",
    "    for i, cid in enumerate(clone_early):\n",
    "        if cid in clone_to_probs:\n",
    "            y_prob[i] = clone_to_probs[cid]\n",
    "        else:\n",
    "            # no future cells for this lineage in this split\n",
    "            if drop_missing_future:\n",
    "                keep[i] = False\n",
    "            else:\n",
    "                y_prob[i] = np.ones(C) / C\n",
    "\n",
    "    X_early = X_early[keep]\n",
    "    y_prob = y_prob[keep]\n",
    "\n",
    "    # normalize (should already be normalized)\n",
    "    y_prob = y_prob / y_prob.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return torch.tensor(X_early, dtype=torch.float32), torch.tensor(y_prob, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3) Linear decoder\n",
    "# -----------------------\n",
    "class LinearSoftmax(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # logits\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 4) Train with early stopping AND print logs like your example\n",
    "# -----------------------\n",
    "def train_kl_earlystop(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    lr=5e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=5000,\n",
    "    batch_size=256,\n",
    "    val_frac=0.2,\n",
    "    patience=150,\n",
    "    min_delta=1e-5,\n",
    "    seed=42,\n",
    "    device=None,\n",
    "    print_every=50,     # <<< prints Epoch 1 and every print_every epochs\n",
    "    verbose=True,\n",
    "):\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    # train/val split\n",
    "    n = X_train.shape[0]\n",
    "    g = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "    perm = torch.randperm(n, generator=g)\n",
    "\n",
    "    n_val = int(round(val_frac * n))\n",
    "    val_idx = perm[:n_val]\n",
    "    tr_idx = perm[n_val:]\n",
    "\n",
    "    X_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n",
    "    X_val, y_val = X_train[val_idx], y_train[val_idx]\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = -1\n",
    "    bad_epochs = 0\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"best_epoch\": None, \"best_val\": None, \"device\": device}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_loss(Xe, ye):\n",
    "        model.eval()\n",
    "        log_probs = torch.log_softmax(model(Xe), dim=1)\n",
    "        return criterion(log_probs, ye).item()\n",
    "\n",
    "    for ep in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        # shuffle train each epoch\n",
    "        perm_tr = torch.randperm(X_tr.shape[0], device=device)\n",
    "        Xs = X_tr[perm_tr]\n",
    "        ys = y_tr[perm_tr]\n",
    "\n",
    "        total = 0.0\n",
    "        for start in range(0, Xs.shape[0], batch_size):\n",
    "            xb = Xs[start:start + batch_size]\n",
    "            yb = ys[start:start + batch_size]\n",
    "\n",
    "            logits = model(xb)\n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "            loss = criterion(log_probs, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total += loss.item() * xb.shape[0]\n",
    "\n",
    "        train_loss = total / Xs.shape[0]\n",
    "        val_loss = eval_loss(X_val, y_val)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # early stopping bookkeeping\n",
    "        improved = (best_val - val_loss) > min_delta\n",
    "        if improved:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = ep\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        if verbose and (ep == 1 or ep % print_every == 0):\n",
    "            print(\n",
    "                f\"Epoch {ep}/{max_epochs} | train={train_loss:.6f} | val={val_loss:.6f} \"\n",
    "                f\"| best_val={best_val:.6f} (ep {best_epoch}) | bad={bad_epochs}/{patience} | device={device}\"\n",
    "            )\n",
    "\n",
    "        if bad_epochs >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {ep}. Best val={best_val:.6f} at epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "    # restore best\n",
    "    model.load_state_dict(best_state)\n",
    "    history[\"best_epoch\"] = best_epoch\n",
    "    history[\"best_val\"] = best_val\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 5) Evaluate KL on test (device-safe)\n",
    "# -----------------------\n",
    "@torch.no_grad()\n",
    "def eval_kl(model, X, y):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    log_probs = torch.log_softmax(model(X), dim=1)\n",
    "    return criterion(log_probs, y).item()\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 6) One-threshold experiment (replaces the sweep wrapper)\n",
    "# -----------------------\n",
    "def run_one_threshold_experiment(\n",
    "    adata_train, X_train,\n",
    "    adata_test, X_test,\n",
    "    lineage_threshold: int,\n",
    "    terminal_types=(\"iEP\", \"Fibroblast\", \"Ambiguous\"),\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    # training hyperparams\n",
    "    lr=5e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=5000,\n",
    "    batch_size=256,\n",
    "    val_frac=0.2,\n",
    "    patience=150,\n",
    "    min_delta=1e-5,\n",
    "    print_every=50,\n",
    "    alpha_smooth=1e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs ONE experiment for a single lineage threshold and prints training logs in the\n",
    "    same style as your former version.\n",
    "\n",
    "    Returns:\n",
    "      summary dict (same fields as your sweep per threshold)\n",
    "    \"\"\"\n",
    "    t = int(lineage_threshold)\n",
    "\n",
    "    # 1) filter train/test separately (no leakage)\n",
    "    ad_tr_f, X_tr_f = filter_by_clone_future_size(adata_train, X_train, min_future_cells=t)\n",
    "    ad_te_f, X_te_f = filter_by_clone_future_size(adata_test, X_test, min_future_cells=t)\n",
    "\n",
    "    # 2) build Day12 -> composition pairs\n",
    "    X_tr12, y_tr = build_targets_from_future(\n",
    "        X_tr_f, ad_tr_f,\n",
    "        terminal_types=terminal_types,\n",
    "        alpha_smooth=alpha_smooth,\n",
    "    )\n",
    "    X_te12, y_te = build_targets_from_future(\n",
    "        X_te_f, ad_te_f,\n",
    "        terminal_types=terminal_types,\n",
    "        alpha_smooth=alpha_smooth,\n",
    "    )\n",
    "\n",
    "    # 3) train linear decoder with early stopping (prints progress)\n",
    "    model = LinearSoftmax(input_size=X_tr12.shape[1], output_size=len(terminal_types))\n",
    "    model, hist = train_kl_earlystop(\n",
    "        model,\n",
    "        X_tr12, y_tr,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        max_epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        val_frac=val_frac,\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        seed=seed,\n",
    "        device=device,\n",
    "        print_every=print_every,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # 4) evaluate on test\n",
    "    kl_test = eval_kl(model, X_te12, y_te)\n",
    "\n",
    "    print(f\"Best epoch: {hist['best_epoch']}\")\n",
    "    print(f\"Test KL (linear): {kl_test:.4f}\")\n",
    "    print(f\"Training device: {hist['device']}\")\n",
    "\n",
    "    summary = {\n",
    "        \"min_future_cells_day28\": t,\n",
    "        \"train_cells_total_after_filter\": ad_tr_f.n_obs,\n",
    "        \"test_cells_total_after_filter\": ad_te_f.n_obs,\n",
    "        \"train_day12_cells_used\": X_tr12.shape[0],\n",
    "        \"test_day12_cells_used\": X_te12.shape[0],\n",
    "        \"best_epoch\": hist[\"best_epoch\"],\n",
    "        \"val_KL_best\": hist[\"best_val\"],\n",
    "        \"test_KL\": kl_test,\n",
    "        \"device\": hist[\"device\"],\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000 | train=3.333264 | val=2.596100 | best_val=2.596100 (ep 1) | bad=0/150 | device=mps\n",
      "Epoch 50/5000 | train=0.442870 | val=0.402214 | best_val=0.402214 (ep 50) | bad=0/150 | device=mps\n",
      "Epoch 100/5000 | train=0.400997 | val=0.368542 | best_val=0.368542 (ep 100) | bad=0/150 | device=mps\n",
      "Epoch 150/5000 | train=0.377902 | val=0.351586 | best_val=0.351586 (ep 150) | bad=0/150 | device=mps\n",
      "Epoch 200/5000 | train=0.364385 | val=0.341627 | best_val=0.341627 (ep 200) | bad=0/150 | device=mps\n",
      "Epoch 250/5000 | train=0.356287 | val=0.335336 | best_val=0.335336 (ep 250) | bad=0/150 | device=mps\n",
      "Epoch 300/5000 | train=0.351143 | val=0.331023 | best_val=0.331023 (ep 300) | bad=0/150 | device=mps\n",
      "Epoch 350/5000 | train=0.347586 | val=0.327770 | best_val=0.327770 (ep 350) | bad=0/150 | device=mps\n",
      "Epoch 400/5000 | train=0.344875 | val=0.325067 | best_val=0.325067 (ep 400) | bad=0/150 | device=mps\n",
      "Epoch 450/5000 | train=0.342615 | val=0.322644 | best_val=0.322644 (ep 450) | bad=0/150 | device=mps\n",
      "Epoch 500/5000 | train=0.340611 | val=0.320375 | best_val=0.320375 (ep 500) | bad=0/150 | device=mps\n",
      "Epoch 550/5000 | train=0.338772 | val=0.318215 | best_val=0.318215 (ep 550) | bad=0/150 | device=mps\n",
      "Epoch 600/5000 | train=0.337061 | val=0.316154 | best_val=0.316154 (ep 600) | bad=0/150 | device=mps\n",
      "Epoch 650/5000 | train=0.335465 | val=0.314201 | best_val=0.314201 (ep 650) | bad=0/150 | device=mps\n",
      "Epoch 700/5000 | train=0.333978 | val=0.312363 | best_val=0.312363 (ep 700) | bad=0/150 | device=mps\n",
      "Epoch 750/5000 | train=0.332596 | val=0.310645 | best_val=0.310645 (ep 750) | bad=0/150 | device=mps\n",
      "Epoch 800/5000 | train=0.331316 | val=0.309048 | best_val=0.309048 (ep 800) | bad=0/150 | device=mps\n",
      "Epoch 850/5000 | train=0.330133 | val=0.307568 | best_val=0.307568 (ep 850) | bad=0/150 | device=mps\n",
      "Epoch 900/5000 | train=0.329039 | val=0.306200 | best_val=0.306200 (ep 900) | bad=0/150 | device=mps\n",
      "Epoch 950/5000 | train=0.328030 | val=0.304936 | best_val=0.304936 (ep 950) | bad=0/150 | device=mps\n",
      "Epoch 1000/5000 | train=0.327097 | val=0.303768 | best_val=0.303768 (ep 1000) | bad=0/150 | device=mps\n",
      "Epoch 1050/5000 | train=0.326235 | val=0.302687 | best_val=0.302687 (ep 1050) | bad=0/150 | device=mps\n",
      "Epoch 1100/5000 | train=0.325437 | val=0.301686 | best_val=0.301686 (ep 1100) | bad=0/150 | device=mps\n",
      "Epoch 1150/5000 | train=0.324697 | val=0.300757 | best_val=0.300757 (ep 1150) | bad=0/150 | device=mps\n",
      "Epoch 1200/5000 | train=0.324010 | val=0.299892 | best_val=0.299892 (ep 1200) | bad=0/150 | device=mps\n",
      "Epoch 1250/5000 | train=0.323372 | val=0.299086 | best_val=0.299086 (ep 1250) | bad=0/150 | device=mps\n",
      "Epoch 1300/5000 | train=0.322777 | val=0.298332 | best_val=0.298332 (ep 1300) | bad=0/150 | device=mps\n",
      "Epoch 1350/5000 | train=0.322222 | val=0.297627 | best_val=0.297627 (ep 1350) | bad=0/150 | device=mps\n",
      "Epoch 1400/5000 | train=0.321704 | val=0.296964 | best_val=0.296964 (ep 1400) | bad=0/150 | device=mps\n",
      "Epoch 1450/5000 | train=0.321220 | val=0.296342 | best_val=0.296342 (ep 1450) | bad=0/150 | device=mps\n",
      "Epoch 1500/5000 | train=0.320766 | val=0.295756 | best_val=0.295756 (ep 1500) | bad=0/150 | device=mps\n",
      "Epoch 1550/5000 | train=0.320342 | val=0.295204 | best_val=0.295204 (ep 1550) | bad=0/150 | device=mps\n",
      "Epoch 1600/5000 | train=0.319943 | val=0.294684 | best_val=0.294684 (ep 1600) | bad=0/150 | device=mps\n",
      "Epoch 1650/5000 | train=0.319570 | val=0.294193 | best_val=0.294193 (ep 1650) | bad=0/150 | device=mps\n",
      "Epoch 1700/5000 | train=0.319219 | val=0.293729 | best_val=0.293729 (ep 1700) | bad=0/150 | device=mps\n",
      "Epoch 1750/5000 | train=0.318890 | val=0.293291 | best_val=0.293291 (ep 1750) | bad=0/150 | device=mps\n",
      "Epoch 1800/5000 | train=0.318581 | val=0.292878 | best_val=0.292878 (ep 1800) | bad=0/150 | device=mps\n",
      "Epoch 1850/5000 | train=0.318290 | val=0.292488 | best_val=0.292488 (ep 1850) | bad=0/150 | device=mps\n",
      "Epoch 1900/5000 | train=0.318016 | val=0.292120 | best_val=0.292120 (ep 1900) | bad=0/150 | device=mps\n",
      "Epoch 1950/5000 | train=0.317759 | val=0.291772 | best_val=0.291772 (ep 1950) | bad=0/150 | device=mps\n",
      "Epoch 2000/5000 | train=0.317518 | val=0.291445 | best_val=0.291445 (ep 2000) | bad=0/150 | device=mps\n",
      "Epoch 2050/5000 | train=0.317290 | val=0.291136 | best_val=0.291136 (ep 2050) | bad=0/150 | device=mps\n",
      "Epoch 2100/5000 | train=0.317075 | val=0.290844 | best_val=0.290844 (ep 2100) | bad=0/150 | device=mps\n",
      "Epoch 2150/5000 | train=0.316873 | val=0.290570 | best_val=0.290570 (ep 2150) | bad=0/150 | device=mps\n",
      "Epoch 2200/5000 | train=0.316683 | val=0.290311 | best_val=0.290311 (ep 2200) | bad=0/150 | device=mps\n",
      "Epoch 2250/5000 | train=0.316503 | val=0.290067 | best_val=0.290076 (ep 2248) | bad=2/150 | device=mps\n",
      "Epoch 2300/5000 | train=0.316333 | val=0.289837 | best_val=0.289841 (ep 2299) | bad=1/150 | device=mps\n",
      "Epoch 2350/5000 | train=0.316172 | val=0.289620 | best_val=0.289620 (ep 2350) | bad=0/150 | device=mps\n",
      "Epoch 2400/5000 | train=0.316020 | val=0.289416 | best_val=0.289424 (ep 2398) | bad=2/150 | device=mps\n",
      "Epoch 2450/5000 | train=0.315875 | val=0.289223 | best_val=0.289227 (ep 2449) | bad=1/150 | device=mps\n",
      "Epoch 2500/5000 | train=0.315739 | val=0.289042 | best_val=0.289042 (ep 2500) | bad=0/150 | device=mps\n",
      "Epoch 2550/5000 | train=0.315609 | val=0.288870 | best_val=0.288877 (ep 2548) | bad=2/150 | device=mps\n",
      "Epoch 2600/5000 | train=0.315486 | val=0.288709 | best_val=0.288709 (ep 2600) | bad=0/150 | device=mps\n",
      "Epoch 2650/5000 | train=0.315368 | val=0.288556 | best_val=0.288562 (ep 2648) | bad=2/150 | device=mps\n",
      "Epoch 2700/5000 | train=0.315256 | val=0.288411 | best_val=0.288411 (ep 2700) | bad=0/150 | device=mps\n",
      "Epoch 2750/5000 | train=0.315150 | val=0.288275 | best_val=0.288280 (ep 2748) | bad=2/150 | device=mps\n",
      "Epoch 2800/5000 | train=0.315048 | val=0.288146 | best_val=0.288146 (ep 2800) | bad=0/150 | device=mps\n",
      "Epoch 2850/5000 | train=0.314951 | val=0.288023 | best_val=0.288026 (ep 2849) | bad=1/150 | device=mps\n",
      "Epoch 2900/5000 | train=0.314858 | val=0.287907 | best_val=0.287909 (ep 2899) | bad=1/150 | device=mps\n",
      "Epoch 2950/5000 | train=0.314769 | val=0.287797 | best_val=0.287799 (ep 2949) | bad=1/150 | device=mps\n",
      "Epoch 3000/5000 | train=0.314683 | val=0.287692 | best_val=0.287695 (ep 2999) | bad=1/150 | device=mps\n",
      "Epoch 3050/5000 | train=0.314601 | val=0.287593 | best_val=0.287595 (ep 3049) | bad=1/150 | device=mps\n",
      "Epoch 3100/5000 | train=0.314522 | val=0.287499 | best_val=0.287504 (ep 3097) | bad=3/150 | device=mps\n",
      "Epoch 3150/5000 | train=0.314446 | val=0.287409 | best_val=0.287418 (ep 3145) | bad=5/150 | device=mps\n",
      "Epoch 3200/5000 | train=0.314373 | val=0.287323 | best_val=0.287325 (ep 3199) | bad=1/150 | device=mps\n",
      "Epoch 3250/5000 | train=0.314303 | val=0.287241 | best_val=0.287245 (ep 3248) | bad=2/150 | device=mps\n",
      "Epoch 3300/5000 | train=0.314235 | val=0.287164 | best_val=0.287168 (ep 3297) | bad=3/150 | device=mps\n",
      "Epoch 3350/5000 | train=0.314169 | val=0.287089 | best_val=0.287095 (ep 3346) | bad=4/150 | device=mps\n",
      "Epoch 3400/5000 | train=0.314105 | val=0.287018 | best_val=0.287021 (ep 3398) | bad=2/150 | device=mps\n",
      "Epoch 3450/5000 | train=0.314044 | val=0.286950 | best_val=0.286955 (ep 3446) | bad=4/150 | device=mps\n",
      "Epoch 3500/5000 | train=0.313984 | val=0.286885 | best_val=0.286892 (ep 3494) | bad=6/150 | device=mps\n",
      "Epoch 3550/5000 | train=0.313926 | val=0.286822 | best_val=0.286829 (ep 3544) | bad=6/150 | device=mps\n",
      "Epoch 3600/5000 | train=0.313870 | val=0.286762 | best_val=0.286764 (ep 3598) | bad=2/150 | device=mps\n",
      "Epoch 3650/5000 | train=0.313816 | val=0.286704 | best_val=0.286712 (ep 3643) | bad=7/150 | device=mps\n",
      "Epoch 3700/5000 | train=0.313762 | val=0.286649 | best_val=0.286649 (ep 3700) | bad=0/150 | device=mps\n",
      "Epoch 3750/5000 | train=0.313711 | val=0.286595 | best_val=0.286595 (ep 3750) | bad=0/150 | device=mps\n",
      "Epoch 3800/5000 | train=0.313661 | val=0.286544 | best_val=0.286544 (ep 3800) | bad=0/150 | device=mps\n",
      "Epoch 3850/5000 | train=0.313612 | val=0.286495 | best_val=0.286501 (ep 3843) | bad=7/150 | device=mps\n",
      "Epoch 3900/5000 | train=0.313564 | val=0.286447 | best_val=0.286449 (ep 3898) | bad=2/150 | device=mps\n",
      "Epoch 3950/5000 | train=0.313517 | val=0.286401 | best_val=0.286408 (ep 3942) | bad=8/150 | device=mps\n",
      "Epoch 4000/5000 | train=0.313471 | val=0.286356 | best_val=0.286366 (ep 3989) | bad=11/150 | device=mps\n",
      "Epoch 4050/5000 | train=0.313427 | val=0.286313 | best_val=0.286314 (ep 4049) | bad=1/150 | device=mps\n",
      "Epoch 4100/5000 | train=0.313383 | val=0.286272 | best_val=0.286273 (ep 4099) | bad=1/150 | device=mps\n",
      "Epoch 4150/5000 | train=0.313340 | val=0.286232 | best_val=0.286241 (ep 4138) | bad=12/150 | device=mps\n",
      "Epoch 4200/5000 | train=0.313299 | val=0.286193 | best_val=0.286200 (ep 4190) | bad=10/150 | device=mps\n",
      "Epoch 4250/5000 | train=0.313258 | val=0.286155 | best_val=0.286159 (ep 4245) | bad=5/150 | device=mps\n",
      "Epoch 4300/5000 | train=0.313217 | val=0.286118 | best_val=0.286128 (ep 4287) | bad=13/150 | device=mps\n",
      "Epoch 4350/5000 | train=0.313178 | val=0.286083 | best_val=0.286087 (ep 4344) | bad=6/150 | device=mps\n",
      "Epoch 4400/5000 | train=0.313139 | val=0.286048 | best_val=0.286056 (ep 4389) | bad=11/150 | device=mps\n",
      "Epoch 4450/5000 | train=0.313101 | val=0.286014 | best_val=0.286015 (ep 4449) | bad=1/150 | device=mps\n",
      "Epoch 4500/5000 | train=0.313064 | val=0.285982 | best_val=0.285984 (ep 4497) | bad=3/150 | device=mps\n",
      "Epoch 4550/5000 | train=0.313027 | val=0.285950 | best_val=0.285953 (ep 4545) | bad=5/150 | device=mps\n",
      "Epoch 4600/5000 | train=0.312990 | val=0.285919 | best_val=0.285922 (ep 4595) | bad=5/150 | device=mps\n",
      "Epoch 4650/5000 | train=0.312955 | val=0.285889 | best_val=0.285891 (ep 4646) | bad=4/150 | device=mps\n",
      "Epoch 4700/5000 | train=0.312920 | val=0.285859 | best_val=0.285861 (ep 4697) | bad=3/150 | device=mps\n",
      "Epoch 4750/5000 | train=0.312885 | val=0.285830 | best_val=0.285840 (ep 4733) | bad=17/150 | device=mps\n",
      "Epoch 4800/5000 | train=0.312851 | val=0.285802 | best_val=0.285809 (ep 4787) | bad=13/150 | device=mps\n",
      "Epoch 4850/5000 | train=0.312817 | val=0.285774 | best_val=0.285778 (ep 4843) | bad=7/150 | device=mps\n",
      "Epoch 4900/5000 | train=0.312784 | val=0.285748 | best_val=0.285748 (ep 4900) | bad=0/150 | device=mps\n",
      "Epoch 4950/5000 | train=0.312751 | val=0.285721 | best_val=0.285727 (ep 4939) | bad=11/150 | device=mps\n",
      "Epoch 5000/5000 | train=0.312719 | val=0.285696 | best_val=0.285696 (ep 4999) | bad=1/150 | device=mps\n",
      "Best epoch: 4999\n",
      "Test KL (linear): 0.5950\n",
      "Training device: mps\n",
      "{'min_future_cells_day28': 0, 'train_cells_total_after_filter': 5032, 'test_cells_total_after_filter': 504, 'train_day12_cells_used': 547, 'test_day12_cells_used': 46, 'best_epoch': 4999, 'val_KL_best': 0.2856960892677307, 'test_KL': 0.5950188636779785, 'device': 'mps'}\n"
     ]
    }
   ],
   "source": [
    "summary = run_one_threshold_experiment(\n",
    "    adata_train, X_train,\n",
    "    adata_test,  X_test,\n",
    "    lineage_threshold=0,\n",
    "    terminal_types=(\"iEP\", \"Fibroblast\", \"Ambiguous\"),\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    max_epochs=5000,\n",
    "    patience=150,\n",
    "    print_every=50,\n",
    ")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titan_env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
