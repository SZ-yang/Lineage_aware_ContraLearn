{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10148,) (1225,)\n",
      "(10148, 10) (1225, 10)\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/Users/apple/Desktop/KB/data\"\n",
    "adata_train = ad.read_h5ad(input_dir+'/LarryData/train_test/Larry_200_train.h5ad')\n",
    "adata_test = ad.read_h5ad(input_dir+'/LarryData/train_test/Larry_200_test.h5ad')\n",
    "\n",
    "train_labels = adata_train.obs[\"clone_id\"].to_numpy()\n",
    "test_labels = adata_test.obs[\"clone_id\"].to_numpy()\n",
    "\n",
    " \n",
    "X_train = np.load(input_dir+'/feat_LCL_2025/Larry_top200/scvi_embedding/Larry_scvi_train_200_embeddings.npy')\n",
    "X_test = np.load(input_dir+'/feat_LCL_2025/Larry_top200/scvi_embedding/Larry_scvi_test_200_embeddings.npy')\n",
    "\n",
    "print(train_labels.shape, test_labels.shape)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# (Optional) Filter to a trajectory by shared barcodes (same as your old Larry code)\n",
    "# -----------------------\n",
    "def filter_to_shared_barcodes(adata, X, keep_barcodes, barcode_key=\"Lib_Cellbarcode\"):\n",
    "    keep_mask = adata.obs[barcode_key].isin(keep_barcodes).to_numpy()\n",
    "    return adata[keep_mask].copy(), X[keep_mask]\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1) Filter by lineage size at FUTURE timepoint (Larry: time_info == 6.0)\n",
    "# -----------------------\n",
    "def filter_by_clone_future_size(\n",
    "    adata,\n",
    "    X,\n",
    "    day_key=\"time_info\",          # <<< Larry\n",
    "    lineage_key=\"clone_id\",\n",
    "    future_day=6.0,               # <<< Larry\n",
    "    min_future_cells=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep all cells (all timepoints) whose clone has >= min_future_cells at future_day (e.g., 6.0).\n",
    "    Computed within this adata only (no leakage across splits).\n",
    "    Returns filtered (adata_sub, X_sub) aligned by obs order.\n",
    "    \"\"\"\n",
    "    # IMPORTANT: time_info is numeric in Larry; compare numerically\n",
    "    is_future = (adata.obs[day_key].to_numpy() == float(future_day))\n",
    "\n",
    "    counts = adata.obs.loc[is_future, lineage_key].value_counts()\n",
    "    keep_clones = set(counts[counts >= int(min_future_cells)].index)\n",
    "\n",
    "    keep_mask = adata.obs[lineage_key].isin(keep_clones).to_numpy()\n",
    "    adata_sub = adata[keep_mask].copy()\n",
    "    X_sub = X[keep_mask]\n",
    "    return adata_sub, X_sub\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 2) Build early-time inputs + future lineage composition targets\n",
    "#    Larry: early_day=2.0, future_day=6.0, celltype_key=\"state_info\"\n",
    "# -----------------------\n",
    "def build_targets_from_future(\n",
    "    X: np.ndarray,\n",
    "    adata,\n",
    "    early_day=2.0,                 # <<< Larry\n",
    "    future_day=6.0,                # <<< Larry\n",
    "    lineage_key=\"clone_id\",\n",
    "    celltype_key=\"state_info\",     # <<< Larry\n",
    "    terminal_types=(\"Undifferentiated\", \"Monocyte\", \"Neutrophil\"),  # <<< Larry\n",
    "    alpha_smooth=1e-3,\n",
    "    drop_missing_future=True,\n",
    "    day_key=\"time_info\",           # <<< Larry\n",
    "):\n",
    "    terminal_types = list(terminal_types)\n",
    "    C = len(terminal_types)\n",
    "\n",
    "    # Future-time cells to compute lineage compositions\n",
    "    is_future = (adata.obs[day_key].to_numpy() == float(future_day))\n",
    "    adata_future = adata[is_future].copy()\n",
    "\n",
    "    # clone_id -> probability vector over terminal_types\n",
    "    clone_to_probs = {}\n",
    "    for clone_id, df in adata_future.obs.groupby(lineage_key):\n",
    "        counts = np.array([(df[celltype_key] == ct).sum() for ct in terminal_types], dtype=float)\n",
    "        counts = counts + alpha_smooth\n",
    "        probs = counts / counts.sum()\n",
    "        clone_to_probs[clone_id] = probs\n",
    "\n",
    "    # Early-time cells as inputs\n",
    "    is_early = (adata.obs[day_key].to_numpy() == float(early_day))\n",
    "    early_idx = np.where(is_early)[0]\n",
    "\n",
    "    X_early = X[early_idx]\n",
    "    clone_early = adata.obs.iloc[early_idx][lineage_key].to_numpy()\n",
    "\n",
    "    y_prob = np.zeros((X_early.shape[0], C), dtype=float)\n",
    "    keep = np.ones(X_early.shape[0], dtype=bool)\n",
    "\n",
    "    for i, cid in enumerate(clone_early):\n",
    "        if cid in clone_to_probs:\n",
    "            y_prob[i] = clone_to_probs[cid]\n",
    "        else:\n",
    "            if drop_missing_future:\n",
    "                keep[i] = False\n",
    "            else:\n",
    "                y_prob[i] = np.ones(C) / C\n",
    "\n",
    "    X_early = X_early[keep]\n",
    "    y_prob = y_prob[keep]\n",
    "    y_prob = y_prob / y_prob.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return torch.tensor(X_early, dtype=torch.float32), torch.tensor(y_prob, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3) Linear decoder\n",
    "# -----------------------\n",
    "class LinearSoftmax(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # logits\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 4) Train with early stopping + CellTag-style logs\n",
    "# -----------------------\n",
    "def train_kl_earlystop(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    lr=5e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=5000,\n",
    "    batch_size=256,\n",
    "    val_frac=0.2,\n",
    "    patience=150,\n",
    "    min_delta=1e-5,\n",
    "    seed=42,\n",
    "    device=None,\n",
    "    print_every=50,\n",
    "    verbose=True,\n",
    "):\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    n = X_train.shape[0]\n",
    "    g = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "    perm = torch.randperm(n, generator=g)\n",
    "\n",
    "    n_val = int(round(val_frac * n))\n",
    "    val_idx = perm[:n_val]\n",
    "    tr_idx = perm[n_val:]\n",
    "\n",
    "    X_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n",
    "    X_val, y_val = X_train[val_idx], y_train[val_idx]\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = -1\n",
    "    bad_epochs = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_loss(Xe, ye):\n",
    "        model.eval()\n",
    "        log_probs = torch.log_softmax(model(Xe), dim=1)\n",
    "        return criterion(log_probs, ye).item()\n",
    "\n",
    "    for ep in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        perm_tr = torch.randperm(X_tr.shape[0], device=device)\n",
    "        Xs = X_tr[perm_tr]\n",
    "        ys = y_tr[perm_tr]\n",
    "\n",
    "        total = 0.0\n",
    "        for start in range(0, Xs.shape[0], batch_size):\n",
    "            xb = Xs[start:start + batch_size]\n",
    "            yb = ys[start:start + batch_size]\n",
    "\n",
    "            logits = model(xb)\n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "            loss = criterion(log_probs, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total += loss.item() * xb.shape[0]\n",
    "\n",
    "        train_loss = total / Xs.shape[0]\n",
    "        val_loss = eval_loss(X_val, y_val)\n",
    "\n",
    "        improved = (best_val - val_loss) > min_delta\n",
    "        if improved:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = ep\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        if verbose and (ep == 1 or ep % print_every == 0):\n",
    "            print(\n",
    "                f\"Epoch {ep}/{max_epochs} | train={train_loss:.6f} | val={val_loss:.6f} \"\n",
    "                f\"| best_val={best_val:.6f} (ep {best_epoch}) | bad={bad_epochs}/{patience} | device={device}\"\n",
    "            )\n",
    "\n",
    "        if bad_epochs >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {ep}. Best val={best_val:.6f} at epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, {\"best_epoch\": best_epoch, \"best_val\": best_val, \"device\": device}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_kl(model, X, y):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    log_probs = torch.log_softmax(model(X), dim=1)\n",
    "    return criterion(log_probs, y).item()\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 5) One-threshold experiment for Larry top200\n",
    "# -----------------------\n",
    "def run_one_threshold_experiment_larry200(\n",
    "    adata_train, X_train,\n",
    "    adata_test, X_test,\n",
    "    lineage_threshold: int,\n",
    "    terminal_types=(\"Undifferentiated\", \"Monocyte\", \"Neutrophil\"),\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    # time/celltype keys for Larry\n",
    "    day_key=\"time_info\",\n",
    "    early_day=2.0,\n",
    "    future_day=6.0,\n",
    "    lineage_key=\"clone_id\",\n",
    "    celltype_key=\"state_info\",\n",
    "    # training hyperparams\n",
    "    lr=5e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=5000,\n",
    "    batch_size=256,\n",
    "    val_frac=0.2,\n",
    "    patience=150,\n",
    "    min_delta=1e-5,\n",
    "    print_every=50,\n",
    "    alpha_smooth=1e-3,\n",
    "    # optional: provide trajectory barcodes to pre-filter BOTH train and test\n",
    "    trajectory_barcodes=None,\n",
    "    barcode_key=\"Lib_Cellbarcode\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Larry top200 compositional prediction:\n",
    "      predict Day6 lineage state_info composition from Day2 embeddings.\n",
    "\n",
    "    If trajectory_barcodes is provided (set/list/np.array of Lib_Cellbarcode),\n",
    "    we pre-filter train/test to those barcodes before everything else.\n",
    "    \"\"\"\n",
    "    t = int(lineage_threshold)\n",
    "\n",
    "    # Optional trajectory filter (matches your old Larry pipeline)\n",
    "    if trajectory_barcodes is not None:\n",
    "        adata_train, X_train = filter_to_shared_barcodes(adata_train, X_train, trajectory_barcodes, barcode_key=barcode_key)\n",
    "        adata_test,  X_test  = filter_to_shared_barcodes(adata_test,  X_test,  trajectory_barcodes, barcode_key=barcode_key)\n",
    "\n",
    "    # 1) filter by future clone size (time_info == 6.0)\n",
    "    ad_tr_f, X_tr_f = filter_by_clone_future_size(\n",
    "        adata_train, X_train,\n",
    "        day_key=day_key, lineage_key=lineage_key, future_day=future_day,\n",
    "        min_future_cells=t\n",
    "    )\n",
    "    ad_te_f, X_te_f = filter_by_clone_future_size(\n",
    "        adata_test, X_test,\n",
    "        day_key=day_key, lineage_key=lineage_key, future_day=future_day,\n",
    "        min_future_cells=t\n",
    "    )\n",
    "\n",
    "    # 2) build (Day2 -> Day6 composition) pairs\n",
    "    X_tr2, y_tr = build_targets_from_future(\n",
    "        X_tr_f, ad_tr_f,\n",
    "        day_key=day_key,\n",
    "        early_day=early_day,\n",
    "        future_day=future_day,\n",
    "        lineage_key=lineage_key,\n",
    "        celltype_key=celltype_key,\n",
    "        terminal_types=terminal_types,\n",
    "        alpha_smooth=alpha_smooth,\n",
    "    )\n",
    "    X_te2, y_te = build_targets_from_future(\n",
    "        X_te_f, ad_te_f,\n",
    "        day_key=day_key,\n",
    "        early_day=early_day,\n",
    "        future_day=future_day,\n",
    "        lineage_key=lineage_key,\n",
    "        celltype_key=celltype_key,\n",
    "        terminal_types=terminal_types,\n",
    "        alpha_smooth=alpha_smooth,\n",
    "    )\n",
    "\n",
    "    # 3) train linear decoder\n",
    "    model = LinearSoftmax(input_size=X_tr2.shape[1], output_size=len(terminal_types))\n",
    "    model, hist = train_kl_earlystop(\n",
    "        model,\n",
    "        X_tr2, y_tr,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        max_epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        val_frac=val_frac,\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        seed=seed,\n",
    "        device=device,\n",
    "        print_every=print_every,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # 4) evaluate test KL\n",
    "    kl_test = eval_kl(model, X_te2, y_te)\n",
    "    print(f\"Best epoch: {hist['best_epoch']}\")\n",
    "    print(f\"Test KL (linear): {kl_test:.4f}\")\n",
    "    print(f\"Training device: {hist['device']}\")\n",
    "\n",
    "    summary = {\n",
    "        \"min_future_cells_time6\": t,\n",
    "        \"train_cells_total_after_filter\": ad_tr_f.n_obs,\n",
    "        \"test_cells_total_after_filter\": ad_te_f.n_obs,\n",
    "        \"train_time2_cells_used\": X_tr2.shape[0],\n",
    "        \"test_time2_cells_used\": X_te2.shape[0],\n",
    "        \"best_epoch\": hist[\"best_epoch\"],\n",
    "        \"val_KL_best\": hist[\"best_val\"],\n",
    "        \"test_KL\": kl_test,\n",
    "        \"device\": hist[\"device\"],\n",
    "        \"early_day\": early_day,\n",
    "        \"future_day\": future_day,\n",
    "        \"terminal_types\": terminal_types,\n",
    "    }\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000 | train=0.653260 | val=0.943850 | best_val=0.943850 (ep 1) | bad=0/150 | device=mps\n",
      "Epoch 50/5000 | train=0.377575 | val=0.536161 | best_val=0.536161 (ep 50) | bad=0/150 | device=mps\n",
      "Epoch 100/5000 | train=0.338137 | val=0.488631 | best_val=0.488631 (ep 100) | bad=0/150 | device=mps\n",
      "Epoch 150/5000 | train=0.324246 | val=0.468707 | best_val=0.468707 (ep 150) | bad=0/150 | device=mps\n",
      "Epoch 200/5000 | train=0.317175 | val=0.454790 | best_val=0.454790 (ep 200) | bad=0/150 | device=mps\n",
      "Epoch 250/5000 | train=0.312892 | val=0.444028 | best_val=0.444028 (ep 250) | bad=0/150 | device=mps\n",
      "Epoch 300/5000 | train=0.310095 | val=0.436366 | best_val=0.436366 (ep 300) | bad=0/150 | device=mps\n",
      "Epoch 350/5000 | train=0.308198 | val=0.431188 | best_val=0.431188 (ep 350) | bad=0/150 | device=mps\n",
      "Epoch 400/5000 | train=0.306853 | val=0.427773 | best_val=0.427773 (ep 400) | bad=0/150 | device=mps\n",
      "Epoch 450/5000 | train=0.305837 | val=0.425547 | best_val=0.425547 (ep 450) | bad=0/150 | device=mps\n",
      "Epoch 500/5000 | train=0.305012 | val=0.424116 | best_val=0.424116 (ep 500) | bad=0/150 | device=mps\n",
      "Epoch 550/5000 | train=0.304301 | val=0.423226 | best_val=0.423226 (ep 550) | bad=0/150 | device=mps\n",
      "Epoch 600/5000 | train=0.303658 | val=0.422715 | best_val=0.422715 (ep 600) | bad=0/150 | device=mps\n",
      "Epoch 650/5000 | train=0.303064 | val=0.422474 | best_val=0.422482 (ep 647) | bad=3/150 | device=mps\n",
      "Epoch 700/5000 | train=0.302507 | val=0.422431 | best_val=0.422427 (ep 682) | bad=18/150 | device=mps\n",
      "Epoch 750/5000 | train=0.301984 | val=0.422534 | best_val=0.422427 (ep 682) | bad=68/150 | device=mps\n",
      "Epoch 800/5000 | train=0.301494 | val=0.422747 | best_val=0.422427 (ep 682) | bad=118/150 | device=mps\n",
      "Early stopping at epoch 832. Best val=0.422427 at epoch 682.\n",
      "Best epoch: 682\n",
      "Test KL (linear): 0.5493\n",
      "Training device: mps\n",
      "{'min_future_cells_time6': 0, 'train_cells_total_after_filter': 10080, 'test_cells_total_after_filter': 1201, 'train_time2_cells_used': 93, 'test_time2_cells_used': 9, 'best_epoch': 682, 'val_KL_best': 0.42242711782455444, 'test_KL': 0.5492576956748962, 'device': 'mps', 'early_day': 2.0, 'future_day': 6.0, 'terminal_types': ('Undifferentiated', 'Monocyte', 'Neutrophil')}\n"
     ]
    }
   ],
   "source": [
    "summary = run_one_threshold_experiment_larry200(\n",
    "    adata_train, X_train,\n",
    "    adata_test,  X_test,\n",
    "    lineage_threshold=0,\n",
    "    terminal_types=(\"Undifferentiated\", \"Monocyte\", \"Neutrophil\"),\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    max_epochs=5000,\n",
    "    patience=150,\n",
    "    print_every=50,\n",
    "    trajectory_barcodes=None,   # or trajectory_barcodes\n",
    ")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titan_env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
