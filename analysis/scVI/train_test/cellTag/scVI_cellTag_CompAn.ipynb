{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5893, 2000) (641, 2000)\n",
      "(5893, 10) (641, 10)\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/Users/apple/Desktop/KB/data\"\n",
    "adata_train = ad.read_h5ad(input_dir+'/BiddyData/Biddy_train.h5ad')\n",
    "adata_test = ad.read_h5ad(input_dir+'/BiddyData/Biddy_test.h5ad')\n",
    "\n",
    "X_train = np.load(input_dir+'/feat_LCL_2025/cell_tag/scvi_embedding/Biddy_scvi_train_embeddings.npy')\n",
    "X_test = np.load(input_dir+'/feat_LCL_2025/cell_tag/scvi_embedding/Biddy_scvi_test_embeddings.npy')\n",
    "\n",
    "print(adata_train.shape, adata_test.shape)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------\n",
    "# 1) Helpers: filter adata + embeddings together\n",
    "# -----------------------\n",
    "def filter_by_clone_future_size(\n",
    "    adata,\n",
    "    X,\n",
    "    day_key=\"reprogramming_day\",\n",
    "    lineage_key=\"clone_id\",\n",
    "    future_day=\"28\",\n",
    "    min_future_cells=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Keep all cells (all days) whose clone has >= min_future_cells at future_day (e.g., Day 28).\n",
    "    Filtering is computed within this adata only (no leakage across splits).\n",
    "    Returns filtered (adata_sub, X_sub) aligned by obs order.\n",
    "    \"\"\"\n",
    "    day = adata.obs[day_key].astype(str)\n",
    "    is_future = (day == str(future_day))\n",
    "\n",
    "    # Count future-day cells per clone\n",
    "    counts = adata.obs.loc[is_future, lineage_key].value_counts()\n",
    "\n",
    "    # Clones to keep\n",
    "    keep_clones = set(counts[counts >= int(min_future_cells)].index)\n",
    "\n",
    "    # Keep all cells from those clones (all days)\n",
    "    keep_mask = adata.obs[lineage_key].isin(keep_clones).to_numpy()\n",
    "    adata_sub = adata[keep_mask].copy()\n",
    "    X_sub = X[keep_mask]\n",
    "\n",
    "    return adata_sub, X_sub\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 2) Build Day12 inputs + lineage composition targets from Day28\n",
    "# -----------------------\n",
    "def build_targets_from_future(\n",
    "    X: np.ndarray,\n",
    "    adata,\n",
    "    early_day=\"12\",\n",
    "    future_day=\"28\",\n",
    "    lineage_key=\"clone_id\",\n",
    "    celltype_key=\"cell_type\",\n",
    "    terminal_types=(\"iEP\", \"Fibroblast\", \"Ambiguous\"),\n",
    "    alpha_smooth=1e-3,\n",
    "    drop_missing_future=True,\n",
    "):\n",
    "    terminal_types = list(terminal_types)\n",
    "    C = len(terminal_types)\n",
    "\n",
    "    # Future-day cells to compute lineage compositions\n",
    "    future_mask = (adata.obs[\"reprogramming_day\"].astype(str) == str(future_day))\n",
    "    adata_future = adata[future_mask].copy()\n",
    "\n",
    "    # clone_id -> probability vector over terminal_types\n",
    "    clone_to_probs = {}\n",
    "    for clone_id, df in adata_future.obs.groupby(lineage_key):\n",
    "        counts = np.array([(df[celltype_key] == ct).sum() for ct in terminal_types], dtype=float)\n",
    "        counts = counts + alpha_smooth\n",
    "        probs = counts / counts.sum()\n",
    "        clone_to_probs[clone_id] = probs\n",
    "\n",
    "    # Early-day cells as inputs\n",
    "    early_mask = (adata.obs[\"reprogramming_day\"].astype(str) == str(early_day))\n",
    "    early_idx = np.where(early_mask.values)[0]\n",
    "\n",
    "    X_early = X[early_idx]\n",
    "    clone_early = adata.obs.iloc[early_idx][lineage_key].to_numpy()\n",
    "\n",
    "    y_prob = np.zeros((X_early.shape[0], C), dtype=float)\n",
    "    keep = np.ones(X_early.shape[0], dtype=bool)\n",
    "\n",
    "    for i, cid in enumerate(clone_early):\n",
    "        if cid in clone_to_probs:\n",
    "            y_prob[i] = clone_to_probs[cid]\n",
    "        else:\n",
    "            # no future cells for this lineage in this split\n",
    "            if drop_missing_future:\n",
    "                keep[i] = False\n",
    "            else:\n",
    "                y_prob[i] = np.ones(C) / C\n",
    "\n",
    "    X_early = X_early[keep]\n",
    "    y_prob = y_prob[keep]\n",
    "\n",
    "    # normalize (should already be normalized)\n",
    "    y_prob = y_prob / y_prob.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return torch.tensor(X_early, dtype=torch.float32), torch.tensor(y_prob, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3) Linear decoder\n",
    "# -----------------------\n",
    "class LinearSoftmax(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)  # logits\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 4) Train with early stopping AND print logs like your example\n",
    "# -----------------------\n",
    "def train_kl_earlystop(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    lr=5e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=5000,\n",
    "    batch_size=256,\n",
    "    val_frac=0.2,\n",
    "    patience=150,\n",
    "    min_delta=1e-5,\n",
    "    seed=42,\n",
    "    device=None,\n",
    "    print_every=50,     # <<< prints Epoch 1 and every print_every epochs\n",
    "    verbose=True,\n",
    "):\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    # train/val split\n",
    "    n = X_train.shape[0]\n",
    "    g = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "    perm = torch.randperm(n, generator=g)\n",
    "\n",
    "    n_val = int(round(val_frac * n))\n",
    "    val_idx = perm[:n_val]\n",
    "    tr_idx = perm[n_val:]\n",
    "\n",
    "    X_tr, y_tr = X_train[tr_idx], y_train[tr_idx]\n",
    "    X_val, y_val = X_train[val_idx], y_train[val_idx]\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = -1\n",
    "    bad_epochs = 0\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"best_epoch\": None, \"best_val\": None, \"device\": device}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_loss(Xe, ye):\n",
    "        model.eval()\n",
    "        log_probs = torch.log_softmax(model(Xe), dim=1)\n",
    "        return criterion(log_probs, ye).item()\n",
    "\n",
    "    for ep in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        # shuffle train each epoch\n",
    "        perm_tr = torch.randperm(X_tr.shape[0], device=device)\n",
    "        Xs = X_tr[perm_tr]\n",
    "        ys = y_tr[perm_tr]\n",
    "\n",
    "        total = 0.0\n",
    "        for start in range(0, Xs.shape[0], batch_size):\n",
    "            xb = Xs[start:start + batch_size]\n",
    "            yb = ys[start:start + batch_size]\n",
    "\n",
    "            logits = model(xb)\n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "            loss = criterion(log_probs, yb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total += loss.item() * xb.shape[0]\n",
    "\n",
    "        train_loss = total / Xs.shape[0]\n",
    "        val_loss = eval_loss(X_val, y_val)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # early stopping bookkeeping\n",
    "        improved = (best_val - val_loss) > min_delta\n",
    "        if improved:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = ep\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "\n",
    "        if verbose and (ep == 1 or ep % print_every == 0):\n",
    "            print(\n",
    "                f\"Epoch {ep}/{max_epochs} | train={train_loss:.6f} | val={val_loss:.6f} \"\n",
    "                f\"| best_val={best_val:.6f} (ep {best_epoch}) | bad={bad_epochs}/{patience} | device={device}\"\n",
    "            )\n",
    "\n",
    "        if bad_epochs >= patience:\n",
    "            if verbose:\n",
    "                print(f\"Early stopping at epoch {ep}. Best val={best_val:.6f} at epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "    # restore best\n",
    "    model.load_state_dict(best_state)\n",
    "    history[\"best_epoch\"] = best_epoch\n",
    "    history[\"best_val\"] = best_val\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 5) Evaluate KL on test (device-safe)\n",
    "# -----------------------\n",
    "@torch.no_grad()\n",
    "def eval_kl(model, X, y):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    log_probs = torch.log_softmax(model(X), dim=1)\n",
    "    return criterion(log_probs, y).item()\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 6) One-threshold experiment (replaces the sweep wrapper)\n",
    "# -----------------------\n",
    "def run_one_threshold_experiment(\n",
    "    adata_train, X_train,\n",
    "    adata_test, X_test,\n",
    "    lineage_threshold: int,\n",
    "    terminal_types=(\"iEP\", \"Fibroblast\", \"Ambiguous\"),\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    # training hyperparams\n",
    "    lr=5e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=5000,\n",
    "    batch_size=256,\n",
    "    val_frac=0.2,\n",
    "    patience=150,\n",
    "    min_delta=1e-5,\n",
    "    print_every=50,\n",
    "    alpha_smooth=1e-3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs ONE experiment for a single lineage threshold and prints training logs in the\n",
    "    same style as your former version.\n",
    "\n",
    "    Returns:\n",
    "      summary dict (same fields as your sweep per threshold)\n",
    "    \"\"\"\n",
    "    t = int(lineage_threshold)\n",
    "\n",
    "    # 1) filter train/test separately (no leakage)\n",
    "    ad_tr_f, X_tr_f = filter_by_clone_future_size(adata_train, X_train, min_future_cells=t)\n",
    "    ad_te_f, X_te_f = filter_by_clone_future_size(adata_test, X_test, min_future_cells=t)\n",
    "\n",
    "    # 2) build Day12 -> composition pairs\n",
    "    X_tr12, y_tr = build_targets_from_future(\n",
    "        X_tr_f, ad_tr_f,\n",
    "        terminal_types=terminal_types,\n",
    "        alpha_smooth=alpha_smooth,\n",
    "    )\n",
    "    X_te12, y_te = build_targets_from_future(\n",
    "        X_te_f, ad_te_f,\n",
    "        terminal_types=terminal_types,\n",
    "        alpha_smooth=alpha_smooth,\n",
    "    )\n",
    "\n",
    "    # 3) train linear decoder with early stopping (prints progress)\n",
    "    model = LinearSoftmax(input_size=X_tr12.shape[1], output_size=len(terminal_types))\n",
    "    model, hist = train_kl_earlystop(\n",
    "        model,\n",
    "        X_tr12, y_tr,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        max_epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        val_frac=val_frac,\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        seed=seed,\n",
    "        device=device,\n",
    "        print_every=print_every,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # 4) evaluate on test\n",
    "    kl_test = eval_kl(model, X_te12, y_te)\n",
    "\n",
    "    print(f\"Best epoch: {hist['best_epoch']}\")\n",
    "    print(f\"Test KL (linear): {kl_test:.4f}\")\n",
    "    print(f\"Training device: {hist['device']}\")\n",
    "\n",
    "    summary = {\n",
    "        \"min_future_cells_day28\": t,\n",
    "        \"train_cells_total_after_filter\": ad_tr_f.n_obs,\n",
    "        \"test_cells_total_after_filter\": ad_te_f.n_obs,\n",
    "        \"train_day12_cells_used\": X_tr12.shape[0],\n",
    "        \"test_day12_cells_used\": X_te12.shape[0],\n",
    "        \"best_epoch\": hist[\"best_epoch\"],\n",
    "        \"val_KL_best\": hist[\"best_val\"],\n",
    "        \"test_KL\": kl_test,\n",
    "        \"device\": hist[\"device\"],\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000 | train=0.686604 | val=0.645877 | best_val=0.645877 (ep 1) | bad=0/150 | device=mps\n",
      "Epoch 50/5000 | train=0.374365 | val=0.333393 | best_val=0.333393 (ep 50) | bad=0/150 | device=mps\n",
      "Epoch 100/5000 | train=0.371332 | val=0.332346 | best_val=0.332346 (ep 100) | bad=0/150 | device=mps\n",
      "Epoch 150/5000 | train=0.370356 | val=0.330777 | best_val=0.330777 (ep 150) | bad=0/150 | device=mps\n",
      "Epoch 200/5000 | train=0.369463 | val=0.329415 | best_val=0.329415 (ep 200) | bad=0/150 | device=mps\n",
      "Epoch 250/5000 | train=0.368727 | val=0.328312 | best_val=0.328312 (ep 250) | bad=0/150 | device=mps\n",
      "Epoch 300/5000 | train=0.368156 | val=0.327462 | best_val=0.327462 (ep 300) | bad=0/150 | device=mps\n",
      "Epoch 350/5000 | train=0.367732 | val=0.326831 | best_val=0.326831 (ep 350) | bad=0/150 | device=mps\n",
      "Epoch 400/5000 | train=0.367427 | val=0.326378 | best_val=0.326386 (ep 399) | bad=1/150 | device=mps\n",
      "Epoch 450/5000 | train=0.367213 | val=0.326061 | best_val=0.326066 (ep 449) | bad=1/150 | device=mps\n",
      "Epoch 500/5000 | train=0.367066 | val=0.325843 | best_val=0.325847 (ep 499) | bad=1/150 | device=mps\n",
      "Epoch 550/5000 | train=0.366967 | val=0.325696 | best_val=0.325699 (ep 549) | bad=1/150 | device=mps\n",
      "Epoch 600/5000 | train=0.366900 | val=0.325598 | best_val=0.325600 (ep 599) | bad=1/150 | device=mps\n",
      "Epoch 650/5000 | train=0.366855 | val=0.325533 | best_val=0.325536 (ep 647) | bad=3/150 | device=mps\n",
      "Epoch 700/5000 | train=0.366826 | val=0.325490 | best_val=0.325495 (ep 693) | bad=7/150 | device=mps\n",
      "Epoch 750/5000 | train=0.366806 | val=0.325461 | best_val=0.325463 (ep 745) | bad=5/150 | device=mps\n",
      "Epoch 800/5000 | train=0.366792 | val=0.325442 | best_val=0.325443 (ep 797) | bad=3/150 | device=mps\n",
      "Epoch 850/5000 | train=0.366783 | val=0.325430 | best_val=0.325433 (ep 834) | bad=16/150 | device=mps\n",
      "Epoch 900/5000 | train=0.366777 | val=0.325421 | best_val=0.325423 (ep 887) | bad=13/150 | device=mps\n",
      "Epoch 950/5000 | train=0.366773 | val=0.325415 | best_val=0.325423 (ep 887) | bad=63/150 | device=mps\n",
      "Epoch 1000/5000 | train=0.366770 | val=0.325412 | best_val=0.325413 (ep 980) | bad=20/150 | device=mps\n",
      "Epoch 1050/5000 | train=0.366768 | val=0.325409 | best_val=0.325413 (ep 980) | bad=70/150 | device=mps\n",
      "Epoch 1100/5000 | train=0.366766 | val=0.325407 | best_val=0.325413 (ep 980) | bad=120/150 | device=mps\n",
      "Early stopping at epoch 1130. Best val=0.325413 at epoch 980.\n",
      "Best epoch: 980\n",
      "Test KL (linear): 0.6273\n",
      "Training device: mps\n",
      "{'min_future_cells_day28': 0, 'train_cells_total_after_filter': 5032, 'test_cells_total_after_filter': 504, 'train_day12_cells_used': 547, 'test_day12_cells_used': 46, 'best_epoch': 980, 'val_KL_best': 0.3254129886627197, 'test_KL': 0.6272875666618347, 'device': 'mps'}\n"
     ]
    }
   ],
   "source": [
    "summary = run_one_threshold_experiment(\n",
    "    adata_train, X_train,\n",
    "    adata_test,  X_test,\n",
    "    lineage_threshold=0,\n",
    "    terminal_types=(\"iEP\", \"Fibroblast\", \"Ambiguous\"),\n",
    "    device=\"mps\",\n",
    "    seed=42,\n",
    "    max_epochs=5000,\n",
    "    patience=150,\n",
    "    print_every=50,\n",
    ")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titan_env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
