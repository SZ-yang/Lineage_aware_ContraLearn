{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8iMuyY0jX4O"
      },
      "source": [
        "**1. Change the directory to the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_zXNChaC4Tk",
        "outputId": "efa06721-44cf-4b3e-9d2d-04a754deb34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgFzoXTPCVub",
        "outputId": "4ee6a6c1-f3fb-4afe-e40d-008514a38c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/scCL/main_semi_test_new\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/scCL/main_semi_test_new')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMuQtMnjnhU"
      },
      "source": [
        "**2. Install the packages for scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rspO9sCEDsTM",
        "outputId": "82da6a11-9d24-49c2-e302-b2bca8edd396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.11)\n",
            "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.15.2 pytorch-lightning-2.6.0 torchmetrics-1.8.2\n",
            "Collecting anndata\n",
            "  Downloading anndata-0.12.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting array-api-compat>=1.7.1 (from anndata)\n",
            "  Downloading array_api_compat-1.13.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: h5py>=3.8 in /usr/local/lib/python3.12/dist-packages (from anndata) (3.15.1)\n",
            "Collecting legacy-api-wrap (from anndata)\n",
            "  Downloading legacy_api_wrap-1.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from anndata) (2.0.2)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.12/dist-packages (from anndata) (25.0)\n",
            "Requirement already satisfied: pandas!=2.1.2,<3,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from anndata) (1.16.3)\n",
            "Collecting zarr!=3.0.*,>=2.18.7 (from anndata)\n",
            "  Downloading zarr-3.1.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.2,<3,>=2.1.0->anndata) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.2,<3,>=2.1.0->anndata) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.2,<3,>=2.1.0->anndata) (2025.3)\n",
            "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata) (1.8.0)\n",
            "Collecting numcodecs>=0.14 (from zarr!=3.0.*,>=2.18.7->anndata)\n",
            "  Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.9 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata) (4.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata) (6.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.2,<3,>=2.1.0->anndata) (1.17.0)\n",
            "Downloading anndata-0.12.7-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.1.5-py3-none-any.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.5-py3-none-any.whl (10 kB)\n",
            "Downloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numcodecs, legacy-api-wrap, donfig, array-api-compat, zarr, anndata\n",
            "Successfully installed anndata-0.12.7 array-api-compat-1.13.0 donfig-0.8.1.post1 legacy-api-wrap-1.5 numcodecs-0.16.5 zarr-3.1.5\n",
            "Collecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (2.0.2)\n",
            "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (1.8.2)\n",
            "Requirement already satisfied: lightning-utilities>0.3.1 in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (0.15.2)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (0.24.0+cu126)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (2.19.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.15.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.3)\n",
            "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2025.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.1.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.10.0->lightning-bolts) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.13.3)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.11)\n",
            "Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-lightning, lightning-bolts\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.6.0\n",
            "    Uninstalling pytorch-lightning-2.6.0:\n",
            "      Successfully uninstalled pytorch-lightning-2.6.0\n",
            "Successfully installed lightning-bolts-0.7.0 pytorch-lightning-1.9.5\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.11.5-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.12.7)\n",
            "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.15.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.5.3)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.5)\n",
            "Requirement already satisfied: matplotlib>=3.7.5 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.6.1)\n",
            "Requirement already satisfied: numba!=0.62.0rc1,>=0.57.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from scanpy) (25.0)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.12/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.0.2)\n",
            "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.6.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.16.3)\n",
            "Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info2 (from scanpy)\n",
            "  Downloading session_info2-0.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.14.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from scanpy) (4.15.0)\n",
            "Requirement already satisfied: umap-learn>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.5.11)\n",
            "Requirement already satisfied: array-api-compat>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from anndata>=0.8->scanpy) (1.13.0)\n",
            "Requirement already satisfied: zarr!=3.0.*,>=2.18.7 in /usr/local/lib/python3.12/dist-packages (from anndata>=0.8->scanpy) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7.5->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba!=0.62.0rc1,>=0.57.1->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.5.3->scanpy) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.3->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.5->scanpy) (1.17.0)\n",
            "Requirement already satisfied: donfig>=0.8 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (0.8.1.post1)\n",
            "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (1.8.0)\n",
            "Requirement already satisfied: numcodecs>=0.14 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (0.16.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy) (6.0.3)\n",
            "Downloading scanpy-1.11.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading session_info2-0.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: session-info2, scanpy\n",
            "Successfully installed scanpy-1.11.5 session-info2-0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pytorch-lightning\n",
        "!pip install anndata\n",
        "!pip install lightning-bolts\n",
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_PUeICjwUp"
      },
      "source": [
        "**3. scCL manual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YXqbZBUVSPBS",
        "outputId": "7dfd8947-e968-4bdd-fe8c-9044f66346b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1769102949.5143735\n",
            "usage: LCL_Main_Semi.py [-h] [--inputFilePath INPUTFILEPATH]\n",
            "                        [--testFilePath TESTFILEPATH]\n",
            "                        [--unlabeled_per_batch UNLABELED_PER_BATCH]\n",
            "                        [--lambda_penalty LAMBDA_PENALTY]\n",
            "                        [--batch_size BATCH_SIZE] [--size_factor SIZE_FACTOR]\n",
            "                        [--temperature TEMPERATURE] [--patience PATIENCE]\n",
            "                        [--min_delta MIN_DELTA] [--max_epoch MAX_EPOCH]\n",
            "                        --output_dir OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                        [--hidden_dims HIDDEN_DIMS]\n",
            "                        [--embedding_size EMBEDDING_SIZE]\n",
            "                        [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Train Anndata for running the algorithm\n",
            "  --testFilePath TESTFILEPATH\n",
            "                        Test Anndata for running the algorithm\n",
            "  --unlabeled_per_batch UNLABELED_PER_BATCH\n",
            "                        number of unlabeled cells per batch\n",
            "  --lambda_penalty LAMBDA_PENALTY\n",
            "                        lambda_penalty for semi-supervised learning\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which\n",
            "                        training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0:\n",
            "                        otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example\n",
            "                        input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python LCL_Main_Semi.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6m8QnBnbd-"
      },
      "source": [
        "**4. Run scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3V30WCiJDs9I",
        "outputId": "63b33263-331a-4f57-f157-00f814a1ef94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1769103323.8194957\n",
            "-------------------------------INFO-------------------------------\n",
            "Anndata Info (train):  /content/drive/MyDrive/Colab Notebooks/data/Biddy_train.h5ad\n",
            "Anndata Info (test):   /content/drive/MyDrive/Colab Notebooks/data/Biddy_test.h5ad\n",
            "batch_size:  50\n",
            "size_factor:  0.04\n",
            "temperature:  0.5\n",
            "number of epochs:  220\n",
            "train_test_ratio:  0.8\n",
            "input_dim (inferred):  2000\n",
            "hidden_dims:  [1024, 256, 64]\n",
            "embedding_size:  32\n",
            "Number of batches: 27749\n",
            "Total labeled pairs: 27749 × 50 = 1387450\n",
            "Unlabeled per batch (from test set): 5\n",
            "num_workers(number of available CPU cores): 8\n",
            "Total batches: 27749\n",
            "Pos-pairs per batch: 50, unlabeled per batch: 5\n",
            "Training/Validation split:\n",
            " number of train batches: 22199, number of val batches: 5550\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/lightning_logs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/content/drive/My Drive/Colab Notebooks/scCL/main_semi_test_new/LCL_Main_Semi.py:151: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  scheduler = LinearWarmupCosineAnnealingLR(\n",
            "\n",
            "  | Name    | Type                     | Params\n",
            "-----------------------------------------------------\n",
            "0 | model   | BaseEncoder_ProjHead_MLP | 2.3 M \n",
            "1 | loss_fn | ContrastiveLoss          | 0     \n",
            "-----------------------------------------------------\n",
            "2.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.3 M     Total params\n",
            "9.348     Total estimated model params size (MB)\n",
            "2026-01-22 17:36:36.675193: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769103396.693121    6300 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769103396.698201    6300 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769103396.711804    6300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769103396.711825    6300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769103396.711828    6300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769103396.711830    6300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-22 17:36:36.715990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6300) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "  self.pid = os.fork()\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:00<00:00,  2.44it/s]/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:32: DeprecationWarning: This property will be removed in 2.0.0. Use `Metric.updated_called` instead.\n",
            "  return fn(*args, **kwargs)\n",
            "Epoch 0:  80% 22180/27749 [04:31<01:08, 81.75it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  80% 22200/27749 [04:33<01:08, 81.12it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  80% 22220/27749 [04:33<01:08, 81.17it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  80% 22240/27749 [04:33<01:07, 81.21it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  80% 22260/27749 [04:33<01:07, 81.26it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  80% 22280/27749 [04:34<01:07, 81.30it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  80% 22300/27749 [04:34<01:06, 81.35it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  80% 22320/27749 [04:34<01:06, 81.40it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22340/27749 [04:34<01:06, 81.44it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22360/27749 [04:34<01:06, 81.49it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22380/27749 [04:34<01:05, 81.53it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22400/27749 [04:34<01:05, 81.58it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22420/27749 [04:34<01:05, 81.62it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22440/27749 [04:34<01:05, 81.67it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22460/27749 [04:34<01:04, 81.72it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22480/27749 [04:34<01:04, 81.76it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22500/27749 [04:35<01:04, 81.81it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22520/27749 [04:35<01:03, 81.86it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22540/27749 [04:35<01:03, 81.90it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22560/27749 [04:35<01:03, 81.95it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22580/27749 [04:35<01:03, 82.00it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  81% 22600/27749 [04:35<01:02, 82.05it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22620/27749 [04:35<01:02, 82.09it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22640/27749 [04:35<01:02, 82.14it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22660/27749 [04:35<01:01, 82.18it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22680/27749 [04:35<01:01, 82.23it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22700/27749 [04:35<01:01, 82.28it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22720/27749 [04:35<01:01, 82.32it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22740/27749 [04:36<01:00, 82.37it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22760/27749 [04:36<01:00, 82.41it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22780/27749 [04:36<01:00, 82.46it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22800/27749 [04:36<00:59, 82.50it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22820/27749 [04:36<00:59, 82.55it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22840/27749 [04:36<00:59, 82.59it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22860/27749 [04:36<00:59, 82.64it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  82% 22880/27749 [04:36<00:58, 82.68it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 22900/27749 [04:36<00:58, 82.73it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 22920/27749 [04:36<00:58, 82.77it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 22940/27749 [04:37<00:58, 82.81it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 22960/27749 [04:37<00:57, 82.86it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 22980/27749 [04:37<00:57, 82.90it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23000/27749 [04:37<00:57, 82.95it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23020/27749 [04:37<00:56, 82.99it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23040/27749 [04:37<00:56, 83.04it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23060/27749 [04:37<00:56, 83.08it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23080/27749 [04:37<00:56, 83.13it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23100/27749 [04:37<00:55, 83.18it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23120/27749 [04:37<00:55, 83.22it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23140/27749 [04:37<00:55, 83.27it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  83% 23160/27749 [04:37<00:55, 83.31it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23180/27749 [04:38<00:54, 83.36it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23200/27749 [04:38<00:54, 83.40it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23220/27749 [04:38<00:54, 83.44it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23240/27749 [04:38<00:54, 83.49it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23260/27749 [04:38<00:53, 83.53it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23280/27749 [04:38<00:53, 83.57it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23300/27749 [04:38<00:53, 83.62it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23320/27749 [04:38<00:52, 83.66it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23340/27749 [04:38<00:52, 83.70it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23360/27749 [04:38<00:52, 83.75it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23380/27749 [04:39<00:52, 83.79it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23400/27749 [04:39<00:51, 83.84it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23420/27749 [04:39<00:51, 83.88it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  84% 23440/27749 [04:39<00:51, 83.93it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23460/27749 [04:39<00:51, 83.97it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23480/27749 [04:39<00:50, 84.01it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23500/27749 [04:39<00:50, 84.06it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23520/27749 [04:39<00:50, 84.10it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23540/27749 [04:39<00:50, 84.14it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23560/27749 [04:39<00:49, 84.18it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23580/27749 [04:39<00:49, 84.23it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23600/27749 [04:40<00:49, 84.27it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23620/27749 [04:40<00:48, 84.31it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23640/27749 [04:40<00:48, 84.35it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23660/27749 [04:40<00:48, 84.39it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23680/27749 [04:40<00:48, 84.43it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23700/27749 [04:40<00:47, 84.47it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  85% 23720/27749 [04:40<00:47, 84.51it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23740/27749 [04:40<00:47, 84.55it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23760/27749 [04:40<00:47, 84.60it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23780/27749 [04:40<00:46, 84.64it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23800/27749 [04:41<00:46, 84.68it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23820/27749 [04:41<00:46, 84.72it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23840/27749 [04:41<00:46, 84.77it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23860/27749 [04:41<00:45, 84.81it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23880/27749 [04:41<00:45, 84.85it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23900/27749 [04:41<00:45, 84.90it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23920/27749 [04:41<00:45, 84.94it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23940/27749 [04:41<00:44, 84.99it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23960/27749 [04:41<00:44, 85.03it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 23980/27749 [04:41<00:44, 85.07it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  86% 24000/27749 [04:41<00:44, 85.12it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24020/27749 [04:42<00:43, 85.16it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24040/27749 [04:42<00:43, 85.20it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24060/27749 [04:42<00:43, 85.25it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24080/27749 [04:42<00:43, 85.29it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24100/27749 [04:42<00:42, 85.34it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24120/27749 [04:42<00:42, 85.38it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24140/27749 [04:42<00:42, 85.42it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24160/27749 [04:42<00:41, 85.47it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24180/27749 [04:42<00:41, 85.51it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24200/27749 [04:42<00:41, 85.55it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24220/27749 [04:42<00:41, 85.60it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24240/27749 [04:43<00:40, 85.64it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24260/27749 [04:43<00:40, 85.68it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  87% 24280/27749 [04:43<00:40, 85.73it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24300/27749 [04:43<00:40, 85.77it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24320/27749 [04:43<00:39, 85.81it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24340/27749 [04:43<00:39, 85.85it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24360/27749 [04:43<00:39, 85.89it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24380/27749 [04:43<00:39, 85.94it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24400/27749 [04:43<00:38, 85.98it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24420/27749 [04:43<00:38, 86.02it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24440/27749 [04:43<00:38, 86.06it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24460/27749 [04:44<00:38, 86.11it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24480/27749 [04:44<00:37, 86.15it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24500/27749 [04:44<00:37, 86.19it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24520/27749 [04:44<00:37, 86.24it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  88% 24540/27749 [04:44<00:37, 86.28it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24560/27749 [04:44<00:36, 86.32it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24580/27749 [04:44<00:36, 86.36it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24600/27749 [04:44<00:36, 86.41it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24620/27749 [04:44<00:36, 86.45it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24640/27749 [04:44<00:35, 86.49it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24660/27749 [04:44<00:35, 86.54it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24680/27749 [04:45<00:35, 86.58it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24700/27749 [04:45<00:35, 86.62it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24720/27749 [04:45<00:34, 86.66it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24740/27749 [04:45<00:34, 86.71it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24760/27749 [04:45<00:34, 86.75it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24780/27749 [04:45<00:34, 86.79it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24800/27749 [04:45<00:33, 86.83it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  89% 24820/27749 [04:45<00:33, 86.88it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 24840/27749 [04:45<00:33, 86.92it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 24860/27749 [04:45<00:33, 86.96it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 24880/27749 [04:45<00:32, 87.00it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 24900/27749 [04:46<00:32, 87.04it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 24920/27749 [04:46<00:32, 87.09it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 24940/27749 [04:46<00:32, 87.13it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 24960/27749 [04:46<00:31, 87.17it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 24980/27749 [04:46<00:31, 87.21it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 25000/27749 [04:46<00:31, 87.25it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 25020/27749 [04:46<00:31, 87.30it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 25040/27749 [04:46<00:31, 87.34it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 25060/27749 [04:46<00:30, 87.38it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 25080/27749 [04:46<00:30, 87.42it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  90% 25100/27749 [04:46<00:30, 87.46it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25120/27749 [04:47<00:30, 87.50it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25140/27749 [04:47<00:29, 87.54it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25160/27749 [04:47<00:29, 87.58it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25180/27749 [04:47<00:29, 87.63it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25200/27749 [04:47<00:29, 87.67it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25220/27749 [04:47<00:28, 87.71it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25240/27749 [04:47<00:28, 87.75it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25260/27749 [04:47<00:28, 87.79it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25280/27749 [04:47<00:28, 87.83it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25300/27749 [04:47<00:27, 87.87it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25320/27749 [04:48<00:27, 87.91it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25340/27749 [04:48<00:27, 87.95it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25360/27749 [04:48<00:27, 87.99it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  91% 25380/27749 [04:48<00:26, 88.03it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25400/27749 [04:48<00:26, 88.07it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25420/27749 [04:48<00:26, 88.11it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25440/27749 [04:48<00:26, 88.16it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25460/27749 [04:48<00:25, 88.20it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25480/27749 [04:48<00:25, 88.24it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25500/27749 [04:48<00:25, 88.28it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25520/27749 [04:48<00:25, 88.32it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25540/27749 [04:49<00:25, 88.36it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25560/27749 [04:49<00:24, 88.40it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25580/27749 [04:49<00:24, 88.44it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25600/27749 [04:49<00:24, 88.48it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25620/27749 [04:49<00:24, 88.52it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25640/27749 [04:49<00:23, 88.56it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  92% 25660/27749 [04:49<00:23, 88.61it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25680/27749 [04:49<00:23, 88.65it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25700/27749 [04:49<00:23, 88.69it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25720/27749 [04:49<00:22, 88.73it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25740/27749 [04:49<00:22, 88.77it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25760/27749 [04:50<00:22, 88.81it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25780/27749 [04:50<00:22, 88.86it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25800/27749 [04:50<00:21, 88.90it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25820/27749 [04:50<00:21, 88.94it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25840/27749 [04:50<00:21, 88.98it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25860/27749 [04:50<00:21, 89.03it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25880/27749 [04:50<00:20, 89.07it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25900/27749 [04:50<00:20, 89.11it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25920/27749 [04:50<00:20, 89.15it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  93% 25940/27749 [04:50<00:20, 89.19it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 25960/27749 [04:50<00:20, 89.23it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 25980/27749 [04:51<00:19, 89.27it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26000/27749 [04:51<00:19, 89.31it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26020/27749 [04:51<00:19, 89.35it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26040/27749 [04:51<00:19, 89.39it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26060/27749 [04:51<00:18, 89.43it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26080/27749 [04:51<00:18, 89.47it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26100/27749 [04:51<00:18, 89.51it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26120/27749 [04:51<00:18, 89.55it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26140/27749 [04:51<00:17, 89.59it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26160/27749 [04:51<00:17, 89.63it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26180/27749 [04:51<00:17, 89.67it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26200/27749 [04:52<00:17, 89.71it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  94% 26220/27749 [04:52<00:17, 89.75it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26240/27749 [04:52<00:16, 89.79it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26260/27749 [04:52<00:16, 89.83it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26280/27749 [04:52<00:16, 89.87it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26300/27749 [04:52<00:16, 89.91it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26320/27749 [04:52<00:15, 89.95it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26340/27749 [04:52<00:15, 89.99it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26360/27749 [04:52<00:15, 90.03it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26380/27749 [04:52<00:15, 90.06it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26400/27749 [04:52<00:14, 90.10it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26420/27749 [04:53<00:14, 90.14it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26440/27749 [04:53<00:14, 90.18it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26460/27749 [04:53<00:14, 90.22it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26480/27749 [04:53<00:14, 90.26it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  95% 26500/27749 [04:53<00:13, 90.30it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26520/27749 [04:53<00:13, 90.34it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26540/27749 [04:53<00:13, 90.38it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26560/27749 [04:53<00:13, 90.42it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26580/27749 [04:53<00:12, 90.46it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26600/27749 [04:53<00:12, 90.50it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26620/27749 [04:54<00:12, 90.54it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26640/27749 [04:54<00:12, 90.57it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26660/27749 [04:54<00:12, 90.61it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26680/27749 [04:54<00:11, 90.65it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26700/27749 [04:54<00:11, 90.69it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26720/27749 [04:54<00:11, 90.73it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26740/27749 [04:54<00:11, 90.77it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  96% 26760/27749 [04:54<00:10, 90.81it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26780/27749 [04:54<00:10, 90.85it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26800/27749 [04:54<00:10, 90.89it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26820/27749 [04:54<00:10, 90.93it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26840/27749 [04:55<00:09, 90.96it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26860/27749 [04:55<00:09, 91.00it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26880/27749 [04:55<00:09, 91.04it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26900/27749 [04:55<00:09, 91.08it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26920/27749 [04:55<00:09, 91.12it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26940/27749 [04:55<00:08, 91.16it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26960/27749 [04:55<00:08, 91.20it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 26980/27749 [04:55<00:08, 91.24it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 27000/27749 [04:55<00:08, 91.28it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 27020/27749 [04:55<00:07, 91.32it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  97% 27040/27749 [04:55<00:07, 91.36it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27060/27749 [04:56<00:07, 91.40it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27080/27749 [04:56<00:07, 91.44it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27100/27749 [04:56<00:07, 91.48it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27120/27749 [04:56<00:06, 91.52it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27140/27749 [04:56<00:06, 91.57it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27160/27749 [04:56<00:06, 91.61it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27180/27749 [04:56<00:06, 91.65it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27200/27749 [04:56<00:05, 91.69it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27220/27749 [04:56<00:05, 91.73it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27240/27749 [04:56<00:05, 91.77it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27260/27749 [04:56<00:05, 91.81it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27280/27749 [04:57<00:05, 91.84it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27300/27749 [04:57<00:04, 91.88it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  98% 27320/27749 [04:57<00:04, 91.92it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27340/27749 [04:57<00:04, 91.96it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27360/27749 [04:57<00:04, 92.00it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27380/27749 [04:57<00:04, 92.04it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27400/27749 [04:57<00:03, 92.07it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27420/27749 [04:57<00:03, 92.11it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27440/27749 [04:57<00:03, 92.15it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27460/27749 [04:57<00:03, 92.18it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27480/27749 [04:57<00:02, 92.22it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27500/27749 [04:58<00:02, 92.26it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27520/27749 [04:58<00:02, 92.29it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27540/27749 [04:58<00:02, 92.33it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27560/27749 [04:58<00:02, 92.37it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27580/27749 [04:58<00:01, 92.41it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0:  99% 27600/27749 [04:58<00:01, 92.45it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0: 100% 27620/27749 [04:58<00:01, 92.49it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0: 100% 27640/27749 [04:58<00:01, 92.53it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0: 100% 27660/27749 [04:58<00:00, 92.57it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0: 100% 27680/27749 [04:58<00:00, 92.61it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0: 100% 27700/27749 [04:58<00:00, 92.64it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0: 100% 27720/27749 [04:59<00:00, 92.68it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0: 100% 27740/27749 [04:59<00:00, 92.72it/s, loss=4.82, v_num=0, train_loss_step=4.820]\n",
            "Epoch 0: 100% 27749/27749 [04:59<00:00, 92.74it/s, loss=4.8, v_num=0, train_loss_step=4.760, val_loss=4.610, avg_val_loss=4.610]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved. New best score: 4.609\n",
            "Epoch 1:  80% 22180/27749 [04:36<01:09, 80.17it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  80% 22200/27749 [04:39<01:09, 79.44it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  80% 22220/27749 [04:39<01:09, 79.49it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  80% 22240/27749 [04:39<01:09, 79.53it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  80% 22260/27749 [04:39<01:08, 79.57it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  80% 22280/27749 [04:39<01:08, 79.62it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  80% 22300/27749 [04:39<01:08, 79.66it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  80% 22320/27749 [04:40<01:08, 79.71it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22340/27749 [04:40<01:07, 79.75it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22360/27749 [04:40<01:07, 79.80it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22380/27749 [04:40<01:07, 79.85it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22400/27749 [04:40<01:06, 79.89it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22420/27749 [04:40<01:06, 79.94it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22440/27749 [04:40<01:06, 79.98it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22460/27749 [04:40<01:06, 80.02it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22480/27749 [04:40<01:05, 80.07it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22500/27749 [04:40<01:05, 80.11it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22520/27749 [04:40<01:05, 80.15it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22540/27749 [04:41<01:04, 80.20it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22560/27749 [04:41<01:04, 80.24it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22580/27749 [04:41<01:04, 80.28it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  81% 22600/27749 [04:41<01:04, 80.33it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22620/27749 [04:41<01:03, 80.37it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22640/27749 [04:41<01:03, 80.41it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22660/27749 [04:41<01:03, 80.46it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22680/27749 [04:41<01:02, 80.50it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22700/27749 [04:41<01:02, 80.55it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22720/27749 [04:41<01:02, 80.59it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22740/27749 [04:42<01:02, 80.64it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22760/27749 [04:42<01:01, 80.68it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22780/27749 [04:42<01:01, 80.73it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22800/27749 [04:42<01:01, 80.77it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22820/27749 [04:42<01:00, 80.82it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22840/27749 [04:42<01:00, 80.86it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22860/27749 [04:42<01:00, 80.91it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  82% 22880/27749 [04:42<01:00, 80.95it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 22900/27749 [04:42<00:59, 81.00it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 22920/27749 [04:42<00:59, 81.04it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 22940/27749 [04:42<00:59, 81.09it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 22960/27749 [04:43<00:59, 81.13it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 22980/27749 [04:43<00:58, 81.17it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23000/27749 [04:43<00:58, 81.21it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23020/27749 [04:43<00:58, 81.26it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23040/27749 [04:43<00:57, 81.30it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23060/27749 [04:43<00:57, 81.34it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23080/27749 [04:43<00:57, 81.39it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23100/27749 [04:43<00:57, 81.43it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23120/27749 [04:43<00:56, 81.48it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23140/27749 [04:43<00:56, 81.52it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  83% 23160/27749 [04:43<00:56, 81.57it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23180/27749 [04:44<00:55, 81.61it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23200/27749 [04:44<00:55, 81.66it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23220/27749 [04:44<00:55, 81.71it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23240/27749 [04:44<00:55, 81.75it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23260/27749 [04:44<00:54, 81.80it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23280/27749 [04:44<00:54, 81.84it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23300/27749 [04:44<00:54, 81.89it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23320/27749 [04:44<00:54, 81.93it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23340/27749 [04:44<00:53, 81.97it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23360/27749 [04:44<00:53, 82.02it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23380/27749 [04:44<00:53, 82.06it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23400/27749 [04:45<00:52, 82.10it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23420/27749 [04:45<00:52, 82.14it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  84% 23440/27749 [04:45<00:52, 82.19it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23460/27749 [04:45<00:52, 82.23it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23480/27749 [04:45<00:51, 82.27it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23500/27749 [04:45<00:51, 82.31it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23520/27749 [04:45<00:51, 82.36it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23540/27749 [04:45<00:51, 82.40it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23560/27749 [04:45<00:50, 82.44it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23580/27749 [04:45<00:50, 82.49it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23600/27749 [04:45<00:50, 82.53it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23620/27749 [04:46<00:50, 82.58it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23640/27749 [04:46<00:49, 82.62it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23660/27749 [04:46<00:49, 82.66it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23680/27749 [04:46<00:49, 82.70it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23700/27749 [04:46<00:48, 82.75it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  85% 23720/27749 [04:46<00:48, 82.79it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23740/27749 [04:46<00:48, 82.84it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23760/27749 [04:46<00:48, 82.88it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23780/27749 [04:46<00:47, 82.92it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23800/27749 [04:46<00:47, 82.97it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23820/27749 [04:46<00:47, 83.01it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23840/27749 [04:47<00:47, 83.05it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23860/27749 [04:47<00:46, 83.10it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23880/27749 [04:47<00:46, 83.14it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23900/27749 [04:47<00:46, 83.18it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23920/27749 [04:47<00:46, 83.22it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23940/27749 [04:47<00:45, 83.26it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23960/27749 [04:47<00:45, 83.30it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 23980/27749 [04:47<00:45, 83.34it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  86% 24000/27749 [04:47<00:44, 83.39it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24020/27749 [04:47<00:44, 83.43it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24040/27749 [04:47<00:44, 83.47it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24060/27749 [04:48<00:44, 83.52it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24080/27749 [04:48<00:43, 83.56it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24100/27749 [04:48<00:43, 83.60it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24120/27749 [04:48<00:43, 83.64it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24140/27749 [04:48<00:43, 83.69it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24160/27749 [04:48<00:42, 83.73it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24180/27749 [04:48<00:42, 83.78it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24200/27749 [04:48<00:42, 83.82it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24220/27749 [04:48<00:42, 83.86it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24240/27749 [04:48<00:41, 83.90it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24260/27749 [04:48<00:41, 83.95it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  87% 24280/27749 [04:49<00:41, 83.99it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24300/27749 [04:49<00:41, 84.03it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24320/27749 [04:49<00:40, 84.08it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24340/27749 [04:49<00:40, 84.12it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24360/27749 [04:49<00:40, 84.16it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24380/27749 [04:49<00:40, 84.21it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24400/27749 [04:49<00:39, 84.25it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24420/27749 [04:49<00:39, 84.29it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24440/27749 [04:49<00:39, 84.33it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24460/27749 [04:49<00:38, 84.37it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24480/27749 [04:50<00:38, 84.41it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24500/27749 [04:50<00:38, 84.45it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24520/27749 [04:50<00:38, 84.50it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  88% 24540/27749 [04:50<00:37, 84.54it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24560/27749 [04:50<00:37, 84.59it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24580/27749 [04:50<00:37, 84.63it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24600/27749 [04:50<00:37, 84.67it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24620/27749 [04:50<00:36, 84.71it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24640/27749 [04:50<00:36, 84.76it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24660/27749 [04:50<00:36, 84.80it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24680/27749 [04:50<00:36, 84.84it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24700/27749 [04:50<00:35, 84.89it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24720/27749 [04:51<00:35, 84.93it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24740/27749 [04:51<00:35, 84.97it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24760/27749 [04:51<00:35, 85.02it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24780/27749 [04:51<00:34, 85.06it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24800/27749 [04:51<00:34, 85.10it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  89% 24820/27749 [04:51<00:34, 85.14it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 24840/27749 [04:51<00:34, 85.19it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 24860/27749 [04:51<00:33, 85.23it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 24880/27749 [04:51<00:33, 85.27it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 24900/27749 [04:51<00:33, 85.31it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 24920/27749 [04:51<00:33, 85.35it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 24940/27749 [04:52<00:32, 85.40it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 24960/27749 [04:52<00:32, 85.44it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 24980/27749 [04:52<00:32, 85.48it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 25000/27749 [04:52<00:32, 85.52it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 25020/27749 [04:52<00:31, 85.57it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 25040/27749 [04:52<00:31, 85.61it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 25060/27749 [04:52<00:31, 85.65it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 25080/27749 [04:52<00:31, 85.70it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  90% 25100/27749 [04:52<00:30, 85.74it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25120/27749 [04:52<00:30, 85.78it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25140/27749 [04:52<00:30, 85.82it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25160/27749 [04:53<00:30, 85.86it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25180/27749 [04:53<00:29, 85.90it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25200/27749 [04:53<00:29, 85.95it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25220/27749 [04:53<00:29, 85.99it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25240/27749 [04:53<00:29, 86.03it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25260/27749 [04:53<00:28, 86.07it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25280/27749 [04:53<00:28, 86.12it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25300/27749 [04:53<00:28, 86.16it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25320/27749 [04:53<00:28, 86.20it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25340/27749 [04:53<00:27, 86.24it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25360/27749 [04:53<00:27, 86.28it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  91% 25380/27749 [04:54<00:27, 86.32it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25400/27749 [04:54<00:27, 86.36it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25420/27749 [04:54<00:26, 86.40it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25440/27749 [04:54<00:26, 86.44it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25460/27749 [04:54<00:26, 86.48it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25480/27749 [04:54<00:26, 86.52it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25500/27749 [04:54<00:25, 86.57it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25520/27749 [04:54<00:25, 86.61it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25540/27749 [04:54<00:25, 86.65it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25560/27749 [04:54<00:25, 86.69it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25580/27749 [04:54<00:25, 86.73it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25600/27749 [04:55<00:24, 86.77it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25620/27749 [04:55<00:24, 86.81it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25640/27749 [04:55<00:24, 86.85it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  92% 25660/27749 [04:55<00:24, 86.89it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25680/27749 [04:55<00:23, 86.94it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25700/27749 [04:55<00:23, 86.98it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25720/27749 [04:55<00:23, 87.02it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25740/27749 [04:55<00:23, 87.06it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25760/27749 [04:55<00:22, 87.10it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25780/27749 [04:55<00:22, 87.14it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25800/27749 [04:55<00:22, 87.18it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25820/27749 [04:56<00:22, 87.22it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25840/27749 [04:56<00:21, 87.26it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25860/27749 [04:56<00:21, 87.30it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25880/27749 [04:56<00:21, 87.34it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25900/27749 [04:56<00:21, 87.38it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25920/27749 [04:56<00:20, 87.42it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  93% 25940/27749 [04:56<00:20, 87.46it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 25960/27749 [04:56<00:20, 87.50it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 25980/27749 [04:56<00:20, 87.54it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26000/27749 [04:56<00:19, 87.58it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26020/27749 [04:56<00:19, 87.62it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26040/27749 [04:57<00:19, 87.66it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26060/27749 [04:57<00:19, 87.70it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26080/27749 [04:57<00:19, 87.74it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26100/27749 [04:57<00:18, 87.78it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26120/27749 [04:57<00:18, 87.82it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26140/27749 [04:57<00:18, 87.86it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26160/27749 [04:57<00:18, 87.90it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26180/27749 [04:57<00:17, 87.94it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26200/27749 [04:57<00:17, 87.98it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  94% 26220/27749 [04:57<00:17, 88.02it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26240/27749 [04:57<00:17, 88.06it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26260/27749 [04:58<00:16, 88.10it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26280/27749 [04:58<00:16, 88.14it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26300/27749 [04:58<00:16, 88.18it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26320/27749 [04:58<00:16, 88.22it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26340/27749 [04:58<00:15, 88.26it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26360/27749 [04:58<00:15, 88.30it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26380/27749 [04:58<00:15, 88.33it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26400/27749 [04:58<00:15, 88.37it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26420/27749 [04:58<00:15, 88.41it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26440/27749 [04:58<00:14, 88.45it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26460/27749 [04:59<00:14, 88.49it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26480/27749 [04:59<00:14, 88.53it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  95% 26500/27749 [04:59<00:14, 88.56it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26520/27749 [04:59<00:13, 88.60it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26540/27749 [04:59<00:13, 88.64it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26560/27749 [04:59<00:13, 88.68it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26580/27749 [04:59<00:13, 88.71it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26600/27749 [04:59<00:12, 88.75it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26620/27749 [04:59<00:12, 88.79it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26640/27749 [04:59<00:12, 88.83it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26660/27749 [04:59<00:12, 88.87it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26680/27749 [05:00<00:12, 88.91it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26700/27749 [05:00<00:11, 88.95it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26720/27749 [05:00<00:11, 88.99it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26740/27749 [05:00<00:11, 89.03it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  96% 26760/27749 [05:00<00:11, 89.07it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26780/27749 [05:00<00:10, 89.11it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26800/27749 [05:00<00:10, 89.15it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26820/27749 [05:00<00:10, 89.19it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26840/27749 [05:00<00:10, 89.23it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26860/27749 [05:00<00:09, 89.27it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26880/27749 [05:00<00:09, 89.31it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26900/27749 [05:01<00:09, 89.34it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26920/27749 [05:01<00:09, 89.38it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26940/27749 [05:01<00:09, 89.42it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26960/27749 [05:01<00:08, 89.46it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 26980/27749 [05:01<00:08, 89.50it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 27000/27749 [05:01<00:08, 89.53it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 27020/27749 [05:01<00:08, 89.57it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  97% 27040/27749 [05:01<00:07, 89.61it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27060/27749 [05:01<00:07, 89.65it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27080/27749 [05:01<00:07, 89.68it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27100/27749 [05:02<00:07, 89.72it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27120/27749 [05:02<00:07, 89.76it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27140/27749 [05:02<00:06, 89.80it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27160/27749 [05:02<00:06, 89.84it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27180/27749 [05:02<00:06, 89.87it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27200/27749 [05:02<00:06, 89.91it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27220/27749 [05:02<00:05, 89.95it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27240/27749 [05:02<00:05, 89.99it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27260/27749 [05:02<00:05, 90.02it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27280/27749 [05:02<00:05, 90.06it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27300/27749 [05:02<00:04, 90.10it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  98% 27320/27749 [05:03<00:04, 90.14it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27340/27749 [05:03<00:04, 90.17it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27360/27749 [05:03<00:04, 90.21it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27380/27749 [05:03<00:04, 90.25it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27400/27749 [05:03<00:03, 90.29it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27420/27749 [05:03<00:03, 90.33it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27440/27749 [05:03<00:03, 90.36it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27460/27749 [05:03<00:03, 90.40it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27480/27749 [05:03<00:02, 90.44it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27500/27749 [05:03<00:02, 90.48it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27520/27749 [05:04<00:02, 90.51it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27540/27749 [05:04<00:02, 90.55it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27560/27749 [05:04<00:02, 90.59it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27580/27749 [05:04<00:01, 90.63it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1:  99% 27600/27749 [05:04<00:01, 90.66it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1: 100% 27620/27749 [05:04<00:01, 90.70it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1: 100% 27640/27749 [05:04<00:01, 90.74it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1: 100% 27660/27749 [05:04<00:00, 90.78it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1: 100% 27680/27749 [05:04<00:00, 90.82it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1: 100% 27700/27749 [05:04<00:00, 90.86it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1: 100% 27720/27749 [05:04<00:00, 90.89it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1: 100% 27740/27749 [05:05<00:00, 90.93it/s, loss=2.95, v_num=0, train_loss_step=2.950, val_loss=4.610, avg_val_loss=4.610, train_loss_epoch=4.810]\n",
            "Epoch 1: 100% 27749/27749 [05:05<00:00, 90.95it/s, loss=2.96, v_num=0, train_loss_step=2.970, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=4.810]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 1.810 >= min_delta = 0.001. New best score: 2.799\n",
            "Epoch 2:  80% 22180/27749 [04:36<01:09, 80.12it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  80% 22200/27749 [04:39<01:09, 79.30it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  80% 22220/27749 [04:40<01:09, 79.35it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  80% 22240/27749 [04:40<01:09, 79.39it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  80% 22260/27749 [04:40<01:09, 79.44it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  80% 22280/27749 [04:40<01:08, 79.48it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  80% 22300/27749 [04:40<01:08, 79.53it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  80% 22320/27749 [04:40<01:08, 79.57it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22340/27749 [04:40<01:07, 79.62it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22360/27749 [04:40<01:07, 79.66it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22380/27749 [04:40<01:07, 79.71it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22400/27749 [04:40<01:07, 79.75it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22420/27749 [04:40<01:06, 79.79it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22440/27749 [04:41<01:06, 79.84it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22460/27749 [04:41<01:06, 79.89it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22480/27749 [04:41<01:05, 79.93it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22500/27749 [04:41<01:05, 79.97it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22520/27749 [04:41<01:05, 80.02it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22540/27749 [04:41<01:05, 80.06it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22560/27749 [04:41<01:04, 80.11it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22580/27749 [04:41<01:04, 80.15it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  81% 22600/27749 [04:41<01:04, 80.19it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22620/27749 [04:41<01:03, 80.24it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22640/27749 [04:42<01:03, 80.28it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22660/27749 [04:42<01:03, 80.33it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22680/27749 [04:42<01:03, 80.37it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22700/27749 [04:42<01:02, 80.41it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22720/27749 [04:42<01:02, 80.46it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22740/27749 [04:42<01:02, 80.50it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22760/27749 [04:42<01:01, 80.54it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22780/27749 [04:42<01:01, 80.59it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22800/27749 [04:42<01:01, 80.63it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22820/27749 [04:42<01:01, 80.68it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22840/27749 [04:42<01:00, 80.72it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22860/27749 [04:43<01:00, 80.77it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  82% 22880/27749 [04:43<01:00, 80.81it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 22900/27749 [04:43<00:59, 80.85it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 22920/27749 [04:43<00:59, 80.90it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 22940/27749 [04:43<00:59, 80.95it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 22960/27749 [04:43<00:59, 80.99it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 22980/27749 [04:43<00:58, 81.04it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23000/27749 [04:43<00:58, 81.08it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23020/27749 [04:43<00:58, 81.13it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23040/27749 [04:43<00:58, 81.17it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23060/27749 [04:43<00:57, 81.21it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23080/27749 [04:44<00:57, 81.26it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23100/27749 [04:44<00:57, 81.30it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23120/27749 [04:44<00:56, 81.35it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23140/27749 [04:44<00:56, 81.39it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  83% 23160/27749 [04:44<00:56, 81.44it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23180/27749 [04:44<00:56, 81.48it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23200/27749 [04:44<00:55, 81.53it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23220/27749 [04:44<00:55, 81.57it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23240/27749 [04:44<00:55, 81.62it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23260/27749 [04:44<00:54, 81.66it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23280/27749 [04:44<00:54, 81.70it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23300/27749 [04:45<00:54, 81.75it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23320/27749 [04:45<00:54, 81.79it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23340/27749 [04:45<00:53, 81.83it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23360/27749 [04:45<00:53, 81.87it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23380/27749 [04:45<00:53, 81.91it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23400/27749 [04:45<00:53, 81.95it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23420/27749 [04:45<00:52, 82.00it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  84% 23440/27749 [04:45<00:52, 82.04it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23460/27749 [04:45<00:52, 82.08it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23480/27749 [04:45<00:51, 82.13it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23500/27749 [04:45<00:51, 82.17it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23520/27749 [04:46<00:51, 82.21it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23540/27749 [04:46<00:51, 82.25it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23560/27749 [04:46<00:50, 82.30it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23580/27749 [04:46<00:50, 82.34it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23600/27749 [04:46<00:50, 82.39it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23620/27749 [04:46<00:50, 82.43it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23640/27749 [04:46<00:49, 82.47it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23660/27749 [04:46<00:49, 82.51it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23680/27749 [04:46<00:49, 82.55it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23700/27749 [04:46<00:49, 82.59it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  85% 23720/27749 [04:47<00:48, 82.63it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23740/27749 [04:47<00:48, 82.67it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23760/27749 [04:47<00:48, 82.72it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23780/27749 [04:47<00:47, 82.76it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23800/27749 [04:47<00:47, 82.80it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23820/27749 [04:47<00:47, 82.84it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23840/27749 [04:47<00:47, 82.88it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23860/27749 [04:47<00:46, 82.92it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23880/27749 [04:47<00:46, 82.96it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23900/27749 [04:47<00:46, 83.00it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23920/27749 [04:48<00:46, 83.04it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23940/27749 [04:48<00:45, 83.08it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23960/27749 [04:48<00:45, 83.12it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 23980/27749 [04:48<00:45, 83.17it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  86% 24000/27749 [04:48<00:45, 83.21it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24020/27749 [04:48<00:44, 83.25it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24040/27749 [04:48<00:44, 83.29it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24060/27749 [04:48<00:44, 83.34it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24080/27749 [04:48<00:44, 83.38it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24100/27749 [04:48<00:43, 83.42it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24120/27749 [04:48<00:43, 83.46it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24140/27749 [04:49<00:43, 83.51it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24160/27749 [04:49<00:42, 83.55it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24180/27749 [04:49<00:42, 83.59it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24200/27749 [04:49<00:42, 83.63it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24220/27749 [04:49<00:42, 83.68it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24240/27749 [04:49<00:41, 83.72it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24260/27749 [04:49<00:41, 83.76it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  87% 24280/27749 [04:49<00:41, 83.80it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24300/27749 [04:49<00:41, 83.85it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24320/27749 [04:49<00:40, 83.89it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24340/27749 [04:50<00:40, 83.93it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24360/27749 [04:50<00:40, 83.97it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24380/27749 [04:50<00:40, 84.02it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24400/27749 [04:50<00:39, 84.06it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24420/27749 [04:50<00:39, 84.10it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24440/27749 [04:50<00:39, 84.14it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24460/27749 [04:50<00:39, 84.18it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24480/27749 [04:50<00:38, 84.22it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24500/27749 [04:50<00:38, 84.26it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24520/27749 [04:50<00:38, 84.31it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  88% 24540/27749 [04:50<00:38, 84.35it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24560/27749 [04:51<00:37, 84.39it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24580/27749 [04:51<00:37, 84.43it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24600/27749 [04:51<00:37, 84.48it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24620/27749 [04:51<00:37, 84.52it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24640/27749 [04:51<00:36, 84.56it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24660/27749 [04:51<00:36, 84.61it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24680/27749 [04:51<00:36, 84.65it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24700/27749 [04:51<00:36, 84.69it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24720/27749 [04:51<00:35, 84.73it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24740/27749 [04:51<00:35, 84.77it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24760/27749 [04:51<00:35, 84.82it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24780/27749 [04:52<00:34, 84.86it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24800/27749 [04:52<00:34, 84.90it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  89% 24820/27749 [04:52<00:34, 84.95it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 24840/27749 [04:52<00:34, 84.99it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 24860/27749 [04:52<00:33, 85.03it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 24880/27749 [04:52<00:33, 85.08it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 24900/27749 [04:52<00:33, 85.12it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 24920/27749 [04:52<00:33, 85.16it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 24940/27749 [04:52<00:32, 85.20it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 24960/27749 [04:52<00:32, 85.24it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 24980/27749 [04:52<00:32, 85.29it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 25000/27749 [04:52<00:32, 85.33it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 25020/27749 [04:53<00:31, 85.37it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 25040/27749 [04:53<00:31, 85.41it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 25060/27749 [04:53<00:31, 85.45it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 25080/27749 [04:53<00:31, 85.49it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  90% 25100/27749 [04:53<00:30, 85.53it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25120/27749 [04:53<00:30, 85.58it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25140/27749 [04:53<00:30, 85.62it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25160/27749 [04:53<00:30, 85.66it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25180/27749 [04:53<00:29, 85.70it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25200/27749 [04:53<00:29, 85.74it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25220/27749 [04:53<00:29, 85.78it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25240/27749 [04:54<00:29, 85.83it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25260/27749 [04:54<00:28, 85.87it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25280/27749 [04:54<00:28, 85.91it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25300/27749 [04:54<00:28, 85.95it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25320/27749 [04:54<00:28, 85.99it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25340/27749 [04:54<00:28, 86.03it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25360/27749 [04:54<00:27, 86.07it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  91% 25380/27749 [04:54<00:27, 86.12it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25400/27749 [04:54<00:27, 86.16it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25420/27749 [04:54<00:27, 86.20it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25440/27749 [04:54<00:26, 86.24it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25460/27749 [04:55<00:26, 86.28it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25480/27749 [04:55<00:26, 86.32it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25500/27749 [04:55<00:26, 86.37it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25520/27749 [04:55<00:25, 86.41it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25540/27749 [04:55<00:25, 86.45it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25560/27749 [04:55<00:25, 86.49it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25580/27749 [04:55<00:25, 86.53it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25600/27749 [04:55<00:24, 86.57it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25620/27749 [04:55<00:24, 86.61it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25640/27749 [04:55<00:24, 86.65it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  92% 25660/27749 [04:56<00:24, 86.69it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25680/27749 [04:56<00:23, 86.73it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25700/27749 [04:56<00:23, 86.77it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25720/27749 [04:56<00:23, 86.81it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25740/27749 [04:56<00:23, 86.85it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25760/27749 [04:56<00:22, 86.89it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25780/27749 [04:56<00:22, 86.93it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25800/27749 [04:56<00:22, 86.97it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25820/27749 [04:56<00:22, 87.01it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25840/27749 [04:56<00:21, 87.05it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25860/27749 [04:56<00:21, 87.09it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25880/27749 [04:57<00:21, 87.13it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25900/27749 [04:57<00:21, 87.17it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25920/27749 [04:57<00:20, 87.21it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  93% 25940/27749 [04:57<00:20, 87.24it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 25960/27749 [04:57<00:20, 87.28it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 25980/27749 [04:57<00:20, 87.32it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26000/27749 [04:57<00:20, 87.36it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26020/27749 [04:57<00:19, 87.40it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26040/27749 [04:57<00:19, 87.44it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26060/27749 [04:57<00:19, 87.48it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26080/27749 [04:57<00:19, 87.53it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26100/27749 [04:58<00:18, 87.57it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26120/27749 [04:58<00:18, 87.61it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26140/27749 [04:58<00:18, 87.65it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26160/27749 [04:58<00:18, 87.69it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26180/27749 [04:58<00:17, 87.73it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26200/27749 [04:58<00:17, 87.77it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  94% 26220/27749 [04:58<00:17, 87.81it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26240/27749 [04:58<00:17, 87.85it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26260/27749 [04:58<00:16, 87.89it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26280/27749 [04:58<00:16, 87.92it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26300/27749 [04:58<00:16, 87.96it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26320/27749 [04:59<00:16, 88.00it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26340/27749 [04:59<00:16, 88.04it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26360/27749 [04:59<00:15, 88.08it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26380/27749 [04:59<00:15, 88.12it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26400/27749 [04:59<00:15, 88.16it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26420/27749 [04:59<00:15, 88.20it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26440/27749 [04:59<00:14, 88.24it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26460/27749 [04:59<00:14, 88.28it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26480/27749 [04:59<00:14, 88.31it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  95% 26500/27749 [04:59<00:14, 88.35it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26520/27749 [05:00<00:13, 88.39it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26540/27749 [05:00<00:13, 88.43it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26560/27749 [05:00<00:13, 88.46it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26580/27749 [05:00<00:13, 88.50it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26600/27749 [05:00<00:12, 88.54it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26620/27749 [05:00<00:12, 88.57it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26640/27749 [05:00<00:12, 88.61it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26660/27749 [05:00<00:12, 88.64it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26680/27749 [05:00<00:12, 88.68it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26700/27749 [05:00<00:11, 88.72it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26720/27749 [05:01<00:11, 88.75it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26740/27749 [05:01<00:11, 88.79it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  96% 26760/27749 [05:01<00:11, 88.83it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26780/27749 [05:01<00:10, 88.87it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26800/27749 [05:01<00:10, 88.90it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26820/27749 [05:01<00:10, 88.94it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26840/27749 [05:01<00:10, 88.98it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26860/27749 [05:01<00:09, 89.02it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26880/27749 [05:01<00:09, 89.05it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26900/27749 [05:01<00:09, 89.09it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26920/27749 [05:02<00:09, 89.13it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26940/27749 [05:02<00:09, 89.17it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26960/27749 [05:02<00:08, 89.21it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 26980/27749 [05:02<00:08, 89.25it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 27000/27749 [05:02<00:08, 89.29it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 27020/27749 [05:02<00:08, 89.33it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  97% 27040/27749 [05:02<00:07, 89.37it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27060/27749 [05:02<00:07, 89.41it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27080/27749 [05:02<00:07, 89.45it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27100/27749 [05:02<00:07, 89.49it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27120/27749 [05:02<00:07, 89.53it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27140/27749 [05:02<00:06, 89.57it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27160/27749 [05:03<00:06, 89.61it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27180/27749 [05:03<00:06, 89.65it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27200/27749 [05:03<00:06, 89.69it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27220/27749 [05:03<00:05, 89.73it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27240/27749 [05:03<00:05, 89.77it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27260/27749 [05:03<00:05, 89.81it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27280/27749 [05:03<00:05, 89.85it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27300/27749 [05:03<00:04, 89.89it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  98% 27320/27749 [05:03<00:04, 89.93it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27340/27749 [05:03<00:04, 89.96it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27360/27749 [05:03<00:04, 90.00it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27380/27749 [05:04<00:04, 90.04it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27400/27749 [05:04<00:03, 90.08it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27420/27749 [05:04<00:03, 90.12it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27440/27749 [05:04<00:03, 90.16it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27460/27749 [05:04<00:03, 90.20it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27480/27749 [05:04<00:02, 90.24it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27500/27749 [05:04<00:02, 90.27it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27520/27749 [05:04<00:02, 90.31it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27540/27749 [05:04<00:02, 90.35it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27560/27749 [05:04<00:02, 90.39it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27580/27749 [05:04<00:01, 90.43it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2:  99% 27600/27749 [05:05<00:01, 90.47it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2: 100% 27620/27749 [05:05<00:01, 90.51it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2: 100% 27640/27749 [05:05<00:01, 90.54it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2: 100% 27660/27749 [05:05<00:00, 90.58it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2: 100% 27680/27749 [05:05<00:00, 90.62it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2: 100% 27700/27749 [05:05<00:00, 90.66it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2: 100% 27720/27749 [05:05<00:00, 90.69it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2: 100% 27740/27749 [05:05<00:00, 90.73it/s, loss=2.9, v_num=0, train_loss_step=2.900, val_loss=2.800, avg_val_loss=2.800, train_loss_epoch=3.090]\n",
            "Epoch 2: 100% 27749/27749 [05:05<00:00, 90.74it/s, loss=2.9, v_num=0, train_loss_step=2.910, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=3.090]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.076 >= min_delta = 0.001. New best score: 2.722\n",
            "Epoch 3:  80% 22180/27749 [04:36<01:09, 80.25it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  80% 22200/27749 [04:39<01:09, 79.37it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  80% 22220/27749 [04:39<01:09, 79.41it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  80% 22240/27749 [04:39<01:09, 79.46it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  80% 22260/27749 [04:39<01:09, 79.50it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  80% 22280/27749 [04:40<01:08, 79.54it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  80% 22300/27749 [04:40<01:08, 79.59it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  80% 22320/27749 [04:40<01:08, 79.63it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22340/27749 [04:40<01:07, 79.68it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22360/27749 [04:40<01:07, 79.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22380/27749 [04:40<01:07, 79.77it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22400/27749 [04:40<01:07, 79.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22420/27749 [04:40<01:06, 79.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22440/27749 [04:40<01:06, 79.90it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22460/27749 [04:40<01:06, 79.94it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22480/27749 [04:41<01:05, 79.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22500/27749 [04:41<01:05, 80.04it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22520/27749 [04:41<01:05, 80.08it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22540/27749 [04:41<01:05, 80.12it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22560/27749 [04:41<01:04, 80.17it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22580/27749 [04:41<01:04, 80.21it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  81% 22600/27749 [04:41<01:04, 80.25it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22620/27749 [04:41<01:03, 80.30it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22640/27749 [04:41<01:03, 80.34it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22660/27749 [04:41<01:03, 80.39it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22680/27749 [04:41<01:03, 80.43it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22700/27749 [04:42<01:02, 80.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22720/27749 [04:42<01:02, 80.52it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22740/27749 [04:42<01:02, 80.56it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22760/27749 [04:42<01:01, 80.61it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22780/27749 [04:42<01:01, 80.65it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22800/27749 [04:42<01:01, 80.70it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22820/27749 [04:42<01:01, 80.74it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22840/27749 [04:42<01:00, 80.78it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22860/27749 [04:42<01:00, 80.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  82% 22880/27749 [04:42<01:00, 80.88it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 22900/27749 [04:42<00:59, 80.92it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 22920/27749 [04:43<00:59, 80.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 22940/27749 [04:43<00:59, 81.01it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 22960/27749 [04:43<00:59, 81.05it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 22980/27749 [04:43<00:58, 81.10it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23000/27749 [04:43<00:58, 81.14it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23020/27749 [04:43<00:58, 81.18it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23040/27749 [04:43<00:57, 81.22it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23060/27749 [04:43<00:57, 81.27it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23080/27749 [04:43<00:57, 81.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23100/27749 [04:43<00:57, 81.35it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23120/27749 [04:44<00:56, 81.39it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23140/27749 [04:44<00:56, 81.44it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  83% 23160/27749 [04:44<00:56, 81.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23180/27749 [04:44<00:56, 81.53it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23200/27749 [04:44<00:55, 81.57it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23220/27749 [04:44<00:55, 81.62it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23240/27749 [04:44<00:55, 81.66it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23260/27749 [04:44<00:54, 81.71it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23280/27749 [04:44<00:54, 81.75it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23300/27749 [04:44<00:54, 81.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23320/27749 [04:44<00:54, 81.84it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23340/27749 [04:45<00:53, 81.88it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23360/27749 [04:45<00:53, 81.93it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23380/27749 [04:45<00:53, 81.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23400/27749 [04:45<00:53, 82.01it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23420/27749 [04:45<00:52, 82.06it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  84% 23440/27749 [04:45<00:52, 82.10it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23460/27749 [04:45<00:52, 82.14it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23480/27749 [04:45<00:51, 82.19it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23500/27749 [04:45<00:51, 82.23it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23520/27749 [04:45<00:51, 82.27it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23540/27749 [04:45<00:51, 82.32it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23560/27749 [04:46<00:50, 82.36it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23580/27749 [04:46<00:50, 82.40it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23600/27749 [04:46<00:50, 82.45it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23620/27749 [04:46<00:50, 82.49it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23640/27749 [04:46<00:49, 82.53it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23660/27749 [04:46<00:49, 82.58it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23680/27749 [04:46<00:49, 82.62it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23700/27749 [04:46<00:48, 82.66it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  85% 23720/27749 [04:46<00:48, 82.71it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23740/27749 [04:46<00:48, 82.75it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23760/27749 [04:46<00:48, 82.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23780/27749 [04:47<00:47, 82.84it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23800/27749 [04:47<00:47, 82.88it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23820/27749 [04:47<00:47, 82.92it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23840/27749 [04:47<00:47, 82.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23860/27749 [04:47<00:46, 83.01it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23880/27749 [04:47<00:46, 83.05it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23900/27749 [04:47<00:46, 83.09it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23920/27749 [04:47<00:46, 83.14it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23940/27749 [04:47<00:45, 83.18it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23960/27749 [04:47<00:45, 83.22it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 23980/27749 [04:48<00:45, 83.26it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  86% 24000/27749 [04:48<00:45, 83.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24020/27749 [04:48<00:44, 83.35it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24040/27749 [04:48<00:44, 83.39it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24060/27749 [04:48<00:44, 83.44it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24080/27749 [04:48<00:43, 83.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24100/27749 [04:48<00:43, 83.53it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24120/27749 [04:48<00:43, 83.57it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24140/27749 [04:48<00:43, 83.61it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24160/27749 [04:48<00:42, 83.66it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24180/27749 [04:48<00:42, 83.70it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24200/27749 [04:48<00:42, 83.74it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24220/27749 [04:49<00:42, 83.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24240/27749 [04:49<00:41, 83.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24260/27749 [04:49<00:41, 83.87it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  87% 24280/27749 [04:49<00:41, 83.91it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24300/27749 [04:49<00:41, 83.96it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24320/27749 [04:49<00:40, 84.00it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24340/27749 [04:49<00:40, 84.04it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24360/27749 [04:49<00:40, 84.08it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24380/27749 [04:49<00:40, 84.12it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24400/27749 [04:49<00:39, 84.17it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24420/27749 [04:49<00:39, 84.21it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24440/27749 [04:50<00:39, 84.25it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24460/27749 [04:50<00:39, 84.30it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24480/27749 [04:50<00:38, 84.34it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24500/27749 [04:50<00:38, 84.38it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24520/27749 [04:50<00:38, 84.43it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  88% 24540/27749 [04:50<00:37, 84.47it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24560/27749 [04:50<00:37, 84.51it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24580/27749 [04:50<00:37, 84.55it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24600/27749 [04:50<00:37, 84.59it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24620/27749 [04:50<00:36, 84.64it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24640/27749 [04:50<00:36, 84.68it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24660/27749 [04:51<00:36, 84.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24680/27749 [04:51<00:36, 84.77it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24700/27749 [04:51<00:35, 84.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24720/27749 [04:51<00:35, 84.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24740/27749 [04:51<00:35, 84.89it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24760/27749 [04:51<00:35, 84.93it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24780/27749 [04:51<00:34, 84.98it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24800/27749 [04:51<00:34, 85.02it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  89% 24820/27749 [04:51<00:34, 85.06it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 24840/27749 [04:51<00:34, 85.11it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 24860/27749 [04:51<00:33, 85.15it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 24880/27749 [04:52<00:33, 85.19it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 24900/27749 [04:52<00:33, 85.23it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 24920/27749 [04:52<00:33, 85.27it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 24940/27749 [04:52<00:32, 85.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 24960/27749 [04:52<00:32, 85.36it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 24980/27749 [04:52<00:32, 85.40it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 25000/27749 [04:52<00:32, 85.44it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 25020/27749 [04:52<00:31, 85.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 25040/27749 [04:52<00:31, 85.53it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 25060/27749 [04:52<00:31, 85.57it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 25080/27749 [04:52<00:31, 85.61it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  90% 25100/27749 [04:53<00:30, 85.65it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25120/27749 [04:53<00:30, 85.69it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25140/27749 [04:53<00:30, 85.73it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25160/27749 [04:53<00:30, 85.77it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25180/27749 [04:53<00:29, 85.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25200/27749 [04:53<00:29, 85.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25220/27749 [04:53<00:29, 85.89it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25240/27749 [04:53<00:29, 85.93it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25260/27749 [04:53<00:28, 85.98it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25280/27749 [04:53<00:28, 86.02it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25300/27749 [04:53<00:28, 86.06it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25320/27749 [04:54<00:28, 86.10it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25340/27749 [04:54<00:27, 86.14it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25360/27749 [04:54<00:27, 86.18it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  91% 25380/27749 [04:54<00:27, 86.23it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25400/27749 [04:54<00:27, 86.27it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25420/27749 [04:54<00:26, 86.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25440/27749 [04:54<00:26, 86.35it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25460/27749 [04:54<00:26, 86.38it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25480/27749 [04:54<00:26, 86.42it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25500/27749 [04:54<00:26, 86.46it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25520/27749 [04:55<00:25, 86.51it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25540/27749 [04:55<00:25, 86.55it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25560/27749 [04:55<00:25, 86.59it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25580/27749 [04:55<00:25, 86.63it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25600/27749 [04:55<00:24, 86.67it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25620/27749 [04:55<00:24, 86.71it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25640/27749 [04:55<00:24, 86.75it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  92% 25660/27749 [04:55<00:24, 86.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25680/27749 [04:55<00:23, 86.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25700/27749 [04:55<00:23, 86.87it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25720/27749 [04:55<00:23, 86.91it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25740/27749 [04:56<00:23, 86.95it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25760/27749 [04:56<00:22, 86.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25780/27749 [04:56<00:22, 87.04it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25800/27749 [04:56<00:22, 87.08it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25820/27749 [04:56<00:22, 87.12it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25840/27749 [04:56<00:21, 87.16it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25860/27749 [04:56<00:21, 87.20it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25880/27749 [04:56<00:21, 87.24it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25900/27749 [04:56<00:21, 87.28it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25920/27749 [04:56<00:20, 87.32it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  93% 25940/27749 [04:56<00:20, 87.36it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 25960/27749 [04:57<00:20, 87.40it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 25980/27749 [04:57<00:20, 87.44it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26000/27749 [04:57<00:19, 87.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26020/27749 [04:57<00:19, 87.52it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26040/27749 [04:57<00:19, 87.56it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26060/27749 [04:57<00:19, 87.60it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26080/27749 [04:57<00:19, 87.64it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26100/27749 [04:57<00:18, 87.68it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26120/27749 [04:57<00:18, 87.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26140/27749 [04:57<00:18, 87.76it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26160/27749 [04:57<00:18, 87.80it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26180/27749 [04:58<00:17, 87.84it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26200/27749 [04:58<00:17, 87.88it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  94% 26220/27749 [04:58<00:17, 87.92it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26240/27749 [04:58<00:17, 87.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26260/27749 [04:58<00:16, 88.01it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26280/27749 [04:58<00:16, 88.05it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26300/27749 [04:58<00:16, 88.09it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26320/27749 [04:58<00:16, 88.13it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26340/27749 [04:58<00:15, 88.17it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26360/27749 [04:58<00:15, 88.21it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26380/27749 [04:58<00:15, 88.25it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26400/27749 [04:59<00:15, 88.29it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26420/27749 [04:59<00:15, 88.33it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26440/27749 [04:59<00:14, 88.37it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26460/27749 [04:59<00:14, 88.41it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26480/27749 [04:59<00:14, 88.45it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  95% 26500/27749 [04:59<00:14, 88.49it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26520/27749 [04:59<00:13, 88.53it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26540/27749 [04:59<00:13, 88.56it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26560/27749 [04:59<00:13, 88.60it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26580/27749 [04:59<00:13, 88.64it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26600/27749 [04:59<00:12, 88.68it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26620/27749 [05:00<00:12, 88.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26640/27749 [05:00<00:12, 88.75it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26660/27749 [05:00<00:12, 88.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26680/27749 [05:00<00:12, 88.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26700/27749 [05:00<00:11, 88.87it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26720/27749 [05:00<00:11, 88.91it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26740/27749 [05:00<00:11, 88.95it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  96% 26760/27749 [05:00<00:11, 88.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26780/27749 [05:00<00:10, 89.03it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26800/27749 [05:00<00:10, 89.07it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26820/27749 [05:00<00:10, 89.11it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26840/27749 [05:01<00:10, 89.15it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26860/27749 [05:01<00:09, 89.19it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26880/27749 [05:01<00:09, 89.23it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26900/27749 [05:01<00:09, 89.27it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26920/27749 [05:01<00:09, 89.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26940/27749 [05:01<00:09, 89.35it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26960/27749 [05:01<00:08, 89.38it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 26980/27749 [05:01<00:08, 89.42it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 27000/27749 [05:01<00:08, 89.46it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 27020/27749 [05:01<00:08, 89.50it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  97% 27040/27749 [05:01<00:07, 89.54it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27060/27749 [05:02<00:07, 89.58it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27080/27749 [05:02<00:07, 89.62it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27100/27749 [05:02<00:07, 89.66it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27120/27749 [05:02<00:07, 89.70it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27140/27749 [05:02<00:06, 89.74it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27160/27749 [05:02<00:06, 89.77it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27180/27749 [05:02<00:06, 89.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27200/27749 [05:02<00:06, 89.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27220/27749 [05:02<00:05, 89.89it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27240/27749 [05:02<00:05, 89.93it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27260/27749 [05:02<00:05, 89.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27280/27749 [05:03<00:05, 90.01it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27300/27749 [05:03<00:04, 90.05it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  98% 27320/27749 [05:03<00:04, 90.09it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27340/27749 [05:03<00:04, 90.13it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27360/27749 [05:03<00:04, 90.16it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27380/27749 [05:03<00:04, 90.20it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27400/27749 [05:03<00:03, 90.24it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27420/27749 [05:03<00:03, 90.28it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27440/27749 [05:03<00:03, 90.32it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27460/27749 [05:03<00:03, 90.35it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27480/27749 [05:04<00:02, 90.39it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27500/27749 [05:04<00:02, 90.43it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27520/27749 [05:04<00:02, 90.47it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27540/27749 [05:04<00:02, 90.51it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27560/27749 [05:04<00:02, 90.54it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27580/27749 [05:04<00:01, 90.58it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3:  99% 27600/27749 [05:04<00:01, 90.62it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3: 100% 27620/27749 [05:04<00:01, 90.65it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3: 100% 27640/27749 [05:04<00:01, 90.69it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3: 100% 27660/27749 [05:04<00:00, 90.73it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3: 100% 27680/27749 [05:04<00:00, 90.77it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3: 100% 27700/27749 [05:05<00:00, 90.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3: 100% 27720/27749 [05:05<00:00, 90.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3: 100% 27740/27749 [05:05<00:00, 90.89it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.720, avg_val_loss=2.720, train_loss_epoch=2.920]\n",
            "Epoch 3: 100% 27749/27749 [05:05<00:00, 90.90it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.920]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 4:  80% 22180/27749 [04:36<01:09, 80.18it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  80% 22200/27749 [04:39<01:09, 79.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  80% 22220/27749 [04:40<01:09, 79.36it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  80% 22240/27749 [04:40<01:09, 79.40it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  80% 22260/27749 [04:40<01:09, 79.45it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  80% 22280/27749 [04:40<01:08, 79.49it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  80% 22300/27749 [04:40<01:08, 79.54it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  80% 22320/27749 [04:40<01:08, 79.58it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22340/27749 [04:40<01:07, 79.63it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22360/27749 [04:40<01:07, 79.67it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22380/27749 [04:40<01:07, 79.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22400/27749 [04:40<01:07, 79.76it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22420/27749 [04:40<01:06, 79.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22440/27749 [04:41<01:06, 79.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22460/27749 [04:41<01:06, 79.90it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22480/27749 [04:41<01:05, 79.94it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22500/27749 [04:41<01:05, 79.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22520/27749 [04:41<01:05, 80.03it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22540/27749 [04:41<01:05, 80.08it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22560/27749 [04:41<01:04, 80.12it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22580/27749 [04:41<01:04, 80.17it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  81% 22600/27749 [04:41<01:04, 80.21it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22620/27749 [04:41<01:03, 80.26it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22640/27749 [04:41<01:03, 80.30it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22660/27749 [04:42<01:03, 80.34it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22680/27749 [04:42<01:03, 80.39it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22700/27749 [04:42<01:02, 80.43it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22720/27749 [04:42<01:02, 80.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22740/27749 [04:42<01:02, 80.52it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22760/27749 [04:42<01:01, 80.57it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22780/27749 [04:42<01:01, 80.61it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22800/27749 [04:42<01:01, 80.66it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22820/27749 [04:42<01:01, 80.70it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22840/27749 [04:42<01:00, 80.75it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22860/27749 [04:42<01:00, 80.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  82% 22880/27749 [04:43<01:00, 80.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 22900/27749 [04:43<00:59, 80.88it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 22920/27749 [04:43<00:59, 80.92it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 22940/27749 [04:43<00:59, 80.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 22960/27749 [04:43<00:59, 81.01it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 22980/27749 [04:43<00:58, 81.06it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23000/27749 [04:43<00:58, 81.10it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23020/27749 [04:43<00:58, 81.15it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23040/27749 [04:43<00:57, 81.19it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23060/27749 [04:43<00:57, 81.24it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23080/27749 [04:43<00:57, 81.28it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23100/27749 [04:44<00:57, 81.33it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23120/27749 [04:44<00:56, 81.37it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23140/27749 [04:44<00:56, 81.41it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  83% 23160/27749 [04:44<00:56, 81.46it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23180/27749 [04:44<00:56, 81.50it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23200/27749 [04:44<00:55, 81.55it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23220/27749 [04:44<00:55, 81.59it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23240/27749 [04:44<00:55, 81.64it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23260/27749 [04:44<00:54, 81.68it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23280/27749 [04:44<00:54, 81.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23300/27749 [04:44<00:54, 81.77it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23320/27749 [04:45<00:54, 81.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23340/27749 [04:45<00:53, 81.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23360/27749 [04:45<00:53, 81.90it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23380/27749 [04:45<00:53, 81.94it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23400/27749 [04:45<00:53, 81.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23420/27749 [04:45<00:52, 82.03it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  84% 23440/27749 [04:45<00:52, 82.08it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23460/27749 [04:45<00:52, 82.12it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23480/27749 [04:45<00:51, 82.16it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23500/27749 [04:45<00:51, 82.20it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23520/27749 [04:45<00:51, 82.25it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23540/27749 [04:46<00:51, 82.29it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23560/27749 [04:46<00:50, 82.33it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23580/27749 [04:46<00:50, 82.37it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23600/27749 [04:46<00:50, 82.42it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23620/27749 [04:46<00:50, 82.46it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23640/27749 [04:46<00:49, 82.50it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23660/27749 [04:46<00:49, 82.55it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23680/27749 [04:46<00:49, 82.59it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23700/27749 [04:46<00:48, 82.64it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  85% 23720/27749 [04:46<00:48, 82.68it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23740/27749 [04:46<00:48, 82.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23760/27749 [04:47<00:48, 82.76it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23780/27749 [04:47<00:47, 82.80it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23800/27749 [04:47<00:47, 82.84it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23820/27749 [04:47<00:47, 82.88it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23840/27749 [04:47<00:47, 82.93it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23860/27749 [04:47<00:46, 82.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23880/27749 [04:47<00:46, 83.01it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23900/27749 [04:47<00:46, 83.06it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23920/27749 [04:47<00:46, 83.10it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23940/27749 [04:47<00:45, 83.14it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23960/27749 [04:48<00:45, 83.18it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 23980/27749 [04:48<00:45, 83.22it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  86% 24000/27749 [04:48<00:45, 83.27it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24020/27749 [04:48<00:44, 83.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24040/27749 [04:48<00:44, 83.35it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24060/27749 [04:48<00:44, 83.40it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24080/27749 [04:48<00:43, 83.44it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24100/27749 [04:48<00:43, 83.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24120/27749 [04:48<00:43, 83.52it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24140/27749 [04:48<00:43, 83.56it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24160/27749 [04:48<00:42, 83.60it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24180/27749 [04:49<00:42, 83.65it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24200/27749 [04:49<00:42, 83.69it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24220/27749 [04:49<00:42, 83.73it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24240/27749 [04:49<00:41, 83.77it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24260/27749 [04:49<00:41, 83.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  87% 24280/27749 [04:49<00:41, 83.86it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24300/27749 [04:49<00:41, 83.90it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24320/27749 [04:49<00:40, 83.94it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24340/27749 [04:49<00:40, 83.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24360/27749 [04:49<00:40, 84.03it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24380/27749 [04:49<00:40, 84.07it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24400/27749 [04:50<00:39, 84.12it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24420/27749 [04:50<00:39, 84.16it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24440/27749 [04:50<00:39, 84.20it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24460/27749 [04:50<00:39, 84.25it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24480/27749 [04:50<00:38, 84.29it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24500/27749 [04:50<00:38, 84.33it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24520/27749 [04:50<00:38, 84.37it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  88% 24540/27749 [04:50<00:38, 84.42it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24560/27749 [04:50<00:37, 84.46it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24580/27749 [04:50<00:37, 84.50it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24600/27749 [04:50<00:37, 84.55it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24620/27749 [04:51<00:36, 84.59it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24640/27749 [04:51<00:36, 84.63it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24660/27749 [04:51<00:36, 84.67it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24680/27749 [04:51<00:36, 84.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24700/27749 [04:51<00:35, 84.76it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24720/27749 [04:51<00:35, 84.80it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24740/27749 [04:51<00:35, 84.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24760/27749 [04:51<00:35, 84.89it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24780/27749 [04:51<00:34, 84.93it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24800/27749 [04:51<00:34, 84.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  89% 24820/27749 [04:51<00:34, 85.02it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 24840/27749 [04:52<00:34, 85.06it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 24860/27749 [04:52<00:33, 85.10it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 24880/27749 [04:52<00:33, 85.15it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 24900/27749 [04:52<00:33, 85.19it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 24920/27749 [04:52<00:33, 85.23it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 24940/27749 [04:52<00:32, 85.28it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 24960/27749 [04:52<00:32, 85.32it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 24980/27749 [04:52<00:32, 85.36it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 25000/27749 [04:52<00:32, 85.41it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 25020/27749 [04:52<00:31, 85.45it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 25040/27749 [04:52<00:31, 85.49it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 25060/27749 [04:52<00:31, 85.53it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 25080/27749 [04:53<00:31, 85.57it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  90% 25100/27749 [04:53<00:30, 85.62it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25120/27749 [04:53<00:30, 85.66it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25140/27749 [04:53<00:30, 85.70it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25160/27749 [04:53<00:30, 85.74it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25180/27749 [04:53<00:29, 85.78it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25200/27749 [04:53<00:29, 85.82it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25220/27749 [04:53<00:29, 85.86it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25240/27749 [04:53<00:29, 85.90it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25260/27749 [04:53<00:28, 85.94it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25280/27749 [04:54<00:28, 85.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25300/27749 [04:54<00:28, 86.03it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25320/27749 [04:54<00:28, 86.07it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25340/27749 [04:54<00:27, 86.11it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25360/27749 [04:54<00:27, 86.15it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  91% 25380/27749 [04:54<00:27, 86.19it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25400/27749 [04:54<00:27, 86.23it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25420/27749 [04:54<00:26, 86.27it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25440/27749 [04:54<00:26, 86.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25460/27749 [04:54<00:26, 86.35it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25480/27749 [04:54<00:26, 86.39it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25500/27749 [04:55<00:26, 86.43it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25520/27749 [04:55<00:25, 86.47it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25540/27749 [04:55<00:25, 86.51it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25560/27749 [04:55<00:25, 86.55it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25580/27749 [04:55<00:25, 86.59it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25600/27749 [04:55<00:24, 86.63it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25620/27749 [04:55<00:24, 86.67it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25640/27749 [04:55<00:24, 86.71it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  92% 25660/27749 [04:55<00:24, 86.75it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25680/27749 [04:55<00:23, 86.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25700/27749 [04:55<00:23, 86.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25720/27749 [04:56<00:23, 86.87it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25740/27749 [04:56<00:23, 86.91it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25760/27749 [04:56<00:22, 86.95it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25780/27749 [04:56<00:22, 86.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25800/27749 [04:56<00:22, 87.04it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25820/27749 [04:56<00:22, 87.08it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25840/27749 [04:56<00:21, 87.12it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25860/27749 [04:56<00:21, 87.16it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25880/27749 [04:56<00:21, 87.20it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25900/27749 [04:56<00:21, 87.24it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25920/27749 [04:56<00:20, 87.28it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  93% 25940/27749 [04:57<00:20, 87.32it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 25960/27749 [04:57<00:20, 87.36it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 25980/27749 [04:57<00:20, 87.40it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26000/27749 [04:57<00:20, 87.44it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26020/27749 [04:57<00:19, 87.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26040/27749 [04:57<00:19, 87.52it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26060/27749 [04:57<00:19, 87.57it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26080/27749 [04:57<00:19, 87.61it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26100/27749 [04:57<00:18, 87.65it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26120/27749 [04:57<00:18, 87.69it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26140/27749 [04:57<00:18, 87.73it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26160/27749 [04:58<00:18, 87.77it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26180/27749 [04:58<00:17, 87.81it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26200/27749 [04:58<00:17, 87.85it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  94% 26220/27749 [04:58<00:17, 87.89it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26240/27749 [04:58<00:17, 87.93it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26260/27749 [04:58<00:16, 87.97it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26280/27749 [04:58<00:16, 88.01it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26300/27749 [04:58<00:16, 88.05it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26320/27749 [04:58<00:16, 88.09it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26340/27749 [04:58<00:15, 88.13it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26360/27749 [04:58<00:15, 88.17it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26380/27749 [04:59<00:15, 88.21it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26400/27749 [04:59<00:15, 88.25it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26420/27749 [04:59<00:15, 88.29it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26440/27749 [04:59<00:14, 88.33it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26460/27749 [04:59<00:14, 88.36it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26480/27749 [04:59<00:14, 88.40it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  95% 26500/27749 [04:59<00:14, 88.44it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26520/27749 [04:59<00:13, 88.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26540/27749 [04:59<00:13, 88.52it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26560/27749 [04:59<00:13, 88.56it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26580/27749 [05:00<00:13, 88.60it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26600/27749 [05:00<00:12, 88.63it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26620/27749 [05:00<00:12, 88.67it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26640/27749 [05:00<00:12, 88.71it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26660/27749 [05:00<00:12, 88.75it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26680/27749 [05:00<00:12, 88.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26700/27749 [05:00<00:11, 88.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26720/27749 [05:00<00:11, 88.87it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26740/27749 [05:00<00:11, 88.90it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  96% 26760/27749 [05:00<00:11, 88.94it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26780/27749 [05:00<00:10, 88.98it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26800/27749 [05:01<00:10, 89.02it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26820/27749 [05:01<00:10, 89.06it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26840/27749 [05:01<00:10, 89.10it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26860/27749 [05:01<00:09, 89.14it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26880/27749 [05:01<00:09, 89.17it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26900/27749 [05:01<00:09, 89.21it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26920/27749 [05:01<00:09, 89.25it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26940/27749 [05:01<00:09, 89.29it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26960/27749 [05:01<00:08, 89.33it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 26980/27749 [05:01<00:08, 89.37it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 27000/27749 [05:02<00:08, 89.40it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 27020/27749 [05:02<00:08, 89.44it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  97% 27040/27749 [05:02<00:07, 89.48it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27060/27749 [05:02<00:07, 89.52it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27080/27749 [05:02<00:07, 89.56it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27100/27749 [05:02<00:07, 89.60it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27120/27749 [05:02<00:07, 89.64it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27140/27749 [05:02<00:06, 89.68it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27160/27749 [05:02<00:06, 89.72it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27180/27749 [05:02<00:06, 89.76it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27200/27749 [05:02<00:06, 89.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27220/27749 [05:03<00:05, 89.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27240/27749 [05:03<00:05, 89.87it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27260/27749 [05:03<00:05, 89.91it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27280/27749 [05:03<00:05, 89.95it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27300/27749 [05:03<00:04, 89.99it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  98% 27320/27749 [05:03<00:04, 90.03it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27340/27749 [05:03<00:04, 90.07it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27360/27749 [05:03<00:04, 90.11it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27380/27749 [05:03<00:04, 90.15it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27400/27749 [05:03<00:03, 90.19it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27420/27749 [05:03<00:03, 90.23it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27440/27749 [05:03<00:03, 90.27it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27460/27749 [05:04<00:03, 90.31it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27480/27749 [05:04<00:02, 90.34it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27500/27749 [05:04<00:02, 90.38it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27520/27749 [05:04<00:02, 90.42it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27540/27749 [05:04<00:02, 90.45it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27560/27749 [05:04<00:02, 90.49it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27580/27749 [05:04<00:01, 90.53it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4:  99% 27600/27749 [05:04<00:01, 90.57it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4: 100% 27620/27749 [05:04<00:01, 90.60it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4: 100% 27640/27749 [05:04<00:01, 90.64it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4: 100% 27660/27749 [05:05<00:00, 90.67it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4: 100% 27680/27749 [05:05<00:00, 90.71it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4: 100% 27700/27749 [05:05<00:00, 90.75it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4: 100% 27720/27749 [05:05<00:00, 90.79it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4: 100% 27740/27749 [05:05<00:00, 90.83it/s, loss=2.89, v_num=0, train_loss_step=2.890, val_loss=2.730, avg_val_loss=2.730, train_loss_epoch=2.900]\n",
            "Epoch 4: 100% 27749/27749 [05:05<00:00, 90.84it/s, loss=2.89, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.900]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 5:  80% 22180/27749 [04:38<01:09, 79.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  80% 22200/27749 [04:42<01:10, 78.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  80% 22220/27749 [04:42<01:10, 78.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  80% 22240/27749 [04:42<01:09, 78.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  80% 22260/27749 [04:42<01:09, 78.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  80% 22280/27749 [04:42<01:09, 78.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  80% 22300/27749 [04:42<01:09, 78.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  80% 22320/27749 [04:42<01:08, 78.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22340/27749 [04:42<01:08, 79.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22360/27749 [04:42<01:08, 79.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22380/27749 [04:42<01:07, 79.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22400/27749 [04:42<01:07, 79.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22420/27749 [04:43<01:07, 79.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22440/27749 [04:43<01:06, 79.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22460/27749 [04:43<01:06, 79.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22480/27749 [04:43<01:06, 79.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22500/27749 [04:43<01:06, 79.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22520/27749 [04:43<01:05, 79.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22540/27749 [04:43<01:05, 79.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22560/27749 [04:43<01:05, 79.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22580/27749 [04:43<01:04, 79.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  81% 22600/27749 [04:43<01:04, 79.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22620/27749 [04:44<01:04, 79.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22640/27749 [04:44<01:04, 79.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22660/27749 [04:44<01:03, 79.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22680/27749 [04:44<01:03, 79.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22700/27749 [04:44<01:03, 79.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22720/27749 [04:44<01:02, 79.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22740/27749 [04:44<01:02, 79.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22760/27749 [04:44<01:02, 79.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22780/27749 [04:44<01:02, 80.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22800/27749 [04:44<01:01, 80.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22820/27749 [04:44<01:01, 80.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22840/27749 [04:45<01:01, 80.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22860/27749 [04:45<01:00, 80.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  82% 22880/27749 [04:45<01:00, 80.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 22900/27749 [04:45<01:00, 80.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 22920/27749 [04:45<01:00, 80.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 22940/27749 [04:45<00:59, 80.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 22960/27749 [04:45<00:59, 80.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 22980/27749 [04:45<00:59, 80.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23000/27749 [04:45<00:59, 80.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23020/27749 [04:45<00:58, 80.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23040/27749 [04:45<00:58, 80.57it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23060/27749 [04:46<00:58, 80.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23080/27749 [04:46<00:57, 80.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23100/27749 [04:46<00:57, 80.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23120/27749 [04:46<00:57, 80.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23140/27749 [04:46<00:57, 80.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  83% 23160/27749 [04:46<00:56, 80.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23180/27749 [04:46<00:56, 80.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23200/27749 [04:46<00:56, 80.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23220/27749 [04:46<00:55, 80.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23240/27749 [04:46<00:55, 80.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23260/27749 [04:47<00:55, 81.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23280/27749 [04:47<00:55, 81.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23300/27749 [04:47<00:54, 81.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23320/27749 [04:47<00:54, 81.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23340/27749 [04:47<00:54, 81.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23360/27749 [04:47<00:54, 81.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23380/27749 [04:47<00:53, 81.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23400/27749 [04:47<00:53, 81.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23420/27749 [04:47<00:53, 81.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  84% 23440/27749 [04:47<00:52, 81.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23460/27749 [04:47<00:52, 81.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23480/27749 [04:48<00:52, 81.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23500/27749 [04:48<00:52, 81.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23520/27749 [04:48<00:51, 81.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23540/27749 [04:48<00:51, 81.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23560/27749 [04:48<00:51, 81.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23580/27749 [04:48<00:51, 81.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23600/27749 [04:48<00:50, 81.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23620/27749 [04:48<00:50, 81.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23640/27749 [04:48<00:50, 81.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23660/27749 [04:48<00:49, 81.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23680/27749 [04:48<00:49, 81.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23700/27749 [04:49<00:49, 81.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  85% 23720/27749 [04:49<00:49, 82.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23740/27749 [04:49<00:48, 82.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23760/27749 [04:49<00:48, 82.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23780/27749 [04:49<00:48, 82.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23800/27749 [04:49<00:48, 82.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23820/27749 [04:49<00:47, 82.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23840/27749 [04:49<00:47, 82.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23860/27749 [04:49<00:47, 82.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23880/27749 [04:49<00:46, 82.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23900/27749 [04:50<00:46, 82.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23920/27749 [04:50<00:46, 82.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23940/27749 [04:50<00:46, 82.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23960/27749 [04:50<00:45, 82.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 23980/27749 [04:50<00:45, 82.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  86% 24000/27749 [04:50<00:45, 82.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24020/27749 [04:50<00:45, 82.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24040/27749 [04:50<00:44, 82.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24060/27749 [04:50<00:44, 82.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24080/27749 [04:50<00:44, 82.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24100/27749 [04:50<00:44, 82.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24120/27749 [04:51<00:43, 82.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24140/27749 [04:51<00:43, 82.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24160/27749 [04:51<00:43, 82.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24180/27749 [04:51<00:42, 83.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24200/27749 [04:51<00:42, 83.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24220/27749 [04:51<00:42, 83.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24240/27749 [04:51<00:42, 83.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24260/27749 [04:51<00:41, 83.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  87% 24280/27749 [04:51<00:41, 83.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24300/27749 [04:51<00:41, 83.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24320/27749 [04:51<00:41, 83.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24340/27749 [04:52<00:40, 83.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24360/27749 [04:52<00:40, 83.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24380/27749 [04:52<00:40, 83.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24400/27749 [04:52<00:40, 83.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24420/27749 [04:52<00:39, 83.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24440/27749 [04:52<00:39, 83.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24460/27749 [04:52<00:39, 83.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24480/27749 [04:52<00:39, 83.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24500/27749 [04:52<00:38, 83.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24520/27749 [04:52<00:38, 83.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  88% 24540/27749 [04:52<00:38, 83.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24560/27749 [04:53<00:38, 83.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24580/27749 [04:53<00:37, 83.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24600/27749 [04:53<00:37, 83.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24620/27749 [04:53<00:37, 83.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24640/27749 [04:53<00:37, 83.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24660/27749 [04:53<00:36, 84.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24680/27749 [04:53<00:36, 84.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24700/27749 [04:53<00:36, 84.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24720/27749 [04:53<00:35, 84.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24740/27749 [04:53<00:35, 84.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24760/27749 [04:53<00:35, 84.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24780/27749 [04:54<00:35, 84.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24800/27749 [04:54<00:34, 84.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  89% 24820/27749 [04:54<00:34, 84.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 24840/27749 [04:54<00:34, 84.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 24860/27749 [04:54<00:34, 84.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 24880/27749 [04:54<00:33, 84.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 24900/27749 [04:54<00:33, 84.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 24920/27749 [04:54<00:33, 84.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 24940/27749 [04:54<00:33, 84.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 24960/27749 [04:54<00:32, 84.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 24980/27749 [04:54<00:32, 84.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 25000/27749 [04:55<00:32, 84.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 25020/27749 [04:55<00:32, 84.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 25040/27749 [04:55<00:31, 84.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 25060/27749 [04:55<00:31, 84.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 25080/27749 [04:55<00:31, 84.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  90% 25100/27749 [04:55<00:31, 84.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25120/27749 [04:55<00:30, 84.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25140/27749 [04:55<00:30, 85.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25160/27749 [04:55<00:30, 85.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25180/27749 [04:55<00:30, 85.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25200/27749 [04:55<00:29, 85.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25220/27749 [04:56<00:29, 85.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25240/27749 [04:56<00:29, 85.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25260/27749 [04:56<00:29, 85.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25280/27749 [04:56<00:28, 85.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25300/27749 [04:56<00:28, 85.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25320/27749 [04:56<00:28, 85.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25340/27749 [04:56<00:28, 85.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25360/27749 [04:56<00:27, 85.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  91% 25380/27749 [04:56<00:27, 85.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25400/27749 [04:56<00:27, 85.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25420/27749 [04:56<00:27, 85.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25440/27749 [04:57<00:26, 85.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25460/27749 [04:57<00:26, 85.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25480/27749 [04:57<00:26, 85.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25500/27749 [04:57<00:26, 85.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25520/27749 [04:57<00:25, 85.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25540/27749 [04:57<00:25, 85.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25560/27749 [04:57<00:25, 85.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25580/27749 [04:57<00:25, 85.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25600/27749 [04:57<00:25, 85.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25620/27749 [04:57<00:24, 85.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25640/27749 [04:58<00:24, 86.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  92% 25660/27749 [04:58<00:24, 86.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25680/27749 [04:58<00:24, 86.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25700/27749 [04:58<00:23, 86.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25720/27749 [04:58<00:23, 86.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25740/27749 [04:58<00:23, 86.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25760/27749 [04:58<00:23, 86.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25780/27749 [04:58<00:22, 86.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25800/27749 [04:58<00:22, 86.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25820/27749 [04:58<00:22, 86.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25840/27749 [04:58<00:22, 86.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25860/27749 [04:59<00:21, 86.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25880/27749 [04:59<00:21, 86.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25900/27749 [04:59<00:21, 86.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25920/27749 [04:59<00:21, 86.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  93% 25940/27749 [04:59<00:20, 86.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 25960/27749 [04:59<00:20, 86.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 25980/27749 [04:59<00:20, 86.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26000/27749 [04:59<00:20, 86.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26020/27749 [04:59<00:19, 86.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26040/27749 [04:59<00:19, 86.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26060/27749 [04:59<00:19, 86.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26080/27749 [05:00<00:19, 86.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26100/27749 [05:00<00:18, 86.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26120/27749 [05:00<00:18, 86.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26140/27749 [05:00<00:18, 87.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26160/27749 [05:00<00:18, 87.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26180/27749 [05:00<00:18, 87.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26200/27749 [05:00<00:17, 87.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  94% 26220/27749 [05:00<00:17, 87.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26240/27749 [05:00<00:17, 87.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26260/27749 [05:00<00:17, 87.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26280/27749 [05:01<00:16, 87.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26300/27749 [05:01<00:16, 87.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26320/27749 [05:01<00:16, 87.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26340/27749 [05:01<00:16, 87.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26360/27749 [05:01<00:15, 87.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26380/27749 [05:01<00:15, 87.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26400/27749 [05:01<00:15, 87.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26420/27749 [05:01<00:15, 87.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26440/27749 [05:01<00:14, 87.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26460/27749 [05:01<00:14, 87.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26480/27749 [05:01<00:14, 87.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  95% 26500/27749 [05:02<00:14, 87.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26520/27749 [05:02<00:14, 87.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26540/27749 [05:02<00:13, 87.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26560/27749 [05:02<00:13, 87.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26580/27749 [05:02<00:13, 87.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26600/27749 [05:02<00:13, 87.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26620/27749 [05:02<00:12, 87.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26640/27749 [05:02<00:12, 88.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26660/27749 [05:02<00:12, 88.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26680/27749 [05:02<00:12, 88.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26700/27749 [05:02<00:11, 88.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26720/27749 [05:03<00:11, 88.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26740/27749 [05:03<00:11, 88.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  96% 26760/27749 [05:03<00:11, 88.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26780/27749 [05:03<00:10, 88.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26800/27749 [05:03<00:10, 88.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26820/27749 [05:03<00:10, 88.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26840/27749 [05:03<00:10, 88.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26860/27749 [05:03<00:10, 88.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26880/27749 [05:03<00:09, 88.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26900/27749 [05:03<00:09, 88.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26920/27749 [05:04<00:09, 88.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26940/27749 [05:04<00:09, 88.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26960/27749 [05:04<00:08, 88.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 26980/27749 [05:04<00:08, 88.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 27000/27749 [05:04<00:08, 88.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 27020/27749 [05:04<00:08, 88.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  97% 27040/27749 [05:04<00:07, 88.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27060/27749 [05:04<00:07, 88.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27080/27749 [05:04<00:07, 88.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27100/27749 [05:04<00:07, 88.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27120/27749 [05:04<00:07, 88.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27140/27749 [05:05<00:06, 88.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27160/27749 [05:05<00:06, 89.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27180/27749 [05:05<00:06, 89.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27200/27749 [05:05<00:06, 89.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27220/27749 [05:05<00:05, 89.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27240/27749 [05:05<00:05, 89.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27260/27749 [05:05<00:05, 89.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27280/27749 [05:05<00:05, 89.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27300/27749 [05:05<00:05, 89.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  98% 27320/27749 [05:05<00:04, 89.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27340/27749 [05:05<00:04, 89.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27360/27749 [05:06<00:04, 89.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27380/27749 [05:06<00:04, 89.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27400/27749 [05:06<00:03, 89.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27420/27749 [05:06<00:03, 89.53it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27440/27749 [05:06<00:03, 89.57it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27460/27749 [05:06<00:03, 89.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27480/27749 [05:06<00:03, 89.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27500/27749 [05:06<00:02, 89.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27520/27749 [05:06<00:02, 89.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27540/27749 [05:06<00:02, 89.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27560/27749 [05:06<00:02, 89.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27580/27749 [05:06<00:01, 89.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5:  99% 27600/27749 [05:07<00:01, 89.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5: 100% 27620/27749 [05:07<00:01, 89.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5: 100% 27640/27749 [05:07<00:01, 89.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5: 100% 27660/27749 [05:07<00:00, 90.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5: 100% 27680/27749 [05:07<00:00, 90.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5: 100% 27700/27749 [05:07<00:00, 90.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5: 100% 27720/27749 [05:07<00:00, 90.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5: 100% 27740/27749 [05:07<00:00, 90.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.890]\n",
            "Epoch 5: 100% 27749/27749 [05:07<00:00, 90.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 6:  80% 22180/27749 [04:42<01:10, 78.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  80% 22200/27749 [04:45<01:11, 77.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  80% 22220/27749 [04:45<01:11, 77.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  80% 22240/27749 [04:45<01:10, 77.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  80% 22260/27749 [04:45<01:10, 77.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  80% 22280/27749 [04:45<01:10, 77.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  80% 22300/27749 [04:46<01:09, 77.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  80% 22320/27749 [04:46<01:09, 78.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22340/27749 [04:46<01:09, 78.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22360/27749 [04:46<01:09, 78.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22380/27749 [04:46<01:08, 78.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22400/27749 [04:46<01:08, 78.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22420/27749 [04:46<01:08, 78.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22440/27749 [04:46<01:07, 78.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22460/27749 [04:46<01:07, 78.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22480/27749 [04:46<01:07, 78.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22500/27749 [04:46<01:06, 78.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22520/27749 [04:47<01:06, 78.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22540/27749 [04:47<01:06, 78.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22560/27749 [04:47<01:06, 78.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22580/27749 [04:47<01:05, 78.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  81% 22600/27749 [04:47<01:05, 78.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22620/27749 [04:47<01:05, 78.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22640/27749 [04:47<01:04, 78.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22660/27749 [04:47<01:04, 78.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22680/27749 [04:47<01:04, 78.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22700/27749 [04:47<01:04, 78.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22720/27749 [04:48<01:03, 78.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22740/27749 [04:48<01:03, 78.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22760/27749 [04:48<01:03, 78.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22780/27749 [04:48<01:02, 79.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22800/27749 [04:48<01:02, 79.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22820/27749 [04:48<01:02, 79.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22840/27749 [04:48<01:02, 79.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22860/27749 [04:48<01:01, 79.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  82% 22880/27749 [04:48<01:01, 79.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 22900/27749 [04:48<01:01, 79.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 22920/27749 [04:48<01:00, 79.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 22940/27749 [04:49<01:00, 79.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 22960/27749 [04:49<01:00, 79.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 22980/27749 [04:49<01:00, 79.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23000/27749 [04:49<00:59, 79.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23020/27749 [04:49<00:59, 79.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23040/27749 [04:49<00:59, 79.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23060/27749 [04:49<00:58, 79.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23080/27749 [04:49<00:58, 79.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23100/27749 [04:49<00:58, 79.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23120/27749 [04:49<00:58, 79.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23140/27749 [04:49<00:57, 79.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  83% 23160/27749 [04:50<00:57, 79.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23180/27749 [04:50<00:57, 79.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23200/27749 [04:50<00:56, 79.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23220/27749 [04:50<00:56, 79.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23240/27749 [04:50<00:56, 80.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23260/27749 [04:50<00:56, 80.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23280/27749 [04:50<00:55, 80.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23300/27749 [04:50<00:55, 80.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23320/27749 [04:50<00:55, 80.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23340/27749 [04:50<00:54, 80.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23360/27749 [04:50<00:54, 80.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23380/27749 [04:51<00:54, 80.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23400/27749 [04:51<00:54, 80.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23420/27749 [04:51<00:53, 80.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  84% 23440/27749 [04:51<00:53, 80.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23460/27749 [04:51<00:53, 80.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23480/27749 [04:51<00:52, 80.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23500/27749 [04:51<00:52, 80.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23520/27749 [04:51<00:52, 80.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23540/27749 [04:51<00:52, 80.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23560/27749 [04:51<00:51, 80.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23580/27749 [04:51<00:51, 80.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23600/27749 [04:52<00:51, 80.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23620/27749 [04:52<00:51, 80.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23640/27749 [04:52<00:50, 80.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23660/27749 [04:52<00:50, 80.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23680/27749 [04:52<00:50, 80.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23700/27749 [04:52<00:49, 81.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  85% 23720/27749 [04:52<00:49, 81.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23740/27749 [04:52<00:49, 81.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23760/27749 [04:52<00:49, 81.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23780/27749 [04:52<00:48, 81.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23800/27749 [04:52<00:48, 81.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23820/27749 [04:53<00:48, 81.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23840/27749 [04:53<00:48, 81.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23860/27749 [04:53<00:47, 81.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23880/27749 [04:53<00:47, 81.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23900/27749 [04:53<00:47, 81.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23920/27749 [04:53<00:46, 81.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23940/27749 [04:53<00:46, 81.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23960/27749 [04:53<00:46, 81.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 23980/27749 [04:53<00:46, 81.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  86% 24000/27749 [04:53<00:45, 81.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24020/27749 [04:54<00:45, 81.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24040/27749 [04:54<00:45, 81.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24060/27749 [04:54<00:45, 81.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24080/27749 [04:54<00:44, 81.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24100/27749 [04:54<00:44, 81.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24120/27749 [04:54<00:44, 81.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24140/27749 [04:54<00:44, 81.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24160/27749 [04:54<00:43, 81.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24180/27749 [04:54<00:43, 82.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24200/27749 [04:54<00:43, 82.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24220/27749 [04:54<00:42, 82.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24240/27749 [04:55<00:42, 82.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24260/27749 [04:55<00:42, 82.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  87% 24280/27749 [04:55<00:42, 82.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24300/27749 [04:55<00:41, 82.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24320/27749 [04:55<00:41, 82.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24340/27749 [04:55<00:41, 82.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24360/27749 [04:55<00:41, 82.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24380/27749 [04:55<00:40, 82.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24400/27749 [04:55<00:40, 82.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24420/27749 [04:55<00:40, 82.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24440/27749 [04:55<00:40, 82.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24460/27749 [04:56<00:39, 82.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24480/27749 [04:56<00:39, 82.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24500/27749 [04:56<00:39, 82.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24520/27749 [04:56<00:39, 82.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  88% 24540/27749 [04:56<00:38, 82.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24560/27749 [04:56<00:38, 82.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24580/27749 [04:56<00:38, 82.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24600/27749 [04:56<00:37, 82.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24620/27749 [04:56<00:37, 82.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24640/27749 [04:56<00:37, 82.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24660/27749 [04:57<00:37, 83.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24680/27749 [04:57<00:36, 83.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24700/27749 [04:57<00:36, 83.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24720/27749 [04:57<00:36, 83.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24740/27749 [04:57<00:36, 83.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24760/27749 [04:57<00:35, 83.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24780/27749 [04:57<00:35, 83.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24800/27749 [04:57<00:35, 83.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  89% 24820/27749 [04:57<00:35, 83.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 24840/27749 [04:57<00:34, 83.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 24860/27749 [04:58<00:34, 83.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 24880/27749 [04:58<00:34, 83.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 24900/27749 [04:58<00:34, 83.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 24920/27749 [04:58<00:33, 83.53it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 24940/27749 [04:58<00:33, 83.57it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 24960/27749 [04:58<00:33, 83.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 24980/27749 [04:58<00:33, 83.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 25000/27749 [04:58<00:32, 83.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 25020/27749 [04:58<00:32, 83.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 25040/27749 [04:58<00:32, 83.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 25060/27749 [04:58<00:32, 83.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 25080/27749 [04:59<00:31, 83.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  90% 25100/27749 [04:59<00:31, 83.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25120/27749 [04:59<00:31, 83.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25140/27749 [04:59<00:31, 83.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25160/27749 [04:59<00:30, 84.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25180/27749 [04:59<00:30, 84.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25200/27749 [04:59<00:30, 84.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25220/27749 [04:59<00:30, 84.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25240/27749 [04:59<00:29, 84.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25260/27749 [04:59<00:29, 84.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25280/27749 [04:59<00:29, 84.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25300/27749 [05:00<00:29, 84.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25320/27749 [05:00<00:28, 84.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25340/27749 [05:00<00:28, 84.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25360/27749 [05:00<00:28, 84.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  91% 25380/27749 [05:00<00:28, 84.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25400/27749 [05:00<00:27, 84.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25420/27749 [05:00<00:27, 84.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25440/27749 [05:00<00:27, 84.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25460/27749 [05:00<00:27, 84.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25480/27749 [05:00<00:26, 84.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25500/27749 [05:00<00:26, 84.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25520/27749 [05:01<00:26, 84.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25540/27749 [05:01<00:26, 84.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25560/27749 [05:01<00:25, 84.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25580/27749 [05:01<00:25, 84.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25600/27749 [05:01<00:25, 84.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25620/27749 [05:01<00:25, 84.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25640/27749 [05:01<00:24, 85.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  92% 25660/27749 [05:01<00:24, 85.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25680/27749 [05:01<00:24, 85.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25700/27749 [05:01<00:24, 85.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25720/27749 [05:01<00:23, 85.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25740/27749 [05:02<00:23, 85.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25760/27749 [05:02<00:23, 85.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25780/27749 [05:02<00:23, 85.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25800/27749 [05:02<00:22, 85.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25820/27749 [05:02<00:22, 85.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25840/27749 [05:02<00:22, 85.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25860/27749 [05:02<00:22, 85.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25880/27749 [05:02<00:21, 85.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25900/27749 [05:02<00:21, 85.53it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25920/27749 [05:02<00:21, 85.57it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  93% 25940/27749 [05:03<00:21, 85.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 25960/27749 [05:03<00:20, 85.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 25980/27749 [05:03<00:20, 85.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26000/27749 [05:03<00:20, 85.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26020/27749 [05:03<00:20, 85.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26040/27749 [05:03<00:19, 85.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26060/27749 [05:03<00:19, 85.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26080/27749 [05:03<00:19, 85.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26100/27749 [05:03<00:19, 85.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26120/27749 [05:03<00:18, 85.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26140/27749 [05:03<00:18, 85.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26160/27749 [05:04<00:18, 86.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26180/27749 [05:04<00:18, 86.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26200/27749 [05:04<00:17, 86.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  94% 26220/27749 [05:04<00:17, 86.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26240/27749 [05:04<00:17, 86.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26260/27749 [05:04<00:17, 86.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26280/27749 [05:04<00:17, 86.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26300/27749 [05:04<00:16, 86.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26320/27749 [05:04<00:16, 86.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26340/27749 [05:04<00:16, 86.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26360/27749 [05:05<00:16, 86.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26380/27749 [05:05<00:15, 86.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26400/27749 [05:05<00:15, 86.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26420/27749 [05:05<00:15, 86.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26440/27749 [05:05<00:15, 86.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26460/27749 [05:05<00:14, 86.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26480/27749 [05:05<00:14, 86.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  95% 26500/27749 [05:05<00:14, 86.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26520/27749 [05:05<00:14, 86.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26540/27749 [05:05<00:13, 86.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26560/27749 [05:05<00:13, 86.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26580/27749 [05:06<00:13, 86.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26600/27749 [05:06<00:13, 86.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26620/27749 [05:06<00:12, 86.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26640/27749 [05:06<00:12, 86.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26660/27749 [05:06<00:12, 87.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26680/27749 [05:06<00:12, 87.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26700/27749 [05:06<00:12, 87.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26720/27749 [05:06<00:11, 87.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26740/27749 [05:06<00:11, 87.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  96% 26760/27749 [05:06<00:11, 87.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26780/27749 [05:06<00:11, 87.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26800/27749 [05:07<00:10, 87.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26820/27749 [05:07<00:10, 87.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26840/27749 [05:07<00:10, 87.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26860/27749 [05:07<00:10, 87.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26880/27749 [05:07<00:09, 87.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26900/27749 [05:07<00:09, 87.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26920/27749 [05:07<00:09, 87.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26940/27749 [05:07<00:09, 87.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26960/27749 [05:07<00:09, 87.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 26980/27749 [05:07<00:08, 87.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 27000/27749 [05:07<00:08, 87.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 27020/27749 [05:08<00:08, 87.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  97% 27040/27749 [05:08<00:08, 87.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27060/27749 [05:08<00:07, 87.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27080/27749 [05:08<00:07, 87.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27100/27749 [05:08<00:07, 87.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27120/27749 [05:08<00:07, 87.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27140/27749 [05:08<00:06, 87.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27160/27749 [05:08<00:06, 87.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27180/27749 [05:08<00:06, 88.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27200/27749 [05:08<00:06, 88.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27220/27749 [05:09<00:06, 88.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27240/27749 [05:09<00:05, 88.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27260/27749 [05:09<00:05, 88.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27280/27749 [05:09<00:05, 88.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27300/27749 [05:09<00:05, 88.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  98% 27320/27749 [05:09<00:04, 88.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27340/27749 [05:09<00:04, 88.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27360/27749 [05:09<00:04, 88.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27380/27749 [05:09<00:04, 88.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27400/27749 [05:09<00:03, 88.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27420/27749 [05:10<00:03, 88.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27440/27749 [05:10<00:03, 88.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27460/27749 [05:10<00:03, 88.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27480/27749 [05:10<00:03, 88.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27500/27749 [05:10<00:02, 88.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27520/27749 [05:10<00:02, 88.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27540/27749 [05:10<00:02, 88.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27560/27749 [05:10<00:02, 88.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27580/27749 [05:10<00:01, 88.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6:  99% 27600/27749 [05:10<00:01, 88.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6: 100% 27620/27749 [05:10<00:01, 88.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6: 100% 27640/27749 [05:11<00:01, 88.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6: 100% 27660/27749 [05:11<00:01, 88.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6: 100% 27680/27749 [05:11<00:00, 88.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6: 100% 27700/27749 [05:11<00:00, 88.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6: 100% 27720/27749 [05:11<00:00, 89.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6: 100% 27740/27749 [05:11<00:00, 89.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.890]\n",
            "Epoch 6: 100% 27749/27749 [05:11<00:00, 89.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.890]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 7:  80% 22180/27749 [04:40<01:10, 79.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  80% 22200/27749 [04:43<01:10, 78.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  80% 22220/27749 [04:43<01:10, 78.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  80% 22240/27749 [04:43<01:10, 78.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  80% 22260/27749 [04:43<01:10, 78.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  80% 22280/27749 [04:44<01:09, 78.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  80% 22300/27749 [04:44<01:09, 78.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  80% 22320/27749 [04:44<01:09, 78.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22340/27749 [04:44<01:08, 78.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22360/27749 [04:44<01:08, 78.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22380/27749 [04:44<01:08, 78.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22400/27749 [04:44<01:07, 78.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22420/27749 [04:44<01:07, 78.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22440/27749 [04:44<01:07, 78.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22460/27749 [04:44<01:07, 78.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22480/27749 [04:44<01:06, 78.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22500/27749 [04:45<01:06, 78.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22520/27749 [04:45<01:06, 78.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22540/27749 [04:45<01:05, 79.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22560/27749 [04:45<01:05, 79.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22580/27749 [04:45<01:05, 79.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  81% 22600/27749 [04:45<01:05, 79.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22620/27749 [04:45<01:04, 79.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22640/27749 [04:45<01:04, 79.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22660/27749 [04:45<01:04, 79.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22680/27749 [04:45<01:03, 79.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22700/27749 [04:45<01:03, 79.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22720/27749 [04:46<01:03, 79.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22740/27749 [04:46<01:03, 79.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22760/27749 [04:46<01:02, 79.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22780/27749 [04:46<01:02, 79.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22800/27749 [04:46<01:02, 79.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22820/27749 [04:46<01:01, 79.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22840/27749 [04:46<01:01, 79.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22860/27749 [04:46<01:01, 79.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  82% 22880/27749 [04:46<01:01, 79.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 22900/27749 [04:46<01:00, 79.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 22920/27749 [04:46<01:00, 79.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 22940/27749 [04:47<01:00, 79.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 22960/27749 [04:47<00:59, 79.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 22980/27749 [04:47<00:59, 79.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23000/27749 [04:47<00:59, 80.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23020/27749 [04:47<00:59, 80.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23040/27749 [04:47<00:58, 80.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23060/27749 [04:47<00:58, 80.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23080/27749 [04:47<00:58, 80.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23100/27749 [04:47<00:57, 80.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23120/27749 [04:47<00:57, 80.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23140/27749 [04:48<00:57, 80.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  83% 23160/27749 [04:48<00:57, 80.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23180/27749 [04:48<00:56, 80.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23200/27749 [04:48<00:56, 80.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23220/27749 [04:48<00:56, 80.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23240/27749 [04:48<00:55, 80.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23260/27749 [04:48<00:55, 80.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23280/27749 [04:48<00:55, 80.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23300/27749 [04:48<00:55, 80.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23320/27749 [04:48<00:54, 80.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23340/27749 [04:48<00:54, 80.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23360/27749 [04:49<00:54, 80.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23380/27749 [04:49<00:54, 80.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23400/27749 [04:49<00:53, 80.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23420/27749 [04:49<00:53, 80.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  84% 23440/27749 [04:49<00:53, 80.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23460/27749 [04:49<00:52, 81.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23480/27749 [04:49<00:52, 81.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23500/27749 [04:49<00:52, 81.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23520/27749 [04:49<00:52, 81.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23540/27749 [04:49<00:51, 81.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23560/27749 [04:49<00:51, 81.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23580/27749 [04:50<00:51, 81.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23600/27749 [04:50<00:51, 81.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23620/27749 [04:50<00:50, 81.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23640/27749 [04:50<00:50, 81.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23660/27749 [04:50<00:50, 81.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23680/27749 [04:50<00:49, 81.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23700/27749 [04:50<00:49, 81.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  85% 23720/27749 [04:50<00:49, 81.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23740/27749 [04:50<00:49, 81.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23760/27749 [04:50<00:48, 81.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23780/27749 [04:50<00:48, 81.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23800/27749 [04:51<00:48, 81.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23820/27749 [04:51<00:48, 81.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23840/27749 [04:51<00:47, 81.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23860/27749 [04:51<00:47, 81.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23880/27749 [04:51<00:47, 81.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23900/27749 [04:51<00:46, 81.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23920/27749 [04:51<00:46, 82.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23940/27749 [04:51<00:46, 82.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23960/27749 [04:51<00:46, 82.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 23980/27749 [04:51<00:45, 82.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  86% 24000/27749 [04:52<00:45, 82.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24020/27749 [04:52<00:45, 82.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24040/27749 [04:52<00:45, 82.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24060/27749 [04:52<00:44, 82.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24080/27749 [04:52<00:44, 82.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24100/27749 [04:52<00:44, 82.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24120/27749 [04:52<00:44, 82.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24140/27749 [04:52<00:43, 82.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24160/27749 [04:52<00:43, 82.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24180/27749 [04:52<00:43, 82.53it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24200/27749 [04:53<00:42, 82.57it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24220/27749 [04:53<00:42, 82.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24240/27749 [04:53<00:42, 82.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24260/27749 [04:53<00:42, 82.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  87% 24280/27749 [04:53<00:41, 82.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24300/27749 [04:53<00:41, 82.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24320/27749 [04:53<00:41, 82.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24340/27749 [04:53<00:41, 82.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24360/27749 [04:53<00:40, 82.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24380/27749 [04:53<00:40, 82.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24400/27749 [04:54<00:40, 82.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24420/27749 [04:54<00:40, 83.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24440/27749 [04:54<00:39, 83.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24460/27749 [04:54<00:39, 83.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24480/27749 [04:54<00:39, 83.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24500/27749 [04:54<00:39, 83.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24520/27749 [04:54<00:38, 83.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  88% 24540/27749 [04:54<00:38, 83.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24560/27749 [04:54<00:38, 83.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24580/27749 [04:54<00:38, 83.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24600/27749 [04:54<00:37, 83.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24620/27749 [04:55<00:37, 83.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24640/27749 [04:55<00:37, 83.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24660/27749 [04:55<00:36, 83.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24680/27749 [04:55<00:36, 83.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24700/27749 [04:55<00:36, 83.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24720/27749 [04:55<00:36, 83.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24740/27749 [04:55<00:35, 83.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24760/27749 [04:55<00:35, 83.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24780/27749 [04:55<00:35, 83.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24800/27749 [04:55<00:35, 83.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  89% 24820/27749 [04:55<00:34, 83.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 24840/27749 [04:56<00:34, 83.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 24860/27749 [04:56<00:34, 83.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 24880/27749 [04:56<00:34, 83.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 24900/27749 [04:56<00:33, 84.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 24920/27749 [04:56<00:33, 84.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 24940/27749 [04:56<00:33, 84.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 24960/27749 [04:56<00:33, 84.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 24980/27749 [04:56<00:32, 84.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 25000/27749 [04:56<00:32, 84.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 25020/27749 [04:56<00:32, 84.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 25040/27749 [04:56<00:32, 84.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 25060/27749 [04:57<00:31, 84.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 25080/27749 [04:57<00:31, 84.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  90% 25100/27749 [04:57<00:31, 84.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25120/27749 [04:57<00:31, 84.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25140/27749 [04:57<00:30, 84.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25160/27749 [04:57<00:30, 84.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25180/27749 [04:57<00:30, 84.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25200/27749 [04:57<00:30, 84.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25220/27749 [04:57<00:29, 84.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25240/27749 [04:57<00:29, 84.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25260/27749 [04:57<00:29, 84.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25280/27749 [04:58<00:29, 84.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25300/27749 [04:58<00:28, 84.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25320/27749 [04:58<00:28, 84.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25340/27749 [04:58<00:28, 84.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25360/27749 [04:58<00:28, 84.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  91% 25380/27749 [04:58<00:27, 85.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25400/27749 [04:58<00:27, 85.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25420/27749 [04:58<00:27, 85.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25440/27749 [04:58<00:27, 85.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25460/27749 [04:58<00:26, 85.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25480/27749 [04:58<00:26, 85.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25500/27749 [04:59<00:26, 85.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25520/27749 [04:59<00:26, 85.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25540/27749 [04:59<00:25, 85.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25560/27749 [04:59<00:25, 85.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25580/27749 [04:59<00:25, 85.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25600/27749 [04:59<00:25, 85.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25620/27749 [04:59<00:24, 85.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25640/27749 [04:59<00:24, 85.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  92% 25660/27749 [04:59<00:24, 85.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25680/27749 [04:59<00:24, 85.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25700/27749 [04:59<00:23, 85.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25720/27749 [05:00<00:23, 85.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25740/27749 [05:00<00:23, 85.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25760/27749 [05:00<00:23, 85.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25780/27749 [05:00<00:22, 85.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25800/27749 [05:00<00:22, 85.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25820/27749 [05:00<00:22, 85.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25840/27749 [05:00<00:22, 85.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25860/27749 [05:00<00:21, 86.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25880/27749 [05:00<00:21, 86.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25900/27749 [05:00<00:21, 86.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25920/27749 [05:00<00:21, 86.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  93% 25940/27749 [05:01<00:20, 86.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 25960/27749 [05:01<00:20, 86.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 25980/27749 [05:01<00:20, 86.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26000/27749 [05:01<00:20, 86.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26020/27749 [05:01<00:20, 86.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26040/27749 [05:01<00:19, 86.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26060/27749 [05:01<00:19, 86.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26080/27749 [05:01<00:19, 86.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26100/27749 [05:01<00:19, 86.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26120/27749 [05:01<00:18, 86.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26140/27749 [05:02<00:18, 86.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26160/27749 [05:02<00:18, 86.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26180/27749 [05:02<00:18, 86.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26200/27749 [05:02<00:17, 86.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  94% 26220/27749 [05:02<00:17, 86.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26240/27749 [05:02<00:17, 86.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26260/27749 [05:02<00:17, 86.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26280/27749 [05:02<00:16, 86.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26300/27749 [05:02<00:16, 86.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26320/27749 [05:02<00:16, 86.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26340/27749 [05:02<00:16, 86.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26360/27749 [05:03<00:15, 87.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26380/27749 [05:03<00:15, 87.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26400/27749 [05:03<00:15, 87.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26420/27749 [05:03<00:15, 87.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26440/27749 [05:03<00:15, 87.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26460/27749 [05:03<00:14, 87.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26480/27749 [05:03<00:14, 87.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  95% 26500/27749 [05:03<00:14, 87.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26520/27749 [05:03<00:14, 87.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26540/27749 [05:03<00:13, 87.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26560/27749 [05:03<00:13, 87.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26580/27749 [05:04<00:13, 87.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26600/27749 [05:04<00:13, 87.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26620/27749 [05:04<00:12, 87.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26640/27749 [05:04<00:12, 87.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26660/27749 [05:04<00:12, 87.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26680/27749 [05:04<00:12, 87.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26700/27749 [05:04<00:11, 87.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26720/27749 [05:04<00:11, 87.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26740/27749 [05:04<00:11, 87.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  96% 26760/27749 [05:04<00:11, 87.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26780/27749 [05:04<00:11, 87.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26800/27749 [05:05<00:10, 87.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26820/27749 [05:05<00:10, 87.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26840/27749 [05:05<00:10, 87.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26860/27749 [05:05<00:10, 87.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26880/27749 [05:05<00:09, 88.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26900/27749 [05:05<00:09, 88.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26920/27749 [05:05<00:09, 88.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26940/27749 [05:05<00:09, 88.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26960/27749 [05:05<00:08, 88.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 26980/27749 [05:05<00:08, 88.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 27000/27749 [05:05<00:08, 88.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 27020/27749 [05:06<00:08, 88.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  97% 27040/27749 [05:06<00:08, 88.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27060/27749 [05:06<00:07, 88.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27080/27749 [05:06<00:07, 88.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27100/27749 [05:06<00:07, 88.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27120/27749 [05:06<00:07, 88.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27140/27749 [05:06<00:06, 88.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27160/27749 [05:06<00:06, 88.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27180/27749 [05:06<00:06, 88.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27200/27749 [05:06<00:06, 88.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27220/27749 [05:06<00:05, 88.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27240/27749 [05:07<00:05, 88.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27260/27749 [05:07<00:05, 88.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27280/27749 [05:07<00:05, 88.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27300/27749 [05:07<00:05, 88.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  98% 27320/27749 [05:07<00:04, 88.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27340/27749 [05:07<00:04, 88.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27360/27749 [05:07<00:04, 88.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27380/27749 [05:07<00:04, 88.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27400/27749 [05:07<00:03, 89.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27420/27749 [05:07<00:03, 89.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27440/27749 [05:08<00:03, 89.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27460/27749 [05:08<00:03, 89.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27480/27749 [05:08<00:03, 89.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27500/27749 [05:08<00:02, 89.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27520/27749 [05:08<00:02, 89.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27540/27749 [05:08<00:02, 89.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27560/27749 [05:08<00:02, 89.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27580/27749 [05:08<00:01, 89.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7:  99% 27600/27749 [05:08<00:01, 89.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7: 100% 27620/27749 [05:08<00:01, 89.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7: 100% 27640/27749 [05:08<00:01, 89.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7: 100% 27660/27749 [05:09<00:00, 89.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7: 100% 27680/27749 [05:09<00:00, 89.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7: 100% 27700/27749 [05:09<00:00, 89.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7: 100% 27720/27749 [05:09<00:00, 89.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7: 100% 27740/27749 [05:09<00:00, 89.65it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 7: 100% 27749/27749 [05:09<00:00, 89.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 8:  80% 22180/27749 [04:37<01:09, 79.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  80% 22200/27749 [04:40<01:10, 79.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  80% 22220/27749 [04:41<01:09, 79.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  80% 22240/27749 [04:41<01:09, 79.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  80% 22260/27749 [04:41<01:09, 79.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  80% 22280/27749 [04:41<01:09, 79.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  80% 22300/27749 [04:41<01:08, 79.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  80% 22320/27749 [04:41<01:08, 79.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22340/27749 [04:41<01:08, 79.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22360/27749 [04:41<01:07, 79.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22380/27749 [04:41<01:07, 79.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22400/27749 [04:41<01:07, 79.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22420/27749 [04:42<01:07, 79.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22440/27749 [04:42<01:06, 79.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22460/27749 [04:42<01:06, 79.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22480/27749 [04:42<01:06, 79.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22500/27749 [04:42<01:05, 79.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22520/27749 [04:42<01:05, 79.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22540/27749 [04:42<01:05, 79.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22560/27749 [04:42<01:05, 79.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22580/27749 [04:42<01:04, 79.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  81% 22600/27749 [04:42<01:04, 79.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22620/27749 [04:42<01:04, 79.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22640/27749 [04:43<01:03, 79.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22660/27749 [04:43<01:03, 80.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22680/27749 [04:43<01:03, 80.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22700/27749 [04:43<01:03, 80.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22720/27749 [04:43<01:02, 80.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22740/27749 [04:43<01:02, 80.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22760/27749 [04:43<01:02, 80.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22780/27749 [04:43<01:01, 80.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22800/27749 [04:43<01:01, 80.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22820/27749 [04:43<01:01, 80.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22840/27749 [04:44<01:01, 80.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22860/27749 [04:44<01:00, 80.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  82% 22880/27749 [04:44<01:00, 80.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 22900/27749 [04:44<01:00, 80.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 22920/27749 [04:44<00:59, 80.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 22940/27749 [04:44<00:59, 80.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 22960/27749 [04:44<00:59, 80.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 22980/27749 [04:44<00:59, 80.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23000/27749 [04:44<00:58, 80.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23020/27749 [04:44<00:58, 80.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23040/27749 [04:44<00:58, 80.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23060/27749 [04:45<00:57, 80.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23080/27749 [04:45<00:57, 80.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23100/27749 [04:45<00:57, 80.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23120/27749 [04:45<00:57, 81.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23140/27749 [04:45<00:56, 81.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  83% 23160/27749 [04:45<00:56, 81.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23180/27749 [04:45<00:56, 81.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23200/27749 [04:45<00:56, 81.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23220/27749 [04:45<00:55, 81.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23240/27749 [04:45<00:55, 81.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23260/27749 [04:45<00:55, 81.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23280/27749 [04:46<00:54, 81.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23300/27749 [04:46<00:54, 81.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23320/27749 [04:46<00:54, 81.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23340/27749 [04:46<00:54, 81.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23360/27749 [04:46<00:53, 81.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23380/27749 [04:46<00:53, 81.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23400/27749 [04:46<00:53, 81.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23420/27749 [04:46<00:53, 81.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  84% 23440/27749 [04:46<00:52, 81.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23460/27749 [04:46<00:52, 81.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23480/27749 [04:47<00:52, 81.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23500/27749 [04:47<00:51, 81.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23520/27749 [04:47<00:51, 81.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23540/27749 [04:47<00:51, 81.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23560/27749 [04:47<00:51, 81.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23580/27749 [04:47<00:50, 82.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23600/27749 [04:47<00:50, 82.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23620/27749 [04:47<00:50, 82.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23640/27749 [04:47<00:50, 82.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23660/27749 [04:47<00:49, 82.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23680/27749 [04:47<00:49, 82.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23700/27749 [04:48<00:49, 82.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  85% 23720/27749 [04:48<00:48, 82.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23740/27749 [04:48<00:48, 82.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23760/27749 [04:48<00:48, 82.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23780/27749 [04:48<00:48, 82.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23800/27749 [04:48<00:47, 82.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23820/27749 [04:48<00:47, 82.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23840/27749 [04:48<00:47, 82.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23860/27749 [04:48<00:47, 82.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23880/27749 [04:48<00:46, 82.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23900/27749 [04:48<00:46, 82.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23920/27749 [04:49<00:46, 82.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23940/27749 [04:49<00:46, 82.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23960/27749 [04:49<00:45, 82.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 23980/27749 [04:49<00:45, 82.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  86% 24000/27749 [04:49<00:45, 82.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24020/27749 [04:49<00:44, 82.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24040/27749 [04:49<00:44, 83.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24060/27749 [04:49<00:44, 83.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24080/27749 [04:49<00:44, 83.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24100/27749 [04:49<00:43, 83.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24120/27749 [04:49<00:43, 83.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24140/27749 [04:50<00:43, 83.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24160/27749 [04:50<00:43, 83.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24180/27749 [04:50<00:42, 83.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24200/27749 [04:50<00:42, 83.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24220/27749 [04:50<00:42, 83.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24240/27749 [04:50<00:42, 83.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24260/27749 [04:50<00:41, 83.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  87% 24280/27749 [04:50<00:41, 83.53it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24300/27749 [04:50<00:41, 83.57it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24320/27749 [04:50<00:41, 83.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24340/27749 [04:50<00:40, 83.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24360/27749 [04:51<00:40, 83.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24380/27749 [04:51<00:40, 83.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24400/27749 [04:51<00:39, 83.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24420/27749 [04:51<00:39, 83.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24440/27749 [04:51<00:39, 83.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24460/27749 [04:51<00:39, 83.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24480/27749 [04:51<00:38, 83.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24500/27749 [04:51<00:38, 83.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24520/27749 [04:51<00:38, 84.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  88% 24540/27749 [04:51<00:38, 84.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24560/27749 [04:51<00:37, 84.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24580/27749 [04:52<00:37, 84.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24600/27749 [04:52<00:37, 84.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24620/27749 [04:52<00:37, 84.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24640/27749 [04:52<00:36, 84.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24660/27749 [04:52<00:36, 84.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24680/27749 [04:52<00:36, 84.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24700/27749 [04:52<00:36, 84.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24720/27749 [04:52<00:35, 84.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24740/27749 [04:52<00:35, 84.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24760/27749 [04:52<00:35, 84.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24780/27749 [04:52<00:35, 84.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24800/27749 [04:53<00:34, 84.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  89% 24820/27749 [04:53<00:34, 84.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 24840/27749 [04:53<00:34, 84.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 24860/27749 [04:53<00:34, 84.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 24880/27749 [04:53<00:33, 84.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 24900/27749 [04:53<00:33, 84.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 24920/27749 [04:53<00:33, 84.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 24940/27749 [04:53<00:33, 84.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 24960/27749 [04:53<00:32, 84.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 24980/27749 [04:53<00:32, 85.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 25000/27749 [04:53<00:32, 85.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 25020/27749 [04:54<00:32, 85.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 25040/27749 [04:54<00:31, 85.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 25060/27749 [04:54<00:31, 85.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 25080/27749 [04:54<00:31, 85.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  90% 25100/27749 [04:54<00:31, 85.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25120/27749 [04:54<00:30, 85.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25140/27749 [04:54<00:30, 85.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25160/27749 [04:54<00:30, 85.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25180/27749 [04:54<00:30, 85.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25200/27749 [04:54<00:29, 85.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25220/27749 [04:54<00:29, 85.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25240/27749 [04:55<00:29, 85.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25260/27749 [04:55<00:29, 85.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25280/27749 [04:55<00:28, 85.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25300/27749 [04:55<00:28, 85.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25320/27749 [04:55<00:28, 85.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25340/27749 [04:55<00:28, 85.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25360/27749 [04:55<00:27, 85.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  91% 25380/27749 [04:55<00:27, 85.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25400/27749 [04:55<00:27, 85.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25420/27749 [04:55<00:27, 85.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25440/27749 [04:55<00:26, 85.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25460/27749 [04:56<00:26, 85.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25480/27749 [04:56<00:26, 86.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25500/27749 [04:56<00:26, 86.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25520/27749 [04:56<00:25, 86.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25540/27749 [04:56<00:25, 86.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25560/27749 [04:56<00:25, 86.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25580/27749 [04:56<00:25, 86.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25600/27749 [04:56<00:24, 86.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25620/27749 [04:56<00:24, 86.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25640/27749 [04:56<00:24, 86.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  92% 25660/27749 [04:56<00:24, 86.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25680/27749 [04:57<00:23, 86.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25700/27749 [04:57<00:23, 86.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25720/27749 [04:57<00:23, 86.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25740/27749 [04:57<00:23, 86.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25760/27749 [04:57<00:22, 86.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25780/27749 [04:57<00:22, 86.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25800/27749 [04:57<00:22, 86.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25820/27749 [04:57<00:22, 86.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25840/27749 [04:57<00:22, 86.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25860/27749 [04:57<00:21, 86.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25880/27749 [04:58<00:21, 86.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25900/27749 [04:58<00:21, 86.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25920/27749 [04:58<00:21, 86.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  93% 25940/27749 [04:58<00:20, 86.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 25960/27749 [04:58<00:20, 86.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 25980/27749 [04:58<00:20, 87.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26000/27749 [04:58<00:20, 87.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26020/27749 [04:58<00:19, 87.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26040/27749 [04:58<00:19, 87.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26060/27749 [04:58<00:19, 87.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26080/27749 [04:58<00:19, 87.23it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26100/27749 [04:59<00:18, 87.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26120/27749 [04:59<00:18, 87.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26140/27749 [04:59<00:18, 87.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26160/27749 [04:59<00:18, 87.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26180/27749 [04:59<00:17, 87.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26200/27749 [04:59<00:17, 87.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  94% 26220/27749 [04:59<00:17, 87.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26240/27749 [04:59<00:17, 87.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26260/27749 [04:59<00:17, 87.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26280/27749 [04:59<00:16, 87.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26300/27749 [05:00<00:16, 87.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26320/27749 [05:00<00:16, 87.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26340/27749 [05:00<00:16, 87.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26360/27749 [05:00<00:15, 87.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26380/27749 [05:00<00:15, 87.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26400/27749 [05:00<00:15, 87.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26420/27749 [05:00<00:15, 87.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26440/27749 [05:00<00:14, 87.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26460/27749 [05:00<00:14, 87.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26480/27749 [05:00<00:14, 88.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  95% 26500/27749 [05:00<00:14, 88.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26520/27749 [05:01<00:13, 88.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26540/27749 [05:01<00:13, 88.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26560/27749 [05:01<00:13, 88.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26580/27749 [05:01<00:13, 88.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26600/27749 [05:01<00:13, 88.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26620/27749 [05:01<00:12, 88.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26640/27749 [05:01<00:12, 88.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26660/27749 [05:01<00:12, 88.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26680/27749 [05:01<00:12, 88.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26700/27749 [05:01<00:11, 88.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26720/27749 [05:01<00:11, 88.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26740/27749 [05:02<00:11, 88.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  96% 26760/27749 [05:02<00:11, 88.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26780/27749 [05:02<00:10, 88.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26800/27749 [05:02<00:10, 88.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26820/27749 [05:02<00:10, 88.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26840/27749 [05:02<00:10, 88.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26860/27749 [05:02<00:10, 88.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26880/27749 [05:02<00:09, 88.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26900/27749 [05:02<00:09, 88.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26920/27749 [05:02<00:09, 88.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26940/27749 [05:02<00:09, 88.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26960/27749 [05:03<00:08, 88.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 26980/27749 [05:03<00:08, 89.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 27000/27749 [05:03<00:08, 89.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 27020/27749 [05:03<00:08, 89.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  97% 27040/27749 [05:03<00:07, 89.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27060/27749 [05:03<00:07, 89.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27080/27749 [05:03<00:07, 89.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27100/27749 [05:03<00:07, 89.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27120/27749 [05:03<00:07, 89.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27140/27749 [05:03<00:06, 89.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27160/27749 [05:03<00:06, 89.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27180/27749 [05:04<00:06, 89.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27200/27749 [05:04<00:06, 89.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27220/27749 [05:04<00:05, 89.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27240/27749 [05:04<00:05, 89.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27260/27749 [05:04<00:05, 89.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27280/27749 [05:04<00:05, 89.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27300/27749 [05:04<00:05, 89.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  98% 27320/27749 [05:04<00:04, 89.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27340/27749 [05:04<00:04, 89.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27360/27749 [05:04<00:04, 89.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27380/27749 [05:04<00:04, 89.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27400/27749 [05:05<00:03, 89.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27420/27749 [05:05<00:03, 89.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27440/27749 [05:05<00:03, 89.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27460/27749 [05:05<00:03, 89.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27480/27749 [05:05<00:02, 89.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27500/27749 [05:05<00:02, 90.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27520/27749 [05:05<00:02, 90.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27540/27749 [05:05<00:02, 90.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27560/27749 [05:05<00:02, 90.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27580/27749 [05:05<00:01, 90.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8:  99% 27600/27749 [05:05<00:01, 90.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8: 100% 27620/27749 [05:06<00:01, 90.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8: 100% 27640/27749 [05:06<00:01, 90.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8: 100% 27660/27749 [05:06<00:00, 90.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8: 100% 27680/27749 [05:06<00:00, 90.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8: 100% 27700/27749 [05:06<00:00, 90.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8: 100% 27720/27749 [05:06<00:00, 90.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8: 100% 27740/27749 [05:06<00:00, 90.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 8: 100% 27749/27749 [05:06<00:00, 90.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 9:  80% 22180/27749 [04:39<01:10, 79.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  80% 22200/27749 [04:42<01:10, 78.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  80% 22220/27749 [04:42<01:10, 78.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  80% 22240/27749 [04:42<01:10, 78.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  80% 22260/27749 [04:42<01:09, 78.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  80% 22280/27749 [04:42<01:09, 78.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  80% 22300/27749 [04:42<01:09, 78.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  80% 22320/27749 [04:43<01:08, 78.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22340/27749 [04:43<01:08, 78.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22360/27749 [04:43<01:08, 78.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22380/27749 [04:43<01:07, 78.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22400/27749 [04:43<01:07, 79.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22420/27749 [04:43<01:07, 79.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22440/27749 [04:43<01:07, 79.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22460/27749 [04:43<01:06, 79.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22480/27749 [04:43<01:06, 79.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22500/27749 [04:43<01:06, 79.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22520/27749 [04:44<01:05, 79.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22540/27749 [04:44<01:05, 79.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22560/27749 [04:44<01:05, 79.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22580/27749 [04:44<01:05, 79.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  81% 22600/27749 [04:44<01:04, 79.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22620/27749 [04:44<01:04, 79.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22640/27749 [04:44<01:04, 79.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22660/27749 [04:44<01:03, 79.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22680/27749 [04:44<01:03, 79.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22700/27749 [04:44<01:03, 79.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22720/27749 [04:44<01:03, 79.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22740/27749 [04:45<01:02, 79.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22760/27749 [04:45<01:02, 79.81it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22780/27749 [04:45<01:02, 79.85it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22800/27749 [04:45<01:01, 79.89it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22820/27749 [04:45<01:01, 79.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22840/27749 [04:45<01:01, 79.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22860/27749 [04:45<01:01, 80.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  82% 22880/27749 [04:45<01:00, 80.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 22900/27749 [04:45<01:00, 80.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 22920/27749 [04:45<01:00, 80.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 22940/27749 [04:46<00:59, 80.19it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 22960/27749 [04:46<00:59, 80.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 22980/27749 [04:46<00:59, 80.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23000/27749 [04:46<00:59, 80.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23020/27749 [04:46<00:58, 80.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23040/27749 [04:46<00:58, 80.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23060/27749 [04:46<00:58, 80.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23080/27749 [04:46<00:58, 80.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23100/27749 [04:46<00:57, 80.53it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23120/27749 [04:46<00:57, 80.57it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23140/27749 [04:47<00:57, 80.61it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  83% 23160/27749 [04:47<00:56, 80.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23180/27749 [04:47<00:56, 80.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23200/27749 [04:47<00:56, 80.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23220/27749 [04:47<00:56, 80.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23240/27749 [04:47<00:55, 80.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23260/27749 [04:47<00:55, 80.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23280/27749 [04:47<00:55, 80.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23300/27749 [04:47<00:54, 80.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23320/27749 [04:47<00:54, 80.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23340/27749 [04:48<00:54, 81.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23360/27749 [04:48<00:54, 81.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23380/27749 [04:48<00:53, 81.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23400/27749 [04:48<00:53, 81.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23420/27749 [04:48<00:53, 81.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  84% 23440/27749 [04:48<00:53, 81.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23460/27749 [04:48<00:52, 81.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23480/27749 [04:48<00:52, 81.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23500/27749 [04:48<00:52, 81.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23520/27749 [04:48<00:51, 81.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23540/27749 [04:48<00:51, 81.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23560/27749 [04:49<00:51, 81.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23580/27749 [04:49<00:51, 81.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23600/27749 [04:49<00:50, 81.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23620/27749 [04:49<00:50, 81.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23640/27749 [04:49<00:50, 81.69it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23660/27749 [04:49<00:50, 81.73it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23680/27749 [04:49<00:49, 81.77it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23700/27749 [04:49<00:49, 81.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  85% 23720/27749 [04:49<00:49, 81.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23740/27749 [04:49<00:48, 81.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23760/27749 [04:49<00:48, 81.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23780/27749 [04:50<00:48, 81.99it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23800/27749 [04:50<00:48, 82.03it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23820/27749 [04:50<00:47, 82.07it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23840/27749 [04:50<00:47, 82.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23860/27749 [04:50<00:47, 82.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23880/27749 [04:50<00:47, 82.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23900/27749 [04:50<00:46, 82.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23920/27749 [04:50<00:46, 82.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23940/27749 [04:50<00:46, 82.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23960/27749 [04:50<00:45, 82.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 23980/27749 [04:50<00:45, 82.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  86% 24000/27749 [04:51<00:45, 82.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24020/27749 [04:51<00:45, 82.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24040/27749 [04:51<00:44, 82.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24060/27749 [04:51<00:44, 82.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24080/27749 [04:51<00:44, 82.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24100/27749 [04:51<00:44, 82.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24120/27749 [04:51<00:43, 82.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24140/27749 [04:51<00:43, 82.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24160/27749 [04:51<00:43, 82.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24180/27749 [04:51<00:43, 82.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24200/27749 [04:51<00:42, 82.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24220/27749 [04:52<00:42, 82.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24240/27749 [04:52<00:42, 82.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24260/27749 [04:52<00:42, 83.01it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  87% 24280/27749 [04:52<00:41, 83.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24300/27749 [04:52<00:41, 83.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24320/27749 [04:52<00:41, 83.14it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24340/27749 [04:52<00:40, 83.18it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24360/27749 [04:52<00:40, 83.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24380/27749 [04:52<00:40, 83.27it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24400/27749 [04:52<00:40, 83.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24420/27749 [04:52<00:39, 83.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24440/27749 [04:53<00:39, 83.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24460/27749 [04:53<00:39, 83.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24480/27749 [04:53<00:39, 83.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24500/27749 [04:53<00:38, 83.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24520/27749 [04:53<00:38, 83.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  88% 24540/27749 [04:53<00:38, 83.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24560/27749 [04:53<00:38, 83.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24580/27749 [04:53<00:37, 83.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24600/27749 [04:53<00:37, 83.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24620/27749 [04:53<00:37, 83.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24640/27749 [04:54<00:37, 83.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24660/27749 [04:54<00:36, 83.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24680/27749 [04:54<00:36, 83.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24700/27749 [04:54<00:36, 83.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24720/27749 [04:54<00:36, 83.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24740/27749 [04:54<00:35, 84.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24760/27749 [04:54<00:35, 84.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24780/27749 [04:54<00:35, 84.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24800/27749 [04:54<00:35, 84.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  89% 24820/27749 [04:54<00:34, 84.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 24840/27749 [04:54<00:34, 84.22it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 24860/27749 [04:55<00:34, 84.26it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 24880/27749 [04:55<00:34, 84.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 24900/27749 [04:55<00:33, 84.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 24920/27749 [04:55<00:33, 84.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 24940/27749 [04:55<00:33, 84.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 24960/27749 [04:55<00:33, 84.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 24980/27749 [04:55<00:32, 84.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 25000/27749 [04:55<00:32, 84.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 25020/27749 [04:55<00:32, 84.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 25040/27749 [04:55<00:32, 84.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 25060/27749 [04:55<00:31, 84.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 25080/27749 [04:56<00:31, 84.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  90% 25100/27749 [04:56<00:31, 84.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25120/27749 [04:56<00:31, 84.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25140/27749 [04:56<00:30, 84.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25160/27749 [04:56<00:30, 84.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25180/27749 [04:56<00:30, 84.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25200/27749 [04:56<00:30, 84.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25220/27749 [04:56<00:29, 85.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25240/27749 [04:56<00:29, 85.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25260/27749 [04:56<00:29, 85.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25280/27749 [04:56<00:29, 85.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25300/27749 [04:57<00:28, 85.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25320/27749 [04:57<00:28, 85.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25340/27749 [04:57<00:28, 85.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25360/27749 [04:57<00:28, 85.29it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  91% 25380/27749 [04:57<00:27, 85.33it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25400/27749 [04:57<00:27, 85.37it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25420/27749 [04:57<00:27, 85.41it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25440/27749 [04:57<00:27, 85.45it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25460/27749 [04:57<00:26, 85.49it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25480/27749 [04:57<00:26, 85.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25500/27749 [04:57<00:26, 85.58it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25520/27749 [04:58<00:26, 85.62it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25540/27749 [04:58<00:25, 85.66it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25560/27749 [04:58<00:25, 85.70it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25580/27749 [04:58<00:25, 85.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25600/27749 [04:58<00:25, 85.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25620/27749 [04:58<00:24, 85.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25640/27749 [04:58<00:24, 85.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  92% 25660/27749 [04:58<00:24, 85.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25680/27749 [04:58<00:24, 85.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25700/27749 [04:58<00:23, 85.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25720/27749 [04:59<00:23, 86.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25740/27749 [04:59<00:23, 86.06it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25760/27749 [04:59<00:23, 86.10it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25780/27749 [04:59<00:22, 86.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25800/27749 [04:59<00:22, 86.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25820/27749 [04:59<00:22, 86.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25840/27749 [04:59<00:22, 86.25it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25860/27749 [04:59<00:21, 86.30it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25880/27749 [04:59<00:21, 86.34it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25900/27749 [04:59<00:21, 86.38it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25920/27749 [04:59<00:21, 86.42it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  93% 25940/27749 [05:00<00:20, 86.46it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 25960/27749 [05:00<00:20, 86.50it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 25980/27749 [05:00<00:20, 86.54it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26000/27749 [05:00<00:20, 86.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26020/27749 [05:00<00:19, 86.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26040/27749 [05:00<00:19, 86.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26060/27749 [05:00<00:19, 86.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26080/27749 [05:00<00:19, 86.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26100/27749 [05:00<00:19, 86.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26120/27749 [05:00<00:18, 86.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26140/27749 [05:00<00:18, 86.87it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26160/27749 [05:00<00:18, 86.91it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26180/27749 [05:01<00:18, 86.95it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26200/27749 [05:01<00:17, 87.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  94% 26220/27749 [05:01<00:17, 87.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26240/27749 [05:01<00:17, 87.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26260/27749 [05:01<00:17, 87.12it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26280/27749 [05:01<00:16, 87.16it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26300/27749 [05:01<00:16, 87.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26320/27749 [05:01<00:16, 87.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26340/27749 [05:01<00:16, 87.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26360/27749 [05:01<00:15, 87.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26380/27749 [05:01<00:15, 87.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26400/27749 [05:02<00:15, 87.39it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26420/27749 [05:02<00:15, 87.43it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26440/27749 [05:02<00:14, 87.47it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26460/27749 [05:02<00:14, 87.52it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26480/27749 [05:02<00:14, 87.56it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  95% 26500/27749 [05:02<00:14, 87.60it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26520/27749 [05:02<00:14, 87.64it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26540/27749 [05:02<00:13, 87.68it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26560/27749 [05:02<00:13, 87.72it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26580/27749 [05:02<00:13, 87.76it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26600/27749 [05:02<00:13, 87.80it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26620/27749 [05:03<00:12, 87.84it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26640/27749 [05:03<00:12, 87.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26660/27749 [05:03<00:12, 87.92it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26680/27749 [05:03<00:12, 87.96it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26700/27749 [05:03<00:11, 88.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26720/27749 [05:03<00:11, 88.04it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26740/27749 [05:03<00:11, 88.08it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  96% 26760/27749 [05:03<00:11, 88.11it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26780/27749 [05:03<00:10, 88.15it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26800/27749 [05:03<00:10, 88.20it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26820/27749 [05:03<00:10, 88.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26840/27749 [05:04<00:10, 88.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26860/27749 [05:04<00:10, 88.31it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26880/27749 [05:04<00:09, 88.35it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26900/27749 [05:04<00:09, 88.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26920/27749 [05:04<00:09, 88.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26940/27749 [05:04<00:09, 88.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26960/27749 [05:04<00:08, 88.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 26980/27749 [05:04<00:08, 88.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 27000/27749 [05:04<00:08, 88.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 27020/27749 [05:04<00:08, 88.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  97% 27040/27749 [05:04<00:07, 88.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27060/27749 [05:05<00:07, 88.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27080/27749 [05:05<00:07, 88.75it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27100/27749 [05:05<00:07, 88.79it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27120/27749 [05:05<00:07, 88.83it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27140/27749 [05:05<00:06, 88.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27160/27749 [05:05<00:06, 88.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27180/27749 [05:05<00:06, 88.94it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27200/27749 [05:05<00:06, 88.98it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27220/27749 [05:05<00:05, 89.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27240/27749 [05:05<00:05, 89.05it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27260/27749 [05:05<00:05, 89.09it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27280/27749 [05:06<00:05, 89.13it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27300/27749 [05:06<00:05, 89.17it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  98% 27320/27749 [05:06<00:04, 89.21it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27340/27749 [05:06<00:04, 89.24it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27360/27749 [05:06<00:04, 89.28it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27380/27749 [05:06<00:04, 89.32it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27400/27749 [05:06<00:03, 89.36it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27420/27749 [05:06<00:03, 89.40it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27440/27749 [05:06<00:03, 89.44it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27460/27749 [05:06<00:03, 89.48it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27480/27749 [05:06<00:03, 89.51it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27500/27749 [05:07<00:02, 89.55it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27520/27749 [05:07<00:02, 89.59it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27540/27749 [05:07<00:02, 89.63it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27560/27749 [05:07<00:02, 89.67it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27580/27749 [05:07<00:01, 89.71it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9:  99% 27600/27749 [05:07<00:01, 89.74it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9: 100% 27620/27749 [05:07<00:01, 89.78it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9: 100% 27640/27749 [05:07<00:01, 89.82it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9: 100% 27660/27749 [05:07<00:00, 89.86it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9: 100% 27680/27749 [05:07<00:00, 89.90it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9: 100% 27700/27749 [05:08<00:00, 89.93it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9: 100% 27720/27749 [05:08<00:00, 89.97it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9: 100% 27740/27749 [05:08<00:00, 90.00it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 9: 100% 27749/27749 [05:08<00:00, 90.02it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 10:  80% 22180/27749 [04:39<01:10, 79.44it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  80% 22200/27749 [04:42<01:10, 78.54it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  80% 22220/27749 [04:42<01:10, 78.59it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  80% 22240/27749 [04:42<01:10, 78.63it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  80% 22260/27749 [04:42<01:09, 78.67it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  80% 22280/27749 [04:43<01:09, 78.72it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  80% 22300/27749 [04:43<01:09, 78.76it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  80% 22320/27749 [04:43<01:08, 78.80it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22340/27749 [04:43<01:08, 78.85it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22360/27749 [04:43<01:08, 78.89it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22380/27749 [04:43<01:08, 78.94it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22400/27749 [04:43<01:07, 78.98it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22420/27749 [04:43<01:07, 79.02it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22440/27749 [04:43<01:07, 79.07it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22460/27749 [04:43<01:06, 79.11it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22480/27749 [04:44<01:06, 79.15it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22500/27749 [04:44<01:06, 79.20it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22520/27749 [04:44<01:05, 79.24it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22540/27749 [04:44<01:05, 79.28it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22560/27749 [04:44<01:05, 79.33it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22580/27749 [04:44<01:05, 79.37it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  81% 22600/27749 [04:44<01:04, 79.41it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22620/27749 [04:44<01:04, 79.46it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22640/27749 [04:44<01:04, 79.50it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22660/27749 [04:44<01:03, 79.54it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22680/27749 [04:44<01:03, 79.59it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22700/27749 [04:45<01:03, 79.63it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22720/27749 [04:45<01:03, 79.68it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22740/27749 [04:45<01:02, 79.72it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22760/27749 [04:45<01:02, 79.76it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22780/27749 [04:45<01:02, 79.81it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22800/27749 [04:45<01:01, 79.85it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22820/27749 [04:45<01:01, 79.89it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22840/27749 [04:45<01:01, 79.93it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22860/27749 [04:45<01:01, 79.98it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  82% 22880/27749 [04:45<01:00, 80.02it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 22900/27749 [04:46<01:00, 80.06it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 22920/27749 [04:46<01:00, 80.11it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 22940/27749 [04:46<00:59, 80.15it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 22960/27749 [04:46<00:59, 80.19it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 22980/27749 [04:46<00:59, 80.24it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23000/27749 [04:46<00:59, 80.28it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23020/27749 [04:46<00:58, 80.32it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23040/27749 [04:46<00:58, 80.37it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23060/27749 [04:46<00:58, 80.41it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23080/27749 [04:46<00:58, 80.45it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23100/27749 [04:46<00:57, 80.50it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23120/27749 [04:47<00:57, 80.54it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23140/27749 [04:47<00:57, 80.59it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  83% 23160/27749 [04:47<00:56, 80.63it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23180/27749 [04:47<00:56, 80.68it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23200/27749 [04:47<00:56, 80.72it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23220/27749 [04:47<00:56, 80.76it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23240/27749 [04:47<00:55, 80.81it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23260/27749 [04:47<00:55, 80.85it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23280/27749 [04:47<00:55, 80.89it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23300/27749 [04:47<00:54, 80.94it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23320/27749 [04:47<00:54, 80.98it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23340/27749 [04:48<00:54, 81.02it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23360/27749 [04:48<00:54, 81.06it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23380/27749 [04:48<00:53, 81.10it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23400/27749 [04:48<00:53, 81.15it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23420/27749 [04:48<00:53, 81.19it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  84% 23440/27749 [04:48<00:53, 81.23it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23460/27749 [04:48<00:52, 81.27it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23480/27749 [04:48<00:52, 81.32it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23500/27749 [04:48<00:52, 81.36it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23520/27749 [04:48<00:51, 81.40it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23540/27749 [04:49<00:51, 81.45it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23560/27749 [04:49<00:51, 81.49it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23580/27749 [04:49<00:51, 81.54it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23600/27749 [04:49<00:50, 81.58it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23620/27749 [04:49<00:50, 81.62it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23640/27749 [04:49<00:50, 81.66it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23660/27749 [04:49<00:50, 81.70it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23680/27749 [04:49<00:49, 81.75it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23700/27749 [04:49<00:49, 81.79it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  85% 23720/27749 [04:49<00:49, 81.83it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23740/27749 [04:49<00:48, 81.87it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23760/27749 [04:50<00:48, 81.92it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23780/27749 [04:50<00:48, 81.96it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23800/27749 [04:50<00:48, 82.00it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23820/27749 [04:50<00:47, 82.04it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23840/27749 [04:50<00:47, 82.09it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23860/27749 [04:50<00:47, 82.13it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23880/27749 [04:50<00:47, 82.17it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23900/27749 [04:50<00:46, 82.22it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23920/27749 [04:50<00:46, 82.26it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23940/27749 [04:50<00:46, 82.30it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23960/27749 [04:50<00:46, 82.34it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 23980/27749 [04:51<00:45, 82.39it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  86% 24000/27749 [04:51<00:45, 82.43it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24020/27749 [04:51<00:45, 82.47it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24040/27749 [04:51<00:44, 82.51it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24060/27749 [04:51<00:44, 82.56it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24080/27749 [04:51<00:44, 82.60it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24100/27749 [04:51<00:44, 82.64it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24120/27749 [04:51<00:43, 82.68it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24140/27749 [04:51<00:43, 82.72it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24160/27749 [04:51<00:43, 82.77it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24180/27749 [04:52<00:43, 82.80it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24200/27749 [04:52<00:42, 82.84it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24220/27749 [04:52<00:42, 82.88it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24240/27749 [04:52<00:42, 82.92it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24260/27749 [04:52<00:42, 82.97it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  87% 24280/27749 [04:52<00:41, 83.01it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24300/27749 [04:52<00:41, 83.05it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24320/27749 [04:52<00:41, 83.09it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24340/27749 [04:52<00:41, 83.14it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24360/27749 [04:52<00:40, 83.18it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24380/27749 [04:52<00:40, 83.22it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24400/27749 [04:53<00:40, 83.27it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24420/27749 [04:53<00:39, 83.31it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24440/27749 [04:53<00:39, 83.35it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24460/27749 [04:53<00:39, 83.40it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24480/27749 [04:53<00:39, 83.44it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24500/27749 [04:53<00:38, 83.48it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24520/27749 [04:53<00:38, 83.52it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  88% 24540/27749 [04:53<00:38, 83.56it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24560/27749 [04:53<00:38, 83.60it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24580/27749 [04:53<00:37, 83.65it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24600/27749 [04:53<00:37, 83.69it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24620/27749 [04:54<00:37, 83.73it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24640/27749 [04:54<00:37, 83.77it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24660/27749 [04:54<00:36, 83.82it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24680/27749 [04:54<00:36, 83.86it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24700/27749 [04:54<00:36, 83.90it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24720/27749 [04:54<00:36, 83.94it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24740/27749 [04:54<00:35, 83.98it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24760/27749 [04:54<00:35, 84.02it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24780/27749 [04:54<00:35, 84.06it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24800/27749 [04:54<00:35, 84.10it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  89% 24820/27749 [04:54<00:34, 84.14it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 24840/27749 [04:55<00:34, 84.18it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 24860/27749 [04:55<00:34, 84.22it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 24880/27749 [04:55<00:34, 84.26it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 24900/27749 [04:55<00:33, 84.30it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 24920/27749 [04:55<00:33, 84.34it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 24940/27749 [04:55<00:33, 84.38it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 24960/27749 [04:55<00:33, 84.42it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 24980/27749 [04:55<00:32, 84.46it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 25000/27749 [04:55<00:32, 84.50it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 25020/27749 [04:55<00:32, 84.54it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 25040/27749 [04:56<00:32, 84.58it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 25060/27749 [04:56<00:31, 84.62it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 25080/27749 [04:56<00:31, 84.66it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  90% 25100/27749 [04:56<00:31, 84.70it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25120/27749 [04:56<00:31, 84.74it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25140/27749 [04:56<00:30, 84.78it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25160/27749 [04:56<00:30, 84.82it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25180/27749 [04:56<00:30, 84.86it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25200/27749 [04:56<00:30, 84.90it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25220/27749 [04:56<00:29, 84.94it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25240/27749 [04:57<00:29, 84.98it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25260/27749 [04:57<00:29, 85.02it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25280/27749 [04:57<00:29, 85.06it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25300/27749 [04:57<00:28, 85.10it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25320/27749 [04:57<00:28, 85.14it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25340/27749 [04:57<00:28, 85.17it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25360/27749 [04:57<00:28, 85.21it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  91% 25380/27749 [04:57<00:27, 85.25it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25400/27749 [04:57<00:27, 85.29it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25420/27749 [04:57<00:27, 85.33it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25440/27749 [04:57<00:27, 85.37it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25460/27749 [04:58<00:26, 85.41it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25480/27749 [04:58<00:26, 85.45it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25500/27749 [04:58<00:26, 85.49it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25520/27749 [04:58<00:26, 85.53it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25540/27749 [04:58<00:25, 85.58it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25560/27749 [04:58<00:25, 85.62it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25580/27749 [04:58<00:25, 85.66it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25600/27749 [04:58<00:25, 85.70it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25620/27749 [04:58<00:24, 85.73it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25640/27749 [04:58<00:24, 85.78it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  92% 25660/27749 [04:59<00:24, 85.81it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25680/27749 [04:59<00:24, 85.85it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25700/27749 [04:59<00:23, 85.89it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25720/27749 [04:59<00:23, 85.93it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25740/27749 [04:59<00:23, 85.97it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25760/27749 [04:59<00:23, 86.01it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25780/27749 [04:59<00:22, 86.05it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25800/27749 [04:59<00:22, 86.09it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25820/27749 [04:59<00:22, 86.12it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25840/27749 [04:59<00:22, 86.16it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25860/27749 [04:59<00:21, 86.20it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25880/27749 [05:00<00:21, 86.24it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25900/27749 [05:00<00:21, 86.29it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25920/27749 [05:00<00:21, 86.33it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  93% 25940/27749 [05:00<00:20, 86.37it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 25960/27749 [05:00<00:20, 86.41it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 25980/27749 [05:00<00:20, 86.45it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26000/27749 [05:00<00:20, 86.49it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26020/27749 [05:00<00:19, 86.53it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26040/27749 [05:00<00:19, 86.58it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26060/27749 [05:00<00:19, 86.62it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26080/27749 [05:00<00:19, 86.65it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26100/27749 [05:01<00:19, 86.69it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26120/27749 [05:01<00:18, 86.73it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26140/27749 [05:01<00:18, 86.77it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26160/27749 [05:01<00:18, 86.81it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26180/27749 [05:01<00:18, 86.85it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26200/27749 [05:01<00:17, 86.89it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  94% 26220/27749 [05:01<00:17, 86.93it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26240/27749 [05:01<00:17, 86.97it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26260/27749 [05:01<00:17, 87.02it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26280/27749 [05:01<00:16, 87.06it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26300/27749 [05:01<00:16, 87.10it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26320/27749 [05:02<00:16, 87.14it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26340/27749 [05:02<00:16, 87.18it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26360/27749 [05:02<00:15, 87.22it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26380/27749 [05:02<00:15, 87.26it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26400/27749 [05:02<00:15, 87.30it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26420/27749 [05:02<00:15, 87.34it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26440/27749 [05:02<00:14, 87.38it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26460/27749 [05:02<00:14, 87.42it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26480/27749 [05:02<00:14, 87.46it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  95% 26500/27749 [05:02<00:14, 87.50it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26520/27749 [05:02<00:14, 87.54it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26540/27749 [05:03<00:13, 87.58it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26560/27749 [05:03<00:13, 87.62it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26580/27749 [05:03<00:13, 87.66it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26600/27749 [05:03<00:13, 87.70it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26620/27749 [05:03<00:12, 87.74it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26640/27749 [05:03<00:12, 87.78it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26660/27749 [05:03<00:12, 87.82it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26680/27749 [05:03<00:12, 87.86it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26700/27749 [05:03<00:11, 87.90it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26720/27749 [05:03<00:11, 87.94it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26740/27749 [05:03<00:11, 87.98it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  96% 26760/27749 [05:04<00:11, 88.02it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26780/27749 [05:04<00:11, 88.06it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26800/27749 [05:04<00:10, 88.10it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26820/27749 [05:04<00:10, 88.14it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26840/27749 [05:04<00:10, 88.18it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26860/27749 [05:04<00:10, 88.22it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26880/27749 [05:04<00:09, 88.26it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26900/27749 [05:04<00:09, 88.30it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26920/27749 [05:04<00:09, 88.33it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26940/27749 [05:04<00:09, 88.37it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26960/27749 [05:04<00:08, 88.40it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 26980/27749 [05:05<00:08, 88.44it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 27000/27749 [05:05<00:08, 88.48it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 27020/27749 [05:05<00:08, 88.52it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  97% 27040/27749 [05:05<00:08, 88.56it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27060/27749 [05:05<00:07, 88.60it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27080/27749 [05:05<00:07, 88.64it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27100/27749 [05:05<00:07, 88.67it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27120/27749 [05:05<00:07, 88.71it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27140/27749 [05:05<00:06, 88.75it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27160/27749 [05:05<00:06, 88.78it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27180/27749 [05:06<00:06, 88.82it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27200/27749 [05:06<00:06, 88.85it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27220/27749 [05:06<00:05, 88.89it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27240/27749 [05:06<00:05, 88.92it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27260/27749 [05:06<00:05, 88.96it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27280/27749 [05:06<00:05, 89.00it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27300/27749 [05:06<00:05, 89.03it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  98% 27320/27749 [05:06<00:04, 89.07it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27340/27749 [05:06<00:04, 89.11it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27360/27749 [05:06<00:04, 89.15it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27380/27749 [05:06<00:04, 89.19it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27400/27749 [05:07<00:03, 89.22it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27420/27749 [05:07<00:03, 89.26it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27440/27749 [05:07<00:03, 89.30it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27460/27749 [05:07<00:03, 89.34it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27480/27749 [05:07<00:03, 89.38it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27500/27749 [05:07<00:02, 89.42it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27520/27749 [05:07<00:02, 89.46it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27540/27749 [05:07<00:02, 89.50it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27560/27749 [05:07<00:02, 89.54it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27580/27749 [05:07<00:01, 89.57it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10:  99% 27600/27749 [05:07<00:01, 89.61it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10: 100% 27620/27749 [05:08<00:01, 89.65it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10: 100% 27640/27749 [05:08<00:01, 89.69it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10: 100% 27660/27749 [05:08<00:00, 89.73it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10: 100% 27680/27749 [05:08<00:00, 89.76it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10: 100% 27700/27749 [05:08<00:00, 89.80it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10: 100% 27720/27749 [05:08<00:00, 89.83it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10: 100% 27740/27749 [05:08<00:00, 89.87it/s, loss=2.88, v_num=0, train_loss_step=2.870, val_loss=2.740, avg_val_loss=2.740, train_loss_epoch=2.880]\n",
            "Epoch 10: 100% 27749/27749 [05:08<00:00, 89.88it/s, loss=2.88, v_num=0, train_loss_step=2.880, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 11:  80% 22180/27749 [04:37<01:09, 79.97it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  80% 22200/27749 [04:40<01:10, 79.06it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  80% 22220/27749 [04:40<01:09, 79.11it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  80% 22240/27749 [04:40<01:09, 79.15it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  80% 22260/27749 [04:41<01:09, 79.20it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  80% 22280/27749 [04:41<01:09, 79.24it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  80% 22300/27749 [04:41<01:08, 79.28it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  80% 22320/27749 [04:41<01:08, 79.33it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22340/27749 [04:41<01:08, 79.37it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22360/27749 [04:41<01:07, 79.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22380/27749 [04:41<01:07, 79.46it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22400/27749 [04:41<01:07, 79.50it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22420/27749 [04:41<01:06, 79.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22440/27749 [04:41<01:06, 79.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22460/27749 [04:42<01:06, 79.64it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22480/27749 [04:42<01:06, 79.68it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22500/27749 [04:42<01:05, 79.73it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22520/27749 [04:42<01:05, 79.77it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22540/27749 [04:42<01:05, 79.82it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22560/27749 [04:42<01:04, 79.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22580/27749 [04:42<01:04, 79.91it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  81% 22600/27749 [04:42<01:04, 79.95it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22620/27749 [04:42<01:04, 80.00it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22640/27749 [04:42<01:03, 80.04it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22660/27749 [04:42<01:03, 80.09it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22680/27749 [04:43<01:03, 80.13it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22700/27749 [04:43<01:02, 80.18it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22720/27749 [04:43<01:02, 80.22it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22740/27749 [04:43<01:02, 80.27it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22760/27749 [04:43<01:02, 80.31it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22780/27749 [04:43<01:01, 80.36it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22800/27749 [04:43<01:01, 80.40it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22820/27749 [04:43<01:01, 80.44it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22840/27749 [04:43<01:00, 80.48it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22860/27749 [04:43<01:00, 80.52it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  82% 22880/27749 [04:43<01:00, 80.57it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 22900/27749 [04:44<01:00, 80.61it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 22920/27749 [04:44<00:59, 80.65it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 22940/27749 [04:44<00:59, 80.69it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 22960/27749 [04:44<00:59, 80.74it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 22980/27749 [04:44<00:59, 80.78it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23000/27749 [04:44<00:58, 80.82it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23020/27749 [04:44<00:58, 80.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23040/27749 [04:44<00:58, 80.91it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23060/27749 [04:44<00:57, 80.95it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23080/27749 [04:44<00:57, 80.99it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23100/27749 [04:45<00:57, 81.04it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23120/27749 [04:45<00:57, 81.08it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23140/27749 [04:45<00:56, 81.12it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  83% 23160/27749 [04:45<00:56, 81.16it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23180/27749 [04:45<00:56, 81.21it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23200/27749 [04:45<00:55, 81.25it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23220/27749 [04:45<00:55, 81.29it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23240/27749 [04:45<00:55, 81.33it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23260/27749 [04:45<00:55, 81.37it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23280/27749 [04:45<00:54, 81.41it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23300/27749 [04:46<00:54, 81.45it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23320/27749 [04:46<00:54, 81.49it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23340/27749 [04:46<00:54, 81.54it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23360/27749 [04:46<00:53, 81.58it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23380/27749 [04:46<00:53, 81.62it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23400/27749 [04:46<00:53, 81.67it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23420/27749 [04:46<00:52, 81.71it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  84% 23440/27749 [04:46<00:52, 81.75it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23460/27749 [04:46<00:52, 81.80it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23480/27749 [04:46<00:52, 81.84it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23500/27749 [04:47<00:51, 81.88it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23520/27749 [04:47<00:51, 81.92it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23540/27749 [04:47<00:51, 81.97it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23560/27749 [04:47<00:51, 82.01it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23580/27749 [04:47<00:50, 82.05it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23600/27749 [04:47<00:50, 82.10it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23620/27749 [04:47<00:50, 82.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23640/27749 [04:47<00:50, 82.18it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23660/27749 [04:47<00:49, 82.22it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23680/27749 [04:47<00:49, 82.26it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23700/27749 [04:47<00:49, 82.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  85% 23720/27749 [04:48<00:48, 82.35it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23740/27749 [04:48<00:48, 82.39it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23760/27749 [04:48<00:48, 82.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23780/27749 [04:48<00:48, 82.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23800/27749 [04:48<00:47, 82.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23820/27749 [04:48<00:47, 82.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23840/27749 [04:48<00:47, 82.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23860/27749 [04:48<00:47, 82.64it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23880/27749 [04:48<00:46, 82.68it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23900/27749 [04:48<00:46, 82.72it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23920/27749 [04:49<00:46, 82.76it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23940/27749 [04:49<00:45, 82.81it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23960/27749 [04:49<00:45, 82.85it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 23980/27749 [04:49<00:45, 82.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  86% 24000/27749 [04:49<00:45, 82.94it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24020/27749 [04:49<00:44, 82.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24040/27749 [04:49<00:44, 83.03it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24060/27749 [04:49<00:44, 83.07it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24080/27749 [04:49<00:44, 83.11it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24100/27749 [04:49<00:43, 83.15it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24120/27749 [04:49<00:43, 83.20it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24140/27749 [04:49<00:43, 83.24it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24160/27749 [04:50<00:43, 83.29it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24180/27749 [04:50<00:42, 83.33it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24200/27749 [04:50<00:42, 83.37it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24220/27749 [04:50<00:42, 83.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24240/27749 [04:50<00:42, 83.46it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24260/27749 [04:50<00:41, 83.50it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  87% 24280/27749 [04:50<00:41, 83.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24300/27749 [04:50<00:41, 83.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24320/27749 [04:50<00:41, 83.63it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24340/27749 [04:50<00:40, 83.67it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24360/27749 [04:50<00:40, 83.71it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24380/27749 [04:51<00:40, 83.75it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24400/27749 [04:51<00:39, 83.80it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24420/27749 [04:51<00:39, 83.84it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24440/27749 [04:51<00:39, 83.88it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24460/27749 [04:51<00:39, 83.92it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24480/27749 [04:51<00:38, 83.96it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24500/27749 [04:51<00:38, 84.01it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24520/27749 [04:51<00:38, 84.05it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  88% 24540/27749 [04:51<00:38, 84.09it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24560/27749 [04:51<00:37, 84.13it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24580/27749 [04:52<00:37, 84.17it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24600/27749 [04:52<00:37, 84.21it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24620/27749 [04:52<00:37, 84.25it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24640/27749 [04:52<00:36, 84.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24660/27749 [04:52<00:36, 84.34it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24680/27749 [04:52<00:36, 84.38it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24700/27749 [04:52<00:36, 84.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24720/27749 [04:52<00:35, 84.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24740/27749 [04:52<00:35, 84.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24760/27749 [04:52<00:35, 84.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24780/27749 [04:52<00:35, 84.58it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24800/27749 [04:53<00:34, 84.62it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  89% 24820/27749 [04:53<00:34, 84.66it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 24840/27749 [04:53<00:34, 84.70it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 24860/27749 [04:53<00:34, 84.74it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 24880/27749 [04:53<00:33, 84.78it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 24900/27749 [04:53<00:33, 84.82it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 24920/27749 [04:53<00:33, 84.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 24940/27749 [04:53<00:33, 84.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 24960/27749 [04:53<00:32, 84.94it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 24980/27749 [04:53<00:32, 84.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 25000/27749 [04:54<00:32, 85.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 25020/27749 [04:54<00:32, 85.06it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 25040/27749 [04:54<00:31, 85.10it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 25060/27749 [04:54<00:31, 85.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 25080/27749 [04:54<00:31, 85.19it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  90% 25100/27749 [04:54<00:31, 85.23it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25120/27749 [04:54<00:30, 85.27it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25140/27749 [04:54<00:30, 85.31it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25160/27749 [04:54<00:30, 85.35it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25180/27749 [04:54<00:30, 85.39it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25200/27749 [04:54<00:29, 85.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25220/27749 [04:55<00:29, 85.48it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25240/27749 [04:55<00:29, 85.52it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25260/27749 [04:55<00:29, 85.56it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25280/27749 [04:55<00:28, 85.60it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25300/27749 [04:55<00:28, 85.65it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25320/27749 [04:55<00:28, 85.69it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25340/27749 [04:55<00:28, 85.73it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25360/27749 [04:55<00:27, 85.77it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  91% 25380/27749 [04:55<00:27, 85.81it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25400/27749 [04:55<00:27, 85.85it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25420/27749 [04:55<00:27, 85.89it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25440/27749 [04:56<00:26, 85.93it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25460/27749 [04:56<00:26, 85.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25480/27749 [04:56<00:26, 86.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25500/27749 [04:56<00:26, 86.06it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25520/27749 [04:56<00:25, 86.10it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25540/27749 [04:56<00:25, 86.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25560/27749 [04:56<00:25, 86.19it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25580/27749 [04:56<00:25, 86.23it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25600/27749 [04:56<00:24, 86.27it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25620/27749 [04:56<00:24, 86.31it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25640/27749 [04:56<00:24, 86.35it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  92% 25660/27749 [04:57<00:24, 86.39it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25680/27749 [04:57<00:23, 86.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25700/27749 [04:57<00:23, 86.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25720/27749 [04:57<00:23, 86.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25740/27749 [04:57<00:23, 86.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25760/27749 [04:57<00:22, 86.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25780/27749 [04:57<00:22, 86.63it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25800/27749 [04:57<00:22, 86.67it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25820/27749 [04:57<00:22, 86.71it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25840/27749 [04:57<00:22, 86.74it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25860/27749 [04:57<00:21, 86.78it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25880/27749 [04:58<00:21, 86.82it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25900/27749 [04:58<00:21, 86.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25920/27749 [04:58<00:21, 86.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  93% 25940/27749 [04:58<00:20, 86.94it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 25960/27749 [04:58<00:20, 86.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 25980/27749 [04:58<00:20, 87.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26000/27749 [04:58<00:20, 87.06it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26020/27749 [04:58<00:19, 87.10it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26040/27749 [04:58<00:19, 87.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26060/27749 [04:58<00:19, 87.18it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26080/27749 [04:59<00:19, 87.22it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26100/27749 [04:59<00:18, 87.26it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26120/27749 [04:59<00:18, 87.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26140/27749 [04:59<00:18, 87.34it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26160/27749 [04:59<00:18, 87.38it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26180/27749 [04:59<00:17, 87.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26200/27749 [04:59<00:17, 87.46it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  94% 26220/27749 [04:59<00:17, 87.50it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26240/27749 [04:59<00:17, 87.54it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26260/27749 [04:59<00:17, 87.58it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26280/27749 [04:59<00:16, 87.62it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26300/27749 [05:00<00:16, 87.66it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26320/27749 [05:00<00:16, 87.70it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26340/27749 [05:00<00:16, 87.74it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26360/27749 [05:00<00:15, 87.78it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26380/27749 [05:00<00:15, 87.83it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26400/27749 [05:00<00:15, 87.87it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26420/27749 [05:00<00:15, 87.91it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26440/27749 [05:00<00:14, 87.95it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26460/27749 [05:00<00:14, 87.99it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26480/27749 [05:00<00:14, 88.03it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  95% 26500/27749 [05:00<00:14, 88.07it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26520/27749 [05:00<00:13, 88.11it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26540/27749 [05:01<00:13, 88.15it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26560/27749 [05:01<00:13, 88.19it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26580/27749 [05:01<00:13, 88.23it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26600/27749 [05:01<00:13, 88.27it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26620/27749 [05:01<00:12, 88.31it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26640/27749 [05:01<00:12, 88.35it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26660/27749 [05:01<00:12, 88.39it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26680/27749 [05:01<00:12, 88.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26700/27749 [05:01<00:11, 88.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26720/27749 [05:01<00:11, 88.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26740/27749 [05:01<00:11, 88.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  96% 26760/27749 [05:02<00:11, 88.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26780/27749 [05:02<00:10, 88.62it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26800/27749 [05:02<00:10, 88.66it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26820/27749 [05:02<00:10, 88.70it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26840/27749 [05:02<00:10, 88.74it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26860/27749 [05:02<00:10, 88.78it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26880/27749 [05:02<00:09, 88.81it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26900/27749 [05:02<00:09, 88.85it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26920/27749 [05:02<00:09, 88.89it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26940/27749 [05:02<00:09, 88.93it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26960/27749 [05:03<00:08, 88.97it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 26980/27749 [05:03<00:08, 89.01it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 27000/27749 [05:03<00:08, 89.05it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 27020/27749 [05:03<00:08, 89.09it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  97% 27040/27749 [05:03<00:07, 89.12it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27060/27749 [05:03<00:07, 89.16it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27080/27749 [05:03<00:07, 89.20it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27100/27749 [05:03<00:07, 89.24it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27120/27749 [05:03<00:07, 89.28it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27140/27749 [05:03<00:06, 89.32it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27160/27749 [05:03<00:06, 89.36it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27180/27749 [05:04<00:06, 89.40it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27200/27749 [05:04<00:06, 89.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27220/27749 [05:04<00:05, 89.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27240/27749 [05:04<00:05, 89.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27260/27749 [05:04<00:05, 89.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27280/27749 [05:04<00:05, 89.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27300/27749 [05:04<00:05, 89.63it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  98% 27320/27749 [05:04<00:04, 89.67it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27340/27749 [05:04<00:04, 89.71it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27360/27749 [05:04<00:04, 89.75it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27380/27749 [05:04<00:04, 89.78it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27400/27749 [05:05<00:03, 89.82it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27420/27749 [05:05<00:03, 89.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27440/27749 [05:05<00:03, 89.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27460/27749 [05:05<00:03, 89.94it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27480/27749 [05:05<00:02, 89.97it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27500/27749 [05:05<00:02, 90.01it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27520/27749 [05:05<00:02, 90.05it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27540/27749 [05:05<00:02, 90.09it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27560/27749 [05:05<00:02, 90.12it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27580/27749 [05:05<00:01, 90.16it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11:  99% 27600/27749 [05:05<00:01, 90.20it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11: 100% 27620/27749 [05:06<00:01, 90.24it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11: 100% 27640/27749 [05:06<00:01, 90.28it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11: 100% 27660/27749 [05:06<00:00, 90.32it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11: 100% 27680/27749 [05:06<00:00, 90.35it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11: 100% 27700/27749 [05:06<00:00, 90.39it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11: 100% 27720/27749 [05:06<00:00, 90.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11: 100% 27740/27749 [05:06<00:00, 90.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.750, avg_val_loss=2.750, train_loss_epoch=2.880]\n",
            "Epoch 11: 100% 27749/27749 [05:06<00:00, 90.48it/s, loss=2.87, v_num=0, train_loss_step=2.880, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.880]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 12:  80% 22180/27749 [04:38<01:09, 79.70it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/5550 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  80% 22200/27749 [04:41<01:10, 78.80it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  80% 22220/27749 [04:41<01:10, 78.84it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  80% 22240/27749 [04:41<01:09, 78.89it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  80% 22260/27749 [04:42<01:09, 78.93it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  80% 22280/27749 [04:42<01:09, 78.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  80% 22300/27749 [04:42<01:08, 79.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  80% 22320/27749 [04:42<01:08, 79.07it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22340/27749 [04:42<01:08, 79.11it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22360/27749 [04:42<01:08, 79.16it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22380/27749 [04:42<01:07, 79.20it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22400/27749 [04:42<01:07, 79.25it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22420/27749 [04:42<01:07, 79.29it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22440/27749 [04:42<01:06, 79.33it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22460/27749 [04:42<01:06, 79.37it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22480/27749 [04:43<01:06, 79.41it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22500/27749 [04:43<01:06, 79.45it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22520/27749 [04:43<01:05, 79.50it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22540/27749 [04:43<01:05, 79.54it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22560/27749 [04:43<01:05, 79.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22580/27749 [04:43<01:04, 79.63it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  81% 22600/27749 [04:43<01:04, 79.67it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22620/27749 [04:43<01:04, 79.72it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22640/27749 [04:43<01:04, 79.76it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22660/27749 [04:43<01:03, 79.81it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22680/27749 [04:44<01:03, 79.85it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22700/27749 [04:44<01:03, 79.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22720/27749 [04:44<01:02, 79.94it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22740/27749 [04:44<01:02, 79.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22760/27749 [04:44<01:02, 80.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22780/27749 [04:44<01:02, 80.07it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22800/27749 [04:44<01:01, 80.11it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22820/27749 [04:44<01:01, 80.15it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22840/27749 [04:44<01:01, 80.19it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22860/27749 [04:44<01:00, 80.24it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  82% 22880/27749 [04:45<01:00, 80.28it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 22900/27749 [04:45<01:00, 80.32it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 22920/27749 [04:45<01:00, 80.37it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 22940/27749 [04:45<00:59, 80.41it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 22960/27749 [04:45<00:59, 80.46it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 22980/27749 [04:45<00:59, 80.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23000/27749 [04:45<00:58, 80.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23020/27749 [04:45<00:58, 80.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23040/27749 [04:45<00:58, 80.64it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23060/27749 [04:45<00:58, 80.68it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23080/27749 [04:45<00:57, 80.73it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23100/27749 [04:45<00:57, 80.77it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23120/27749 [04:46<00:57, 80.82it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23140/27749 [04:46<00:56, 80.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  83% 23160/27749 [04:46<00:56, 80.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23180/27749 [04:46<00:56, 80.95it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23200/27749 [04:46<00:56, 80.99it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23220/27749 [04:46<00:55, 81.03it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23240/27749 [04:46<00:55, 81.08it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23260/27749 [04:46<00:55, 81.12it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23280/27749 [04:46<00:55, 81.16it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23300/27749 [04:46<00:54, 81.21it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23320/27749 [04:47<00:54, 81.25it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23340/27749 [04:47<00:54, 81.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23360/27749 [04:47<00:53, 81.34it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23380/27749 [04:47<00:53, 81.38it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23400/27749 [04:47<00:53, 81.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23420/27749 [04:47<00:53, 81.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  84% 23440/27749 [04:47<00:52, 81.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23460/27749 [04:47<00:52, 81.56it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23480/27749 [04:47<00:52, 81.60it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23500/27749 [04:47<00:52, 81.64it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23520/27749 [04:47<00:51, 81.68it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23540/27749 [04:48<00:51, 81.73it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23560/27749 [04:48<00:51, 81.77it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23580/27749 [04:48<00:50, 81.81it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23600/27749 [04:48<00:50, 81.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23620/27749 [04:48<00:50, 81.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23640/27749 [04:48<00:50, 81.94it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23660/27749 [04:48<00:49, 81.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23680/27749 [04:48<00:49, 82.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23700/27749 [04:48<00:49, 82.06it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  85% 23720/27749 [04:48<00:49, 82.10it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23740/27749 [04:49<00:48, 82.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23760/27749 [04:49<00:48, 82.19it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23780/27749 [04:49<00:48, 82.23it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23800/27749 [04:49<00:48, 82.27it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23820/27749 [04:49<00:47, 82.31it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23840/27749 [04:49<00:47, 82.36it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23860/27749 [04:49<00:47, 82.40it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23880/27749 [04:49<00:46, 82.44it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23900/27749 [04:49<00:46, 82.49it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23920/27749 [04:49<00:46, 82.53it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23940/27749 [04:49<00:46, 82.57it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23960/27749 [04:50<00:45, 82.61it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 23980/27749 [04:50<00:45, 82.66it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  86% 24000/27749 [04:50<00:45, 82.70it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24020/27749 [04:50<00:45, 82.75it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24040/27749 [04:50<00:44, 82.79it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24060/27749 [04:50<00:44, 82.83it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24080/27749 [04:50<00:44, 82.88it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24100/27749 [04:50<00:44, 82.92it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24120/27749 [04:50<00:43, 82.96it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24140/27749 [04:50<00:43, 83.00it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24160/27749 [04:50<00:43, 83.05it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24180/27749 [04:51<00:42, 83.09it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24200/27749 [04:51<00:42, 83.13it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24220/27749 [04:51<00:42, 83.17it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24240/27749 [04:51<00:42, 83.22it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24260/27749 [04:51<00:41, 83.26it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  87% 24280/27749 [04:51<00:41, 83.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24300/27749 [04:51<00:41, 83.34it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24320/27749 [04:51<00:41, 83.39it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24340/27749 [04:51<00:40, 83.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24360/27749 [04:51<00:40, 83.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24380/27749 [04:51<00:40, 83.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24400/27749 [04:52<00:40, 83.56it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24420/27749 [04:52<00:39, 83.60it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24440/27749 [04:52<00:39, 83.64it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24460/27749 [04:52<00:39, 83.68it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24480/27749 [04:52<00:39, 83.72it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24500/27749 [04:52<00:38, 83.76it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24520/27749 [04:52<00:38, 83.80it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  88% 24540/27749 [04:52<00:38, 83.84it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24560/27749 [04:52<00:38, 83.88it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24580/27749 [04:52<00:37, 83.92it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24600/27749 [04:52<00:37, 83.96it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24620/27749 [04:53<00:37, 84.00it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24640/27749 [04:53<00:36, 84.05it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24660/27749 [04:53<00:36, 84.09it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24680/27749 [04:53<00:36, 84.13it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24700/27749 [04:53<00:36, 84.17it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24720/27749 [04:53<00:35, 84.21it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24740/27749 [04:53<00:35, 84.25it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24760/27749 [04:53<00:35, 84.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24780/27749 [04:53<00:35, 84.34it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24800/27749 [04:53<00:34, 84.38it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  89% 24820/27749 [04:54<00:34, 84.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 24840/27749 [04:54<00:34, 84.46it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 24860/27749 [04:54<00:34, 84.50it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 24880/27749 [04:54<00:33, 84.54it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 24900/27749 [04:54<00:33, 84.58it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 24920/27749 [04:54<00:33, 84.63it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 24940/27749 [04:54<00:33, 84.66it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 24960/27749 [04:54<00:32, 84.70it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 24980/27749 [04:54<00:32, 84.74it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 25000/27749 [04:54<00:32, 84.78it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 25020/27749 [04:54<00:32, 84.83it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 25040/27749 [04:55<00:31, 84.87it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 25060/27749 [04:55<00:31, 84.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 25080/27749 [04:55<00:31, 84.94it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  90% 25100/27749 [04:55<00:31, 84.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25120/27749 [04:55<00:30, 85.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25140/27749 [04:55<00:30, 85.06it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25160/27749 [04:55<00:30, 85.10it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25180/27749 [04:55<00:30, 85.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25200/27749 [04:55<00:29, 85.18it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25220/27749 [04:55<00:29, 85.21it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25240/27749 [04:56<00:29, 85.25it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25260/27749 [04:56<00:29, 85.29it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25280/27749 [04:56<00:28, 85.33it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25300/27749 [04:56<00:28, 85.38it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25320/27749 [04:56<00:28, 85.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25340/27749 [04:56<00:28, 85.46it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25360/27749 [04:56<00:27, 85.50it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  91% 25380/27749 [04:56<00:27, 85.53it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25400/27749 [04:56<00:27, 85.57it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25420/27749 [04:56<00:27, 85.61it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25440/27749 [04:57<00:26, 85.65it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25460/27749 [04:57<00:26, 85.69it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25480/27749 [04:57<00:26, 85.73it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25500/27749 [04:57<00:26, 85.77it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25520/27749 [04:57<00:25, 85.81it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25540/27749 [04:57<00:25, 85.85it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25560/27749 [04:57<00:25, 85.89it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25580/27749 [04:57<00:25, 85.93it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25600/27749 [04:57<00:24, 85.97it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25620/27749 [04:57<00:24, 86.01it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25640/27749 [04:57<00:24, 86.05it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  92% 25660/27749 [04:58<00:24, 86.09it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25680/27749 [04:58<00:24, 86.13it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25700/27749 [04:58<00:23, 86.17it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25720/27749 [04:58<00:23, 86.22it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25740/27749 [04:58<00:23, 86.26it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25760/27749 [04:58<00:23, 86.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25780/27749 [04:58<00:22, 86.33it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25800/27749 [04:58<00:22, 86.37it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25820/27749 [04:58<00:22, 86.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25840/27749 [04:58<00:22, 86.45it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25860/27749 [04:58<00:21, 86.49it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25880/27749 [04:59<00:21, 86.54it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25900/27749 [04:59<00:21, 86.58it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25920/27749 [04:59<00:21, 86.61it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  93% 25940/27749 [04:59<00:20, 86.66it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 25960/27749 [04:59<00:20, 86.70it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 25980/27749 [04:59<00:20, 86.74it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26000/27749 [04:59<00:20, 86.78it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26020/27749 [04:59<00:19, 86.82it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26040/27749 [04:59<00:19, 86.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26060/27749 [04:59<00:19, 86.90it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26080/27749 [04:59<00:19, 86.94it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26100/27749 [05:00<00:18, 86.98it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26120/27749 [05:00<00:18, 87.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26140/27749 [05:00<00:18, 87.06it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26160/27749 [05:00<00:18, 87.10it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26180/27749 [05:00<00:18, 87.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26200/27749 [05:00<00:17, 87.18it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  94% 26220/27749 [05:00<00:17, 87.22it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26240/27749 [05:00<00:17, 87.27it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26260/27749 [05:00<00:17, 87.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26280/27749 [05:00<00:16, 87.35it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26300/27749 [05:00<00:16, 87.39it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26320/27749 [05:01<00:16, 87.43it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26340/27749 [05:01<00:16, 87.47it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26360/27749 [05:01<00:15, 87.51it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26380/27749 [05:01<00:15, 87.55it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26400/27749 [05:01<00:15, 87.59it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26420/27749 [05:01<00:15, 87.63it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26440/27749 [05:01<00:14, 87.67it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26460/27749 [05:01<00:14, 87.71it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26480/27749 [05:01<00:14, 87.75it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  95% 26500/27749 [05:01<00:14, 87.79it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26520/27749 [05:01<00:13, 87.82it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26540/27749 [05:02<00:13, 87.86it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26560/27749 [05:02<00:13, 87.91it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26580/27749 [05:02<00:13, 87.95it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26600/27749 [05:02<00:13, 87.99it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26620/27749 [05:02<00:12, 88.02it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26640/27749 [05:02<00:12, 88.06it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26660/27749 [05:02<00:12, 88.10it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26680/27749 [05:02<00:12, 88.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26700/27749 [05:02<00:11, 88.18it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26720/27749 [05:02<00:11, 88.22it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26740/27749 [05:02<00:11, 88.26it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  96% 26760/27749 [05:03<00:11, 88.30it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26780/27749 [05:03<00:10, 88.34it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26800/27749 [05:03<00:10, 88.38it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26820/27749 [05:03<00:10, 88.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26840/27749 [05:03<00:10, 88.46it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26860/27749 [05:03<00:10, 88.50it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26880/27749 [05:03<00:09, 88.54it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26900/27749 [05:03<00:09, 88.58it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26920/27749 [05:03<00:09, 88.62it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26940/27749 [05:03<00:09, 88.66it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26960/27749 [05:03<00:08, 88.70it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 26980/27749 [05:04<00:08, 88.74it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 27000/27749 [05:04<00:08, 88.77it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 27020/27749 [05:04<00:08, 88.81it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  97% 27040/27749 [05:04<00:07, 88.85it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27060/27749 [05:04<00:07, 88.89it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27080/27749 [05:04<00:07, 88.93it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27100/27749 [05:04<00:07, 88.97it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27120/27749 [05:04<00:07, 89.00it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27140/27749 [05:04<00:06, 89.04it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27160/27749 [05:04<00:06, 89.08it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27180/27749 [05:04<00:06, 89.12it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27200/27749 [05:05<00:06, 89.16it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27220/27749 [05:05<00:05, 89.20it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27240/27749 [05:05<00:05, 89.24it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27260/27749 [05:05<00:05, 89.27it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27280/27749 [05:05<00:05, 89.31it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27300/27749 [05:05<00:05, 89.35it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  98% 27320/27749 [05:05<00:04, 89.38it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27340/27749 [05:05<00:04, 89.42it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27360/27749 [05:05<00:04, 89.46it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27380/27749 [05:05<00:04, 89.50it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27400/27749 [05:06<00:03, 89.54it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27420/27749 [05:06<00:03, 89.58it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27440/27749 [05:06<00:03, 89.62it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27460/27749 [05:06<00:03, 89.65it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27480/27749 [05:06<00:02, 89.69it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27500/27749 [05:06<00:02, 89.73it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27520/27749 [05:06<00:02, 89.77it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27540/27749 [05:06<00:02, 89.81it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27560/27749 [05:06<00:02, 89.85it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27580/27749 [05:06<00:01, 89.89it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12:  99% 27600/27749 [05:06<00:01, 89.93it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12: 100% 27620/27749 [05:07<00:01, 89.97it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12: 100% 27640/27749 [05:07<00:01, 90.01it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12: 100% 27660/27749 [05:07<00:00, 90.04it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12: 100% 27680/27749 [05:07<00:00, 90.08it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12: 100% 27700/27749 [05:07<00:00, 90.12it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12: 100% 27720/27749 [05:07<00:00, 90.15it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12: 100% 27740/27749 [05:07<00:00, 90.19it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "Epoch 12: 100% 27749/27749 [05:07<00:00, 90.21it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Monitored metric avg_val_loss did not improve in the last 10 records. Best score: 2.722. Signaling Trainer to stop.\n",
            "Epoch 12: 100% 27749/27749 [05:07<00:00, 90.14it/s, loss=2.87, v_num=0, train_loss_step=2.870, val_loss=2.760, avg_val_loss=2.760, train_loss_epoch=2.870]\n",
            "trainning done\n",
            "\n",
            "-------------------------------Feature Extractin-------------------------------\n",
            "#--------------------------------------------Feature Extracting(pairs)------------------------------------------------------\n",
            "num of batches for all cells (not cell pairs): 118\n",
            "/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=6300) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "  self.pid = os.fork()\n",
            "Shape of the feature representation generated by the base encoder: (5893, 64)\n",
            "end time: 1769107398.5297964\n",
            "Execution time: 1.13 hours\n"
          ]
        }
      ],
      "source": [
        "!python LCL_Main_Semi.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Biddy_train.h5ad\" \\\n",
        "                                              --testFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Biddy_test.h5ad\" \\\n",
        "                                              --batch_size 50 \\\n",
        "                                              --size_factor 0.04 \\\n",
        "                                              --unlabeled_per_batch 5 \\\n",
        "                                              --lambda_penalty 0.05 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty\" \\\n",
        "                                              --train_test 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzXLxalfnumD"
      },
      "source": [
        "**5. extract the features**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python feature_extraction.py --help"
      ],
      "metadata": {
        "id": "d8HHCBIOWxgQ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e874c967-d054-4c30-b512-9188c231a064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: feature_extraction.py [-h] --inputFilePath INPUTFILEPATH\n",
            "                             [--batch_size BATCH_SIZE] --output_dir OUTPUT_DIR\n",
            "                             --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                             [--hidden_dims HIDDEN_DIMS]\n",
            "                             [--embedding_size EMBEDDING_SIZE]\n",
            "                             [--out_file_name OUT_FILE_NAME]\n",
            "\n",
            "Extract base-encoder features (h) from a trained LCL checkpoint.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Path to AnnData (.h5ad) for feature extraction\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for featurizing all cells\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save extracted feature .npy\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to trained Lightning .ckpt file\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        Comma-separated hidden dims, e.g. \"1024,256,64\"\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        Projection head output dimension (kept for init)\n",
            "  --out_file_name OUT_FILE_NAME\n",
            "                        Optional output .npy name (default: auto)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RL94aUsczXKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python feature_extraction.py \\\n",
        "  --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Biddy_train.h5ad\" \\\n",
        "  --batch_size 50 \\\n",
        "  --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty\" \\\n",
        "  --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\" \\\n",
        "  --out_file_name \"train_base_embed.npy\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRpHFU3vdOs_",
        "outputId": "098161a3-8054-4d7b-d5c8-bdc088cfa5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected input_dim = 2000, n_obs = 5893\n",
            "=== STATE_DICT LOAD REPORT (bare BaseEncoder_ProjHead_MLP) ===\n",
            "missing: 0\n",
            "unexpected: 0\n",
            "missing examples: []\n",
            "unexpected examples: []\n",
            "=============================================================\n",
            "Extracted base features shape: (5893, 64)\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/train_base_embed.npy\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/cell_ids_bs50.txt\n",
            "Done in 0.01 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9384065-2366-4dbb-b2ad-9197980f143c",
        "collapsed": true,
        "id": "s-Zog6SpsEJE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected input_dim = 2000, n_obs = 641\n",
            "=== STATE_DICT LOAD REPORT (bare BaseEncoder_ProjHead_MLP) ===\n",
            "missing: 0\n",
            "unexpected: 0\n",
            "missing examples: []\n",
            "unexpected examples: []\n",
            "=============================================================\n",
            "Extracted base features shape: (641, 64)\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/test_base_embed.npy\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/cell_ids_bs50.txt\n",
            "Done in 0.01 minutes\n"
          ]
        }
      ],
      "source": [
        "!python feature_extraction.py \\\n",
        "  --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Biddy_test.h5ad\" \\\n",
        "  --batch_size 50 \\\n",
        "  --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty\" \\\n",
        "  --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\" \\\n",
        "  --out_file_name \"test_base_embed.npy\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python project_head_extraction.py \\\n",
        "  --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Biddy_train.h5ad\" \\\n",
        "  --batch_size 50 \\\n",
        "  --out_file_name 'train_proj_embed.npy' \\\n",
        "  --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty\" \\\n",
        "  --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzINhEkYmO__",
        "outputId": "a90f4ce8-1af6-4da7-87be-d43f33a6db46",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------- PROJECTION EMBEDDING EXTRACTION ---------------------\n",
            "inputFilePath: /content/drive/MyDrive/Colab Notebooks/data/Biddy_train.h5ad\n",
            "resume_from_checkpoint: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\n",
            "inferred input_dim: 2000\n",
            "hidden_dims: [1024, 256, 64]\n",
            "embedding_size: 32\n",
            "batch_size (inference): 50\n",
            "=== STATE_DICT LOAD REPORT ===\n",
            "missing: 0\n",
            "unexpected: 0\n",
            "missing examples: []\n",
            "unexpected examples: []\n",
            "================================\n",
            "[OK] Loaded checkpoint: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/train_proj_embed.npy  (shape=(5893, 32))\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/cell_ids.txt  (n=5893)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python project_head_extraction.py \\\n",
        "  --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Biddy_test.h5ad\" \\\n",
        "  --batch_size 50 \\\n",
        "  --out_file_name 'test_proj_embed.npy' \\\n",
        "  --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty\" \\\n",
        "  --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_ablvvmcKY",
        "outputId": "d4d353b6-6223-49cc-bda9-45dd2642003b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------- PROJECTION EMBEDDING EXTRACTION ---------------------\n",
            "inputFilePath: /content/drive/MyDrive/Colab Notebooks/data/Biddy_test.h5ad\n",
            "resume_from_checkpoint: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\n",
            "inferred input_dim: 2000\n",
            "hidden_dims: [1024, 256, 64]\n",
            "embedding_size: 32\n",
            "batch_size (inference): 50\n",
            "=== STATE_DICT LOAD REPORT ===\n",
            "missing: 0\n",
            "unexpected: 0\n",
            "missing examples: []\n",
            "unexpected examples: []\n",
            "================================\n",
            "[OK] Loaded checkpoint: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/test_proj_embed.npy  (shape=(641, 32))\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Cell_tag/feat_celltag_lambda005_unlab5_bs50_testAsPenalty/cell_ids.txt  (n=641)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOzHpGtv6qWa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}