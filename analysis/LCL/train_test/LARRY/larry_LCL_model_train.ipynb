{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8iMuyY0jX4O"
      },
      "source": [
        "**1. Change the directory to the google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_zXNChaC4Tk",
        "outputId": "c63fb17c-c83a-4a0a-936c-a08576fc9568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgFzoXTPCVub",
        "outputId": "2b757c1f-fc39-471b-9ba3-ee4965391b66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/scCL/main_semi_test_new\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/scCL/main_semi_test_new')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMuQtMnjnhU"
      },
      "source": [
        "**2. Install the packages for scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rspO9sCEDsTM",
        "outputId": "6d69d570-af2a-4c13-c78e-ea9083b45073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (25.0)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.11)\n",
            "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.15.2 pytorch-lightning-2.6.0 torchmetrics-1.8.2\n",
            "Collecting anndata\n",
            "  Downloading anndata-0.12.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting array-api-compat>=1.7.1 (from anndata)\n",
            "  Downloading array_api_compat-1.13.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: h5py>=3.8 in /usr/local/lib/python3.12/dist-packages (from anndata) (3.15.1)\n",
            "Collecting legacy-api-wrap (from anndata)\n",
            "  Downloading legacy_api_wrap-1.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from anndata) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from anndata) (2.0.2)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.12/dist-packages (from anndata) (25.0)\n",
            "Requirement already satisfied: pandas!=2.1.2,<3,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from anndata) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from anndata) (1.16.3)\n",
            "Collecting zarr!=3.0.*,>=2.18.7 (from anndata)\n",
            "  Downloading zarr-3.1.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.2,<3,>=2.1.0->anndata) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.2,<3,>=2.1.0->anndata) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.2,<3,>=2.1.0->anndata) (2025.3)\n",
            "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata) (1.8.0)\n",
            "Collecting numcodecs>=0.14 (from zarr!=3.0.*,>=2.18.7->anndata)\n",
            "  Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.9 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata) (4.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata) (6.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.2,<3,>=2.1.0->anndata) (1.17.0)\n",
            "Downloading anndata-0.12.7-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.1.5-py3-none-any.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.5-py3-none-any.whl (10 kB)\n",
            "Downloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numcodecs, legacy-api-wrap, donfig, array-api-compat, zarr, anndata\n",
            "Successfully installed anndata-0.12.7 array-api-compat-1.13.0 donfig-0.8.1.post1 legacy-api-wrap-1.5 numcodecs-0.16.5 zarr-3.1.5\n",
            "Collecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.7.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (2.0.2)\n",
            "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (1.8.2)\n",
            "Requirement already satisfied: lightning-utilities>0.3.1 in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (0.15.2)\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (0.24.0+cu126)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from lightning-bolts) (2.19.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.15.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.3)\n",
            "Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2025.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->lightning-bolts) (3.1.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.10.0->lightning-bolts) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.13.3)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->lightning-bolts) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.11)\n",
            "Downloading lightning_bolts-0.7.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-lightning, lightning-bolts\n",
            "  Attempting uninstall: pytorch-lightning\n",
            "    Found existing installation: pytorch-lightning 2.6.0\n",
            "    Uninstalling pytorch-lightning-2.6.0:\n",
            "      Successfully uninstalled pytorch-lightning-2.6.0\n",
            "Successfully installed lightning-bolts-0.7.0 pytorch-lightning-1.9.5\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.12-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: anndata>=0.10.8 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.12.7)\n",
            "Collecting fast-array-utils>=1.2.1 (from fast-array-utils[accel,sparse]>=1.2.1->scanpy)\n",
            "  Downloading fast_array_utils-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: h5py>=3.11 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.15.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.5.3)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.5 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.5)\n",
            "Requirement already satisfied: matplotlib>=3.9 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.8.8 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.6.1)\n",
            "Requirement already satisfied: numba>=0.60 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=25 in /usr/local/lib/python3.12/dist-packages (from scanpy) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.0.2)\n",
            "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.6.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.16.3)\n",
            "Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info2 (from scanpy)\n",
            "  Downloading session_info2-0.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.14.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from scanpy) (4.15.0)\n",
            "Requirement already satisfied: umap-learn>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.5.11)\n",
            "Requirement already satisfied: array-api-compat>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from anndata>=0.10.8->scanpy) (1.13.0)\n",
            "Requirement already satisfied: zarr!=3.0.*,>=2.18.7 in /usr/local/lib/python3.12/dist-packages (from anndata>=0.10.8->scanpy) (3.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.9->scanpy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.9->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.9->scanpy) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.9->scanpy) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.9->scanpy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.9->scanpy) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.9->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.60->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->scanpy) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.9->scanpy) (1.17.0)\n",
            "Requirement already satisfied: donfig>=0.8 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.10.8->scanpy) (0.8.1.post1)\n",
            "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.10.8->scanpy) (1.8.0)\n",
            "Requirement already satisfied: numcodecs>=0.14 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.10.8->scanpy) (0.16.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from donfig>=0.8->zarr!=3.0.*,>=2.18.7->anndata>=0.10.8->scanpy) (6.0.3)\n",
            "Downloading scanpy-1.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fast_array_utils-1.3.1-py3-none-any.whl (36 kB)\n",
            "Downloading session_info2-0.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: session-info2, fast-array-utils, scanpy\n",
            "Successfully installed fast-array-utils-1.3.1 scanpy-1.12 session-info2-0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install pytorch-lightning\n",
        "!pip install anndata\n",
        "!pip install lightning-bolts\n",
        "!pip install scanpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_PUeICjwUp"
      },
      "source": [
        "**3. scCL manual**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YXqbZBUVSPBS",
        "outputId": "f9726432-b16a-4529-b7d8-4de41c913eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1768834073.6533647\n",
            "usage: LCL_Main_Semi.py [-h] [--inputFilePath INPUTFILEPATH]\n",
            "                        [--testFilePath TESTFILEPATH]\n",
            "                        [--unlabeled_per_batch UNLABELED_PER_BATCH]\n",
            "                        [--lambda_penalty LAMBDA_PENALTY]\n",
            "                        [--batch_size BATCH_SIZE] [--size_factor SIZE_FACTOR]\n",
            "                        [--temperature TEMPERATURE] [--patience PATIENCE]\n",
            "                        [--min_delta MIN_DELTA] [--max_epoch MAX_EPOCH]\n",
            "                        --output_dir OUTPUT_DIR [--train_test TRAIN_TEST]\n",
            "                        [--hidden_dims HIDDEN_DIMS]\n",
            "                        [--embedding_size EMBEDDING_SIZE]\n",
            "                        [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
            "\n",
            "Run the contrastive learning model on provided single-cell data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --inputFilePath INPUTFILEPATH\n",
            "                        Train Anndata for running the algorithm\n",
            "  --testFilePath TESTFILEPATH\n",
            "                        Test Anndata for running the algorithm\n",
            "  --unlabeled_per_batch UNLABELED_PER_BATCH\n",
            "                        number of unlabeled cells per batch\n",
            "  --lambda_penalty LAMBDA_PENALTY\n",
            "                        lambda_penalty for semi-supervised learning\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training and validation\n",
            "  --size_factor SIZE_FACTOR\n",
            "                        Size factor range from 0 to 1\n",
            "  --temperature TEMPERATURE\n",
            "                        Temperature parameter for contrastive loss\n",
            "  --patience PATIENCE   Number of epochs with no improvement after which\n",
            "                        training will be stopped\n",
            "  --min_delta MIN_DELTA\n",
            "                        Minimum change to qualify as an improvement\n",
            "  --max_epoch MAX_EPOCH\n",
            "                        Maximum number of epochs\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory to save outputs\n",
            "  --train_test TRAIN_TEST\n",
            "                        1: split the data for train and validation; 0:\n",
            "                        otherwise\n",
            "  --hidden_dims HIDDEN_DIMS\n",
            "                        dimensions of each layer of base encoder. example\n",
            "                        input: 1024,256,64\n",
            "  --embedding_size EMBEDDING_SIZE\n",
            "                        the output dimension of projection head\n",
            "  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n",
            "                        Path to a checkpoint to resume from\n"
          ]
        }
      ],
      "source": [
        "!python LCL_Main_Semi.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN6m8QnBnbd-"
      },
      "source": [
        "**4. Run scCL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3V30WCiJDs9I",
        "outputId": "8a9e2e23-9e47-4d5d-b552-c114f47a5fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.12/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "start time: 1768834086.1015213\n",
            "-------------------------------INFO-------------------------------\n",
            "Anndata Info (train):  /content/drive/MyDrive/Colab Notebooks/data/Larry_train.h5ad\n",
            "Anndata Info (test):   /content/drive/MyDrive/Colab Notebooks/data/Larry_test.h5ad\n",
            "batch_size:  260\n",
            "size_factor:  0.45\n",
            "temperature:  0.5\n",
            "number of epochs:  220\n",
            "train_test_ratio:  0.8\n",
            "input_dim (inferred):  2000\n",
            "hidden_dims:  [1024, 256, 64]\n",
            "embedding_size:  32\n",
            "Number of batches: 5899\n",
            "Total labeled pairs: 5899 × 260 = 1533740\n",
            "Unlabeled per batch (from test set): 15\n",
            "num_workers(number of available CPU cores): 8\n",
            "Total batches: 5899\n",
            "Pos-pairs per batch: 260, unlabeled per batch: 15\n",
            "Training/Validation split:\n",
            " number of train batches: 4719, number of val batches: 1180\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/ exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/content/drive/My Drive/Colab Notebooks/scCL/main_semi_test_new/LCL_Main_Semi.py:151: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  scheduler = LinearWarmupCosineAnnealingLR(\n",
            "\n",
            "  | Name    | Type                     | Params\n",
            "-----------------------------------------------------\n",
            "0 | model   | BaseEncoder_ProjHead_MLP | 2.3 M \n",
            "1 | loss_fn | ContrastiveLoss          | 0     \n",
            "-----------------------------------------------------\n",
            "2.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "2.3 M     Total params\n",
            "9.348     Total estimated model params size (MB)\n",
            "2026-01-19 14:49:02.455499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768834142.472954    1979 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768834142.477952    1979 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768834142.491365    1979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768834142.491388    1979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768834142.491391    1979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768834142.491393    1979 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-19 14:49:02.495583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/httplib2/auth.py:19: DeprecationWarning: 'setName' deprecated - use 'set_name'\n",
            "  token = pp.Word(tchar).setName(\"token\")\n",
            "/usr/local/lib/python3.12/dist-packages/httplib2/auth.py:20: DeprecationWarning: 'leaveWhitespace' deprecated - use 'leave_whitespace'\n",
            "  token68 = pp.Combine(pp.Word(\"-._~+/\" + pp.nums + pp.alphas) + pp.Optional(pp.Word(\"=\").leaveWhitespace())).setName(\n",
            "/usr/local/lib/python3.12/dist-packages/httplib2/auth.py:24: DeprecationWarning: 'setParseAction' deprecated - use 'set_parse_action'\n",
            "  quoted_string = pp.dblQuotedString.copy().setName(\"quoted-string\").setParseAction(unquote)\n",
            "/usr/local/lib/python3.12/dist-packages/httplib2/auth.py:25: DeprecationWarning: 'addParseAction' deprecated - use 'add_parse_action'\n",
            "  auth_param_name = token.copy().setName(\"auth-param-name\").addParseAction(downcaseTokens)\n",
            "/usr/local/lib/python3.12/dist-packages/httplib2/auth.py:27: DeprecationWarning: 'delimitedList' deprecated - use 'DelimitedList'\n",
            "  params = pp.Dict(pp.delimitedList(pp.Group(auth_param)))\n",
            "Sanity Checking: 0it [00:00, ?it/s]/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1979) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "  self.pid = os.fork()\n",
            "Sanity Checking DataLoader 0: 100% 2/2 [00:00<00:00,  2.54it/s]/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:32: DeprecationWarning: This property will be removed in 2.0.0. Use `Metric.updated_called` instead.\n",
            "  return fn(*args, **kwargs)\n",
            "Epoch 0:  80% 4700/5899 [01:29<00:22, 52.37it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  80% 4720/5899 [01:32<00:23, 51.18it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  80% 4740/5899 [01:32<00:22, 51.29it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  81% 4760/5899 [01:32<00:22, 51.41it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  81% 4780/5899 [01:32<00:21, 51.53it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  81% 4800/5899 [01:32<00:21, 51.65it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  82% 4820/5899 [01:33<00:20, 51.77it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  82% 4840/5899 [01:33<00:20, 51.89it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  82% 4860/5899 [01:33<00:19, 52.01it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  83% 4880/5899 [01:33<00:19, 52.13it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  83% 4900/5899 [01:33<00:19, 52.25it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  83% 4920/5899 [01:33<00:18, 52.37it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  84% 4940/5899 [01:34<00:18, 52.48it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  84% 4960/5899 [01:34<00:17, 52.59it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  84% 4980/5899 [01:34<00:17, 52.70it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  85% 5000/5899 [01:34<00:17, 52.81it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  85% 5020/5899 [01:34<00:16, 52.93it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  85% 5040/5899 [01:35<00:16, 53.04it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  86% 5060/5899 [01:35<00:15, 53.15it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  86% 5080/5899 [01:35<00:15, 53.26it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  86% 5100/5899 [01:35<00:14, 53.38it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  87% 5120/5899 [01:35<00:14, 53.48it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  87% 5140/5899 [01:35<00:14, 53.60it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  87% 5160/5899 [01:36<00:13, 53.71it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  88% 5180/5899 [01:36<00:13, 53.83it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  88% 5200/5899 [01:36<00:12, 53.94it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  88% 5220/5899 [01:36<00:12, 54.06it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  89% 5240/5899 [01:36<00:12, 54.17it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  89% 5260/5899 [01:36<00:11, 54.28it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  90% 5280/5899 [01:37<00:11, 54.40it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  90% 5300/5899 [01:37<00:10, 54.51it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  90% 5320/5899 [01:37<00:10, 54.63it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  91% 5340/5899 [01:37<00:10, 54.74it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  91% 5360/5899 [01:37<00:09, 54.85it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  91% 5380/5899 [01:37<00:09, 54.97it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  92% 5400/5899 [01:38<00:09, 55.08it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  92% 5420/5899 [01:38<00:08, 55.19it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  92% 5440/5899 [01:38<00:08, 55.30it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  93% 5460/5899 [01:38<00:07, 55.41it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  93% 5480/5899 [01:38<00:07, 55.52it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  93% 5500/5899 [01:38<00:07, 55.64it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  94% 5520/5899 [01:39<00:06, 55.74it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  94% 5540/5899 [01:39<00:06, 55.85it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  94% 5560/5899 [01:39<00:06, 55.95it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  95% 5580/5899 [01:39<00:05, 56.06it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  95% 5600/5899 [01:39<00:05, 56.16it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  95% 5620/5899 [01:39<00:04, 56.27it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  96% 5640/5899 [01:40<00:04, 56.38it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  96% 5660/5899 [01:40<00:04, 56.49it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  96% 5680/5899 [01:40<00:03, 56.60it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  97% 5700/5899 [01:40<00:03, 56.71it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  97% 5720/5899 [01:40<00:03, 56.81it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  97% 5740/5899 [01:40<00:02, 56.92it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  98% 5760/5899 [01:41<00:02, 57.03it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  98% 5780/5899 [01:41<00:02, 57.13it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  98% 5800/5899 [01:41<00:01, 57.24it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  99% 5820/5899 [01:41<00:01, 57.34it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  99% 5840/5899 [01:41<00:01, 57.44it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0:  99% 5860/5899 [01:41<00:00, 57.54it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0: 100% 5880/5899 [01:42<00:00, 57.64it/s, loss=6.28, v_num=6, train_loss_step=6.240]\n",
            "Epoch 0: 100% 5899/5899 [01:42<00:00, 57.74it/s, loss=6.28, v_num=6, train_loss_step=6.310, val_loss=6.220, avg_val_loss=6.220]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved. New best score: 6.219\n",
            "Epoch 1:  80% 4700/5899 [01:33<00:23, 50.47it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  80% 4720/5899 [01:35<00:23, 49.26it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  80% 4740/5899 [01:35<00:23, 49.38it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  81% 4760/5899 [01:36<00:23, 49.49it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  81% 4780/5899 [01:36<00:22, 49.60it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  81% 4800/5899 [01:36<00:22, 49.72it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  82% 4820/5899 [01:36<00:21, 49.84it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  82% 4840/5899 [01:36<00:21, 49.96it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  82% 4860/5899 [01:37<00:20, 50.07it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  83% 4880/5899 [01:37<00:20, 50.18it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  83% 4900/5899 [01:37<00:19, 50.30it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  83% 4920/5899 [01:37<00:19, 50.42it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  84% 4940/5899 [01:37<00:18, 50.53it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  84% 4960/5899 [01:37<00:18, 50.65it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  84% 4980/5899 [01:38<00:18, 50.76it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  85% 5000/5899 [01:38<00:17, 50.88it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  85% 5020/5899 [01:38<00:17, 50.99it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  85% 5040/5899 [01:38<00:16, 51.10it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  86% 5060/5899 [01:38<00:16, 51.21it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  86% 5080/5899 [01:38<00:15, 51.32it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  86% 5100/5899 [01:39<00:15, 51.44it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  87% 5120/5899 [01:39<00:15, 51.54it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  87% 5140/5899 [01:39<00:14, 51.65it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  87% 5160/5899 [01:39<00:14, 51.76it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  88% 5180/5899 [01:39<00:13, 51.86it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  88% 5200/5899 [01:40<00:13, 51.96it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  88% 5220/5899 [01:40<00:13, 52.08it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  89% 5240/5899 [01:40<00:12, 52.18it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  89% 5260/5899 [01:40<00:12, 52.29it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  90% 5280/5899 [01:40<00:11, 52.40it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  90% 5300/5899 [01:40<00:11, 52.51it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  90% 5320/5899 [01:41<00:11, 52.61it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  91% 5340/5899 [01:41<00:10, 52.73it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  91% 5360/5899 [01:41<00:10, 52.83it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  91% 5380/5899 [01:41<00:09, 52.94it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  92% 5400/5899 [01:41<00:09, 53.05it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  92% 5420/5899 [01:41<00:09, 53.16it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  92% 5440/5899 [01:42<00:08, 53.27it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  93% 5460/5899 [01:42<00:08, 53.38it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  93% 5480/5899 [01:42<00:07, 53.48it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  93% 5500/5899 [01:42<00:07, 53.59it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  94% 5520/5899 [01:42<00:07, 53.70it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  94% 5540/5899 [01:42<00:06, 53.81it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  94% 5560/5899 [01:43<00:06, 53.92it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  95% 5580/5899 [01:43<00:05, 54.03it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  95% 5600/5899 [01:43<00:05, 54.14it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  95% 5620/5899 [01:43<00:05, 54.24it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  96% 5640/5899 [01:43<00:04, 54.35it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  96% 5660/5899 [01:43<00:04, 54.45it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  96% 5680/5899 [01:44<00:04, 54.56it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  97% 5700/5899 [01:44<00:03, 54.66it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  97% 5720/5899 [01:44<00:03, 54.77it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  97% 5740/5899 [01:44<00:02, 54.87it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  98% 5760/5899 [01:44<00:02, 54.97it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  98% 5780/5899 [01:44<00:02, 55.07it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  98% 5800/5899 [01:45<00:01, 55.17it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  99% 5820/5899 [01:45<00:01, 55.26it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  99% 5840/5899 [01:45<00:01, 55.35it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1:  99% 5860/5899 [01:45<00:00, 55.44it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1: 100% 5880/5899 [01:45<00:00, 55.55it/s, loss=5.03, v_num=6, train_loss_step=5.220, val_loss=6.220, avg_val_loss=6.220, train_loss_epoch=6.280]\n",
            "Epoch 1: 100% 5899/5899 [01:46<00:00, 55.64it/s, loss=5.02, v_num=6, train_loss_step=5.000, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=6.280]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 1.249 >= min_delta = 0.001. New best score: 4.970\n",
            "Epoch 2:  80% 4700/5899 [01:33<00:23, 50.43it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  80% 4720/5899 [01:36<00:23, 49.13it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  80% 4740/5899 [01:36<00:23, 49.24it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  81% 4760/5899 [01:36<00:23, 49.35it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  81% 4780/5899 [01:36<00:22, 49.47it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  81% 4800/5899 [01:36<00:22, 49.59it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  82% 4820/5899 [01:36<00:21, 49.70it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  82% 4840/5899 [01:37<00:21, 49.82it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  82% 4860/5899 [01:37<00:20, 49.94it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  83% 4880/5899 [01:37<00:20, 50.06it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  83% 4900/5899 [01:37<00:19, 50.18it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  83% 4920/5899 [01:37<00:19, 50.29it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  84% 4940/5899 [01:37<00:19, 50.41it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  84% 4960/5899 [01:38<00:18, 50.53it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  84% 4980/5899 [01:38<00:18, 50.64it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  85% 5000/5899 [01:38<00:17, 50.76it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  85% 5020/5899 [01:38<00:17, 50.88it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  85% 5040/5899 [01:38<00:16, 50.99it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  86% 5060/5899 [01:39<00:16, 51.11it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  86% 5080/5899 [01:39<00:15, 51.23it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  86% 5100/5899 [01:39<00:15, 51.34it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  87% 5120/5899 [01:39<00:15, 51.45it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  87% 5140/5899 [01:39<00:14, 51.57it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  87% 5160/5899 [01:39<00:14, 51.68it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  88% 5180/5899 [01:40<00:13, 51.79it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  88% 5200/5899 [01:40<00:13, 51.91it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  88% 5220/5899 [01:40<00:13, 52.02it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  89% 5240/5899 [01:40<00:12, 52.13it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  89% 5260/5899 [01:40<00:12, 52.24it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  90% 5280/5899 [01:40<00:11, 52.35it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  90% 5300/5899 [01:41<00:11, 52.46it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  90% 5320/5899 [01:41<00:11, 52.56it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  91% 5340/5899 [01:41<00:10, 52.67it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  91% 5360/5899 [01:41<00:10, 52.78it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  91% 5380/5899 [01:41<00:09, 52.88it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  92% 5400/5899 [01:41<00:09, 52.99it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  92% 5420/5899 [01:42<00:09, 53.10it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  92% 5440/5899 [01:42<00:08, 53.20it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  93% 5460/5899 [01:42<00:08, 53.30it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  93% 5480/5899 [01:42<00:07, 53.41it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  93% 5500/5899 [01:42<00:07, 53.52it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  94% 5520/5899 [01:42<00:07, 53.62it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  94% 5540/5899 [01:43<00:06, 53.72it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  94% 5560/5899 [01:43<00:06, 53.83it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  95% 5580/5899 [01:43<00:05, 53.93it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  95% 5600/5899 [01:43<00:05, 54.02it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  95% 5620/5899 [01:43<00:05, 54.12it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  96% 5640/5899 [01:44<00:04, 54.22it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  96% 5660/5899 [01:44<00:04, 54.33it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  96% 5680/5899 [01:44<00:04, 54.43it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  97% 5700/5899 [01:44<00:03, 54.54it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  97% 5720/5899 [01:44<00:03, 54.64it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  97% 5740/5899 [01:44<00:02, 54.73it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  98% 5760/5899 [01:45<00:02, 54.83it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  98% 5780/5899 [01:45<00:02, 54.92it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  98% 5800/5899 [01:45<00:01, 55.02it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  99% 5820/5899 [01:45<00:01, 55.11it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  99% 5840/5899 [01:45<00:01, 55.21it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2:  99% 5860/5899 [01:45<00:00, 55.31it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2: 100% 5880/5899 [01:46<00:00, 55.42it/s, loss=4.7, v_num=6, train_loss_step=4.840, val_loss=4.970, avg_val_loss=4.970, train_loss_epoch=5.270]\n",
            "Epoch 2: 100% 5899/5899 [01:46<00:00, 55.51it/s, loss=4.7, v_num=6, train_loss_step=4.700, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=5.270]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.330 >= min_delta = 0.001. New best score: 4.640\n",
            "Epoch 3:  80% 4700/5899 [01:33<00:23, 50.28it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  80% 4720/5899 [01:36<00:24, 48.85it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  80% 4740/5899 [01:36<00:23, 48.96it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  81% 4760/5899 [01:37<00:23, 49.07it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  81% 4780/5899 [01:37<00:22, 49.18it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  81% 4800/5899 [01:37<00:22, 49.29it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  82% 4820/5899 [01:37<00:21, 49.39it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  82% 4840/5899 [01:37<00:21, 49.51it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  82% 4860/5899 [01:37<00:20, 49.62it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  83% 4880/5899 [01:38<00:20, 49.73it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  83% 4900/5899 [01:38<00:20, 49.84it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  83% 4920/5899 [01:38<00:19, 49.96it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  84% 4940/5899 [01:38<00:19, 50.07it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  84% 4960/5899 [01:38<00:18, 50.18it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  84% 4980/5899 [01:39<00:18, 50.29it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  85% 5000/5899 [01:39<00:17, 50.40it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  85% 5020/5899 [01:39<00:17, 50.51it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  85% 5040/5899 [01:39<00:16, 50.63it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  86% 5060/5899 [01:39<00:16, 50.74it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  86% 5080/5899 [01:39<00:16, 50.86it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  86% 5100/5899 [01:40<00:15, 50.97it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  87% 5120/5899 [01:40<00:15, 51.08it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  87% 5140/5899 [01:40<00:14, 51.20it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  87% 5160/5899 [01:40<00:14, 51.31it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  88% 5180/5899 [01:40<00:13, 51.43it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  88% 5200/5899 [01:40<00:13, 51.54it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  88% 5220/5899 [01:41<00:13, 51.65it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  89% 5240/5899 [01:41<00:12, 51.76it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  89% 5260/5899 [01:41<00:12, 51.88it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  90% 5280/5899 [01:41<00:11, 51.99it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  90% 5300/5899 [01:41<00:11, 52.11it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  90% 5320/5899 [01:41<00:11, 52.22it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  91% 5340/5899 [01:42<00:10, 52.33it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  91% 5360/5899 [01:42<00:10, 52.44it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  91% 5380/5899 [01:42<00:09, 52.56it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  92% 5400/5899 [01:42<00:09, 52.67it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  92% 5420/5899 [01:42<00:09, 52.77it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  92% 5440/5899 [01:42<00:08, 52.89it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  93% 5460/5899 [01:43<00:08, 53.00it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  93% 5480/5899 [01:43<00:07, 53.11it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  93% 5500/5899 [01:43<00:07, 53.21it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  94% 5520/5899 [01:43<00:07, 53.31it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  94% 5540/5899 [01:43<00:06, 53.42it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  94% 5560/5899 [01:43<00:06, 53.52it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  95% 5580/5899 [01:44<00:05, 53.63it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  95% 5600/5899 [01:44<00:05, 53.74it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  95% 5620/5899 [01:44<00:05, 53.85it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  96% 5640/5899 [01:44<00:04, 53.96it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  96% 5660/5899 [01:44<00:04, 54.06it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  96% 5680/5899 [01:44<00:04, 54.16it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  97% 5700/5899 [01:45<00:03, 54.27it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  97% 5720/5899 [01:45<00:03, 54.37it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  97% 5740/5899 [01:45<00:02, 54.48it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  98% 5760/5899 [01:45<00:02, 54.59it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  98% 5780/5899 [01:45<00:02, 54.70it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  98% 5800/5899 [01:45<00:01, 54.80it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  99% 5820/5899 [01:46<00:01, 54.90it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  99% 5840/5899 [01:46<00:01, 55.01it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3:  99% 5860/5899 [01:46<00:00, 55.11it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3: 100% 5880/5899 [01:46<00:00, 55.21it/s, loss=4.58, v_num=6, train_loss_step=4.650, val_loss=4.640, avg_val_loss=4.640, train_loss_epoch=4.820]\n",
            "Epoch 3: 100% 5899/5899 [01:46<00:00, 55.31it/s, loss=4.58, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.820]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.115 >= min_delta = 0.001. New best score: 4.525\n",
            "Epoch 4:  80% 4700/5899 [01:34<00:24, 49.94it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  80% 4720/5899 [01:37<00:24, 48.45it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  80% 4740/5899 [01:37<00:23, 48.57it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  81% 4760/5899 [01:37<00:23, 48.68it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  81% 4780/5899 [01:37<00:22, 48.79it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  81% 4800/5899 [01:38<00:22, 48.90it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  82% 4820/5899 [01:38<00:22, 49.02it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  82% 4840/5899 [01:38<00:21, 49.13it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  82% 4860/5899 [01:38<00:21, 49.25it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  83% 4880/5899 [01:38<00:20, 49.37it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  83% 4900/5899 [01:39<00:20, 49.48it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  83% 4920/5899 [01:39<00:19, 49.59it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  84% 4940/5899 [01:39<00:19, 49.70it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  84% 4960/5899 [01:39<00:18, 49.82it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  84% 4980/5899 [01:39<00:18, 49.93it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  85% 5000/5899 [01:39<00:17, 50.05it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  85% 5020/5899 [01:40<00:17, 50.16it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  85% 5040/5899 [01:40<00:17, 50.28it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  86% 5060/5899 [01:40<00:16, 50.40it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  86% 5080/5899 [01:40<00:16, 50.51it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  86% 5100/5899 [01:40<00:15, 50.62it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  87% 5120/5899 [01:40<00:15, 50.73it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  87% 5140/5899 [01:41<00:14, 50.83it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  87% 5160/5899 [01:41<00:14, 50.94it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  88% 5180/5899 [01:41<00:14, 51.05it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  88% 5200/5899 [01:41<00:13, 51.17it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  88% 5220/5899 [01:41<00:13, 51.28it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  89% 5240/5899 [01:41<00:12, 51.39it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  89% 5260/5899 [01:42<00:12, 51.51it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  90% 5280/5899 [01:42<00:11, 51.62it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  90% 5300/5899 [01:42<00:11, 51.73it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  90% 5320/5899 [01:42<00:11, 51.85it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  91% 5340/5899 [01:42<00:10, 51.96it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  91% 5360/5899 [01:42<00:10, 52.07it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  91% 5380/5899 [01:43<00:09, 52.18it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  92% 5400/5899 [01:43<00:09, 52.29it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  92% 5420/5899 [01:43<00:09, 52.40it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  92% 5440/5899 [01:43<00:08, 52.51it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  93% 5460/5899 [01:43<00:08, 52.62it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  93% 5480/5899 [01:43<00:07, 52.72it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  93% 5500/5899 [01:44<00:07, 52.83it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  94% 5520/5899 [01:44<00:07, 52.94it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  94% 5540/5899 [01:44<00:06, 53.05it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  94% 5560/5899 [01:44<00:06, 53.16it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  95% 5580/5899 [01:44<00:05, 53.26it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  95% 5600/5899 [01:44<00:05, 53.37it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  95% 5620/5899 [01:45<00:05, 53.48it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  96% 5640/5899 [01:45<00:04, 53.58it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  96% 5660/5899 [01:45<00:04, 53.69it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  96% 5680/5899 [01:45<00:04, 53.80it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  97% 5700/5899 [01:45<00:03, 53.90it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  97% 5720/5899 [01:45<00:03, 54.01it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  97% 5740/5899 [01:46<00:02, 54.11it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  98% 5760/5899 [01:46<00:02, 54.22it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  98% 5780/5899 [01:46<00:02, 54.31it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  98% 5800/5899 [01:46<00:01, 54.41it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  99% 5820/5899 [01:46<00:01, 54.51it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  99% 5840/5899 [01:46<00:01, 54.62it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4:  99% 5860/5899 [01:47<00:00, 54.72it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4: 100% 5880/5899 [01:47<00:00, 54.82it/s, loss=4.52, v_num=6, train_loss_step=4.580, val_loss=4.530, avg_val_loss=4.530, train_loss_epoch=4.630]\n",
            "Epoch 4: 100% 5899/5899 [01:47<00:00, 54.92it/s, loss=4.52, v_num=6, train_loss_step=4.520, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.630]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.057 >= min_delta = 0.001. New best score: 4.468\n",
            "Epoch 5:  80% 4700/5899 [01:33<00:23, 50.27it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  80% 4720/5899 [01:36<00:24, 48.71it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  80% 4740/5899 [01:37<00:23, 48.82it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  81% 4760/5899 [01:37<00:23, 48.93it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  81% 4780/5899 [01:37<00:22, 49.05it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  81% 4800/5899 [01:37<00:22, 49.16it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  82% 4820/5899 [01:37<00:21, 49.27it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  82% 4840/5899 [01:38<00:21, 49.39it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  82% 4860/5899 [01:38<00:20, 49.50it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  83% 4880/5899 [01:38<00:20, 49.62it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  83% 4900/5899 [01:38<00:20, 49.73it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  83% 4920/5899 [01:38<00:19, 49.85it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  84% 4940/5899 [01:38<00:19, 49.96it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  84% 4960/5899 [01:39<00:18, 50.08it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  84% 4980/5899 [01:39<00:18, 50.19it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  85% 5000/5899 [01:39<00:17, 50.31it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  85% 5020/5899 [01:39<00:17, 50.43it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  85% 5040/5899 [01:39<00:16, 50.55it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  86% 5060/5899 [01:39<00:16, 50.66it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  86% 5080/5899 [01:40<00:16, 50.77it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  86% 5100/5899 [01:40<00:15, 50.89it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  87% 5120/5899 [01:40<00:15, 51.01it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  87% 5140/5899 [01:40<00:14, 51.12it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  87% 5160/5899 [01:40<00:14, 51.24it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  88% 5180/5899 [01:40<00:14, 51.35it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  88% 5200/5899 [01:41<00:13, 51.47it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  88% 5220/5899 [01:41<00:13, 51.57it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  89% 5240/5899 [01:41<00:12, 51.69it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  89% 5260/5899 [01:41<00:12, 51.80it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  90% 5280/5899 [01:41<00:11, 51.90it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  90% 5300/5899 [01:41<00:11, 52.01it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  90% 5320/5899 [01:42<00:11, 52.12it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  91% 5340/5899 [01:42<00:10, 52.23it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  91% 5360/5899 [01:42<00:10, 52.33it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  91% 5380/5899 [01:42<00:09, 52.43it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  92% 5400/5899 [01:42<00:09, 52.55it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  92% 5420/5899 [01:42<00:09, 52.66it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  92% 5440/5899 [01:43<00:08, 52.77it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  93% 5460/5899 [01:43<00:08, 52.87it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  93% 5480/5899 [01:43<00:07, 52.98it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  93% 5500/5899 [01:43<00:07, 53.08it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  94% 5520/5899 [01:43<00:07, 53.18it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  94% 5540/5899 [01:43<00:06, 53.29it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  94% 5560/5899 [01:44<00:06, 53.39it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  95% 5580/5899 [01:44<00:05, 53.50it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  95% 5600/5899 [01:44<00:05, 53.61it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  95% 5620/5899 [01:44<00:05, 53.72it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  96% 5640/5899 [01:44<00:04, 53.82it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  96% 5660/5899 [01:44<00:04, 53.93it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  96% 5680/5899 [01:45<00:04, 54.03it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  97% 5700/5899 [01:45<00:03, 54.13it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  97% 5720/5899 [01:45<00:03, 54.24it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  97% 5740/5899 [01:45<00:02, 54.35it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  98% 5760/5899 [01:45<00:02, 54.45it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  98% 5780/5899 [01:45<00:02, 54.56it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  98% 5800/5899 [01:46<00:01, 54.66it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  99% 5820/5899 [01:46<00:01, 54.77it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  99% 5840/5899 [01:46<00:01, 54.87it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5:  99% 5860/5899 [01:46<00:00, 54.98it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5: 100% 5880/5899 [01:46<00:00, 55.08it/s, loss=4.49, v_num=6, train_loss_step=4.530, val_loss=4.470, avg_val_loss=4.470, train_loss_epoch=4.550]\n",
            "Epoch 5: 100% 5899/5899 [01:46<00:00, 55.18it/s, loss=4.49, v_num=6, train_loss_step=4.490, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.550]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.030 >= min_delta = 0.001. New best score: 4.438\n",
            "Epoch 6:  80% 4700/5899 [01:34<00:24, 49.92it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  80% 4720/5899 [01:37<00:24, 48.32it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  80% 4740/5899 [01:37<00:23, 48.43it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  81% 4760/5899 [01:38<00:23, 48.55it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  81% 4780/5899 [01:38<00:22, 48.66it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  81% 4800/5899 [01:38<00:22, 48.78it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  82% 4820/5899 [01:38<00:22, 48.89it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  82% 4840/5899 [01:38<00:21, 49.01it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  82% 4860/5899 [01:38<00:21, 49.12it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  83% 4880/5899 [01:39<00:20, 49.24it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  83% 4900/5899 [01:39<00:20, 49.35it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  83% 4920/5899 [01:39<00:19, 49.47it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  84% 4940/5899 [01:39<00:19, 49.58it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  84% 4960/5899 [01:39<00:18, 49.69it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  84% 4980/5899 [01:39<00:18, 49.80it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  85% 5000/5899 [01:40<00:18, 49.92it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  85% 5020/5899 [01:40<00:17, 50.03it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  85% 5040/5899 [01:40<00:17, 50.14it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  86% 5060/5899 [01:40<00:16, 50.25it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  86% 5080/5899 [01:40<00:16, 50.37it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  86% 5100/5899 [01:41<00:15, 50.48it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  87% 5120/5899 [01:41<00:15, 50.60it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  87% 5140/5899 [01:41<00:14, 50.71it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  87% 5160/5899 [01:41<00:14, 50.82it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  88% 5180/5899 [01:41<00:14, 50.94it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  88% 5200/5899 [01:41<00:13, 51.05it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  88% 5220/5899 [01:42<00:13, 51.16it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  89% 5240/5899 [01:42<00:12, 51.27it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  89% 5260/5899 [01:42<00:12, 51.38it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  90% 5280/5899 [01:42<00:12, 51.49it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  90% 5300/5899 [01:42<00:11, 51.60it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  90% 5320/5899 [01:42<00:11, 51.71it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  91% 5340/5899 [01:43<00:10, 51.82it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  91% 5360/5899 [01:43<00:10, 51.93it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  91% 5380/5899 [01:43<00:09, 52.04it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  92% 5400/5899 [01:43<00:09, 52.15it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  92% 5420/5899 [01:43<00:09, 52.26it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  92% 5440/5899 [01:43<00:08, 52.37it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  93% 5460/5899 [01:44<00:08, 52.47it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  93% 5480/5899 [01:44<00:07, 52.57it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  93% 5500/5899 [01:44<00:07, 52.68it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  94% 5520/5899 [01:44<00:07, 52.78it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  94% 5540/5899 [01:44<00:06, 52.88it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  94% 5560/5899 [01:44<00:06, 52.99it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  95% 5580/5899 [01:45<00:06, 53.09it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  95% 5600/5899 [01:45<00:05, 53.20it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  95% 5620/5899 [01:45<00:05, 53.30it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  96% 5640/5899 [01:45<00:04, 53.40it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  96% 5660/5899 [01:45<00:04, 53.49it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  96% 5680/5899 [01:45<00:04, 53.59it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  97% 5700/5899 [01:46<00:03, 53.69it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  97% 5720/5899 [01:46<00:03, 53.79it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  97% 5740/5899 [01:46<00:02, 53.89it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  98% 5760/5899 [01:46<00:02, 53.99it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  98% 5780/5899 [01:46<00:02, 54.09it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  98% 5800/5899 [01:47<00:01, 54.19it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  99% 5820/5899 [01:47<00:01, 54.29it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  99% 5840/5899 [01:47<00:01, 54.39it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6:  99% 5860/5899 [01:47<00:00, 54.49it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6: 100% 5880/5899 [01:47<00:00, 54.59it/s, loss=4.47, v_num=6, train_loss_step=4.510, val_loss=4.440, avg_val_loss=4.440, train_loss_epoch=4.510]\n",
            "Epoch 6: 100% 5899/5899 [01:47<00:00, 54.69it/s, loss=4.47, v_num=6, train_loss_step=4.470, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.510]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.020 >= min_delta = 0.001. New best score: 4.418\n",
            "Epoch 7:  80% 4700/5899 [01:33<00:23, 50.07it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  80% 4720/5899 [01:37<00:24, 48.41it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  80% 4740/5899 [01:37<00:23, 48.51it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  81% 4760/5899 [01:37<00:23, 48.63it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  81% 4780/5899 [01:38<00:22, 48.74it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  81% 4800/5899 [01:38<00:22, 48.86it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  82% 4820/5899 [01:38<00:22, 48.98it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  82% 4840/5899 [01:38<00:21, 49.09it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  82% 4860/5899 [01:38<00:21, 49.19it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  83% 4880/5899 [01:38<00:20, 49.30it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  83% 4900/5899 [01:39<00:20, 49.41it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  83% 4920/5899 [01:39<00:19, 49.53it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  84% 4940/5899 [01:39<00:19, 49.64it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  84% 4960/5899 [01:39<00:18, 49.75it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  84% 4980/5899 [01:39<00:18, 49.87it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  85% 5000/5899 [01:40<00:17, 49.99it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  85% 5020/5899 [01:40<00:17, 50.10it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  85% 5040/5899 [01:40<00:17, 50.21it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  86% 5060/5899 [01:40<00:16, 50.32it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  86% 5080/5899 [01:40<00:16, 50.43it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  86% 5100/5899 [01:40<00:15, 50.55it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  87% 5120/5899 [01:41<00:15, 50.66it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  87% 5140/5899 [01:41<00:14, 50.77it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  87% 5160/5899 [01:41<00:14, 50.88it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  88% 5180/5899 [01:41<00:14, 50.99it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  88% 5200/5899 [01:41<00:13, 51.11it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  88% 5220/5899 [01:41<00:13, 51.22it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  89% 5240/5899 [01:42<00:12, 51.33it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  89% 5260/5899 [01:42<00:12, 51.44it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  90% 5280/5899 [01:42<00:12, 51.55it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  90% 5300/5899 [01:42<00:11, 51.66it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  90% 5320/5899 [01:42<00:11, 51.77it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  91% 5340/5899 [01:42<00:10, 51.88it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  91% 5360/5899 [01:43<00:10, 51.99it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  91% 5380/5899 [01:43<00:09, 52.10it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  92% 5400/5899 [01:43<00:09, 52.20it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  92% 5420/5899 [01:43<00:09, 52.31it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  92% 5440/5899 [01:43<00:08, 52.42it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  93% 5460/5899 [01:43<00:08, 52.53it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  93% 5480/5899 [01:44<00:07, 52.63it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  93% 5500/5899 [01:44<00:07, 52.73it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  94% 5520/5899 [01:44<00:07, 52.83it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  94% 5540/5899 [01:44<00:06, 52.93it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  94% 5560/5899 [01:44<00:06, 53.04it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  95% 5580/5899 [01:45<00:06, 53.14it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  95% 5600/5899 [01:45<00:05, 53.24it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  95% 5620/5899 [01:45<00:05, 53.34it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  96% 5640/5899 [01:45<00:04, 53.43it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  96% 5660/5899 [01:45<00:04, 53.53it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  96% 5680/5899 [01:45<00:04, 53.64it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  97% 5700/5899 [01:46<00:03, 53.74it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  97% 5720/5899 [01:46<00:03, 53.84it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  97% 5740/5899 [01:46<00:02, 53.94it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  98% 5760/5899 [01:46<00:02, 54.04it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  98% 5780/5899 [01:46<00:02, 54.15it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  98% 5800/5899 [01:46<00:01, 54.25it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  99% 5820/5899 [01:47<00:01, 54.35it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  99% 5840/5899 [01:47<00:01, 54.46it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7:  99% 5860/5899 [01:47<00:00, 54.56it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7: 100% 5880/5899 [01:47<00:00, 54.66it/s, loss=4.46, v_num=6, train_loss_step=4.490, val_loss=4.420, avg_val_loss=4.420, train_loss_epoch=4.480]\n",
            "Epoch 7: 100% 5899/5899 [01:47<00:00, 54.75it/s, loss=4.46, v_num=6, train_loss_step=4.460, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.480]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.011 >= min_delta = 0.001. New best score: 4.407\n",
            "Epoch 8:  80% 4700/5899 [01:34<00:24, 49.73it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  80% 4720/5899 [01:38<00:24, 48.08it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  80% 4740/5899 [01:38<00:24, 48.19it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  81% 4760/5899 [01:38<00:23, 48.30it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  81% 4780/5899 [01:38<00:23, 48.42it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  81% 4800/5899 [01:38<00:22, 48.54it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  82% 4820/5899 [01:39<00:22, 48.65it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  82% 4840/5899 [01:39<00:21, 48.76it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  82% 4860/5899 [01:39<00:21, 48.87it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  83% 4880/5899 [01:39<00:20, 48.98it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  83% 4900/5899 [01:39<00:20, 49.09it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  83% 4920/5899 [01:39<00:19, 49.21it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  84% 4940/5899 [01:40<00:19, 49.32it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  84% 4960/5899 [01:40<00:18, 49.43it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  84% 4980/5899 [01:40<00:18, 49.55it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  85% 5000/5899 [01:40<00:18, 49.66it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  85% 5020/5899 [01:40<00:17, 49.78it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  85% 5040/5899 [01:41<00:17, 49.89it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  86% 5060/5899 [01:41<00:16, 50.00it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  86% 5080/5899 [01:41<00:16, 50.11it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  86% 5100/5899 [01:41<00:15, 50.21it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  87% 5120/5899 [01:41<00:15, 50.32it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  87% 5140/5899 [01:41<00:15, 50.43it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  87% 5160/5899 [01:42<00:14, 50.54it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  88% 5180/5899 [01:42<00:14, 50.65it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  88% 5200/5899 [01:42<00:13, 50.76it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  88% 5220/5899 [01:42<00:13, 50.88it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  89% 5240/5899 [01:42<00:12, 50.99it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  89% 5260/5899 [01:42<00:12, 51.10it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  90% 5280/5899 [01:43<00:12, 51.20it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  90% 5300/5899 [01:43<00:11, 51.32it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  90% 5320/5899 [01:43<00:11, 51.43it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  91% 5340/5899 [01:43<00:10, 51.54it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  91% 5360/5899 [01:43<00:10, 51.66it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  91% 5380/5899 [01:43<00:10, 51.77it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  92% 5400/5899 [01:44<00:09, 51.88it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  92% 5420/5899 [01:44<00:09, 51.97it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  92% 5440/5899 [01:44<00:08, 52.07it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  93% 5460/5899 [01:44<00:08, 52.18it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  93% 5480/5899 [01:44<00:08, 52.29it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  93% 5500/5899 [01:44<00:07, 52.39it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  94% 5520/5899 [01:45<00:07, 52.50it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  94% 5540/5899 [01:45<00:06, 52.60it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  94% 5560/5899 [01:45<00:06, 52.70it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  95% 5580/5899 [01:45<00:06, 52.81it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  95% 5600/5899 [01:45<00:05, 52.91it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  95% 5620/5899 [01:45<00:05, 53.02it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  96% 5640/5899 [01:46<00:04, 53.13it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  96% 5660/5899 [01:46<00:04, 53.23it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  96% 5680/5899 [01:46<00:04, 53.34it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  97% 5700/5899 [01:46<00:03, 53.45it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  97% 5720/5899 [01:46<00:03, 53.55it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  97% 5740/5899 [01:46<00:02, 53.66it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  98% 5760/5899 [01:47<00:02, 53.76it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  98% 5780/5899 [01:47<00:02, 53.87it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  98% 5800/5899 [01:47<00:01, 53.98it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  99% 5820/5899 [01:47<00:01, 54.08it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  99% 5840/5899 [01:47<00:01, 54.18it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8:  99% 5860/5899 [01:47<00:00, 54.28it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8: 100% 5880/5899 [01:48<00:00, 54.39it/s, loss=4.45, v_num=6, train_loss_step=4.480, val_loss=4.410, avg_val_loss=4.410, train_loss_epoch=4.470]\n",
            "Epoch 8: 100% 5899/5899 [01:48<00:00, 54.48it/s, loss=4.45, v_num=6, train_loss_step=4.440, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.470]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.013 >= min_delta = 0.001. New best score: 4.394\n",
            "Epoch 9:  80% 4700/5899 [01:34<00:24, 49.49it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  80% 4720/5899 [01:38<00:24, 47.85it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  80% 4740/5899 [01:38<00:24, 47.96it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  81% 4760/5899 [01:39<00:23, 48.07it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  81% 4780/5899 [01:39<00:23, 48.18it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  81% 4800/5899 [01:39<00:22, 48.29it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  82% 4820/5899 [01:39<00:22, 48.40it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  82% 4840/5899 [01:39<00:21, 48.50it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  82% 4860/5899 [01:39<00:21, 48.61it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  83% 4880/5899 [01:40<00:20, 48.72it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  83% 4900/5899 [01:40<00:20, 48.83it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  83% 4920/5899 [01:40<00:20, 48.94it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  84% 4940/5899 [01:40<00:19, 49.05it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  84% 4960/5899 [01:40<00:19, 49.16it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  84% 4980/5899 [01:41<00:18, 49.28it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  85% 5000/5899 [01:41<00:18, 49.39it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  85% 5020/5899 [01:41<00:17, 49.50it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  85% 5040/5899 [01:41<00:17, 49.61it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  86% 5060/5899 [01:41<00:16, 49.73it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  86% 5080/5899 [01:41<00:16, 49.84it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  86% 5100/5899 [01:42<00:15, 49.95it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  87% 5120/5899 [01:42<00:15, 50.06it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  87% 5140/5899 [01:42<00:15, 50.17it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  87% 5160/5899 [01:42<00:14, 50.28it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  88% 5180/5899 [01:42<00:14, 50.39it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  88% 5200/5899 [01:42<00:13, 50.50it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  88% 5220/5899 [01:43<00:13, 50.60it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  89% 5240/5899 [01:43<00:12, 50.70it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  89% 5260/5899 [01:43<00:12, 50.81it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  90% 5280/5899 [01:43<00:12, 50.91it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  90% 5300/5899 [01:43<00:11, 51.02it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  90% 5320/5899 [01:44<00:11, 51.13it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  91% 5340/5899 [01:44<00:10, 51.24it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  91% 5360/5899 [01:44<00:10, 51.35it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  91% 5380/5899 [01:44<00:10, 51.46it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  92% 5400/5899 [01:44<00:09, 51.56it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  92% 5420/5899 [01:44<00:09, 51.67it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  92% 5440/5899 [01:45<00:08, 51.78it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  93% 5460/5899 [01:45<00:08, 51.88it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  93% 5480/5899 [01:45<00:08, 51.99it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  93% 5500/5899 [01:45<00:07, 52.10it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  94% 5520/5899 [01:45<00:07, 52.21it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  94% 5540/5899 [01:45<00:06, 52.31it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  94% 5560/5899 [01:46<00:06, 52.41it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  95% 5580/5899 [01:46<00:06, 52.52it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  95% 5600/5899 [01:46<00:05, 52.63it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  95% 5620/5899 [01:46<00:05, 52.73it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  96% 5640/5899 [01:46<00:04, 52.84it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  96% 5660/5899 [01:46<00:04, 52.95it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  96% 5680/5899 [01:47<00:04, 53.06it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  97% 5700/5899 [01:47<00:03, 53.17it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  97% 5720/5899 [01:47<00:03, 53.27it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  97% 5740/5899 [01:47<00:02, 53.38it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  98% 5760/5899 [01:47<00:02, 53.49it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  98% 5780/5899 [01:47<00:02, 53.59it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  98% 5800/5899 [01:48<00:01, 53.70it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  99% 5820/5899 [01:48<00:01, 53.80it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  99% 5840/5899 [01:48<00:01, 53.90it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9:  99% 5860/5899 [01:48<00:00, 54.00it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9: 100% 5880/5899 [01:48<00:00, 54.10it/s, loss=4.44, v_num=6, train_loss_step=4.460, val_loss=4.390, avg_val_loss=4.390, train_loss_epoch=4.460]\n",
            "Epoch 9: 100% 5899/5899 [01:48<00:00, 54.20it/s, loss=4.44, v_num=6, train_loss_step=4.430, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.460]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.010 >= min_delta = 0.001. New best score: 4.384\n",
            "Epoch 10:  80% 4700/5899 [01:35<00:24, 49.07it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  80% 4720/5899 [01:39<00:24, 47.46it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  80% 4740/5899 [01:39<00:24, 47.57it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  81% 4760/5899 [01:39<00:23, 47.69it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  81% 4780/5899 [01:39<00:23, 47.80it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  81% 4800/5899 [01:40<00:22, 47.92it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  82% 4820/5899 [01:40<00:22, 48.03it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  82% 4840/5899 [01:40<00:21, 48.15it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  82% 4860/5899 [01:40<00:21, 48.26it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  83% 4880/5899 [01:40<00:21, 48.38it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  83% 4900/5899 [01:41<00:20, 48.50it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  83% 4920/5899 [01:41<00:20, 48.62it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  84% 4940/5899 [01:41<00:19, 48.73it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  84% 4960/5899 [01:41<00:19, 48.84it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  84% 4980/5899 [01:41<00:18, 48.95it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  85% 5000/5899 [01:41<00:18, 49.07it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  85% 5020/5899 [01:42<00:17, 49.19it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  85% 5040/5899 [01:42<00:17, 49.30it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  86% 5060/5899 [01:42<00:16, 49.41it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  86% 5080/5899 [01:42<00:16, 49.52it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  86% 5100/5899 [01:42<00:16, 49.63it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  87% 5120/5899 [01:42<00:15, 49.74it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  87% 5140/5899 [01:43<00:15, 49.85it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  87% 5160/5899 [01:43<00:14, 49.95it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  88% 5180/5899 [01:43<00:14, 50.06it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  88% 5200/5899 [01:43<00:13, 50.17it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  88% 5220/5899 [01:43<00:13, 50.27it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  89% 5240/5899 [01:44<00:13, 50.37it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  89% 5260/5899 [01:44<00:12, 50.48it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  90% 5280/5899 [01:44<00:12, 50.58it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  90% 5300/5899 [01:44<00:11, 50.68it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  90% 5320/5899 [01:44<00:11, 50.79it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  91% 5340/5899 [01:44<00:10, 50.90it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  91% 5360/5899 [01:45<00:10, 51.01it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  91% 5380/5899 [01:45<00:10, 51.11it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  92% 5400/5899 [01:45<00:09, 51.22it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  92% 5420/5899 [01:45<00:09, 51.32it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  92% 5440/5899 [01:45<00:08, 51.42it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  93% 5460/5899 [01:45<00:08, 51.52it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  93% 5480/5899 [01:46<00:08, 51.63it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  93% 5500/5899 [01:46<00:07, 51.72it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  94% 5520/5899 [01:46<00:07, 51.83it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  94% 5540/5899 [01:46<00:06, 51.92it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  94% 5560/5899 [01:46<00:06, 52.02it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  95% 5580/5899 [01:47<00:06, 52.13it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  95% 5600/5899 [01:47<00:05, 52.23it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  95% 5620/5899 [01:47<00:05, 52.34it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  96% 5640/5899 [01:47<00:04, 52.44it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  96% 5660/5899 [01:47<00:04, 52.55it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  96% 5680/5899 [01:47<00:04, 52.65it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  97% 5700/5899 [01:48<00:03, 52.76it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  97% 5720/5899 [01:48<00:03, 52.86it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  97% 5740/5899 [01:48<00:03, 52.96it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  98% 5760/5899 [01:48<00:02, 53.06it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  98% 5780/5899 [01:48<00:02, 53.16it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  98% 5800/5899 [01:48<00:01, 53.26it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  99% 5820/5899 [01:49<00:01, 53.36it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  99% 5840/5899 [01:49<00:01, 53.46it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10:  99% 5860/5899 [01:49<00:00, 53.56it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10: 100% 5880/5899 [01:49<00:00, 53.66it/s, loss=4.43, v_num=6, train_loss_step=4.450, val_loss=4.380, avg_val_loss=4.380, train_loss_epoch=4.440]\n",
            "Epoch 10: 100% 5899/5899 [01:49<00:00, 53.75it/s, loss=4.43, v_num=6, train_loss_step=4.420, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.440]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.010 >= min_delta = 0.001. New best score: 4.374\n",
            "Epoch 11:  80% 4700/5899 [01:35<00:24, 49.44it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  80% 4720/5899 [01:38<00:24, 47.79it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  80% 4740/5899 [01:38<00:24, 47.89it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  81% 4760/5899 [01:39<00:23, 48.01it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  81% 4780/5899 [01:39<00:23, 48.12it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  81% 4800/5899 [01:39<00:22, 48.24it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  82% 4820/5899 [01:39<00:22, 48.35it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  82% 4840/5899 [01:39<00:21, 48.47it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  82% 4860/5899 [01:40<00:21, 48.58it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  83% 4880/5899 [01:40<00:20, 48.69it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  83% 4900/5899 [01:40<00:20, 48.81it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  83% 4920/5899 [01:40<00:20, 48.92it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  84% 4940/5899 [01:40<00:19, 49.03it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  84% 4960/5899 [01:40<00:19, 49.14it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  84% 4980/5899 [01:41<00:18, 49.25it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  85% 5000/5899 [01:41<00:18, 49.36it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  85% 5020/5899 [01:41<00:17, 49.47it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  85% 5040/5899 [01:41<00:17, 49.58it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  86% 5060/5899 [01:41<00:16, 49.68it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  86% 5080/5899 [01:42<00:16, 49.78it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  86% 5100/5899 [01:42<00:16, 49.90it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  87% 5120/5899 [01:42<00:15, 50.01it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  87% 5140/5899 [01:42<00:15, 50.12it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  87% 5160/5899 [01:42<00:14, 50.23it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  88% 5180/5899 [01:42<00:14, 50.34it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  88% 5200/5899 [01:43<00:13, 50.45it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  88% 5220/5899 [01:43<00:13, 50.55it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  89% 5240/5899 [01:43<00:13, 50.66it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  89% 5260/5899 [01:43<00:12, 50.77it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  90% 5280/5899 [01:43<00:12, 50.87it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  90% 5300/5899 [01:43<00:11, 50.97it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  90% 5320/5899 [01:44<00:11, 51.08it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  91% 5340/5899 [01:44<00:10, 51.19it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  91% 5360/5899 [01:44<00:10, 51.29it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  91% 5380/5899 [01:44<00:10, 51.39it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  92% 5400/5899 [01:44<00:09, 51.50it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  92% 5420/5899 [01:45<00:09, 51.60it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  92% 5440/5899 [01:45<00:08, 51.71it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  93% 5460/5899 [01:45<00:08, 51.81it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  93% 5480/5899 [01:45<00:08, 51.91it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  93% 5500/5899 [01:45<00:07, 52.02it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  94% 5520/5899 [01:45<00:07, 52.11it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  94% 5540/5899 [01:46<00:06, 52.21it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  94% 5560/5899 [01:46<00:06, 52.31it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  95% 5580/5899 [01:46<00:06, 52.41it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  95% 5600/5899 [01:46<00:05, 52.52it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  95% 5620/5899 [01:46<00:05, 52.62it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  96% 5640/5899 [01:46<00:04, 52.72it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  96% 5660/5899 [01:47<00:04, 52.82it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  96% 5680/5899 [01:47<00:04, 52.93it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  97% 5700/5899 [01:47<00:03, 53.04it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  97% 5720/5899 [01:47<00:03, 53.14it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  97% 5740/5899 [01:47<00:02, 53.25it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  98% 5760/5899 [01:47<00:02, 53.35it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  98% 5780/5899 [01:48<00:02, 53.46it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  98% 5800/5899 [01:48<00:01, 53.56it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  99% 5820/5899 [01:48<00:01, 53.66it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  99% 5840/5899 [01:48<00:01, 53.77it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11:  99% 5860/5899 [01:48<00:00, 53.86it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11: 100% 5880/5899 [01:48<00:00, 53.95it/s, loss=4.42, v_num=6, train_loss_step=4.440, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "Epoch 11: 100% 5899/5899 [01:49<00:00, 54.05it/s, loss=4.42, v_num=6, train_loss_step=4.420, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.430]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.007 >= min_delta = 0.001. New best score: 4.366\n",
            "Epoch 12:  80% 4700/5899 [01:36<00:24, 48.63it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  80% 4720/5899 [01:40<00:25, 46.92it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  80% 4740/5899 [01:40<00:24, 47.03it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  81% 4760/5899 [01:40<00:24, 47.14it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  81% 4780/5899 [01:41<00:23, 47.25it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  81% 4800/5899 [01:41<00:23, 47.37it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  82% 4820/5899 [01:41<00:22, 47.48it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  82% 4840/5899 [01:41<00:22, 47.59it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  82% 4860/5899 [01:41<00:21, 47.71it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  83% 4880/5899 [01:42<00:21, 47.82it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  83% 4900/5899 [01:42<00:20, 47.94it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  83% 4920/5899 [01:42<00:20, 48.06it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  84% 4940/5899 [01:42<00:19, 48.17it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  84% 4960/5899 [01:42<00:19, 48.29it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  84% 4980/5899 [01:42<00:18, 48.40it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  85% 5000/5899 [01:43<00:18, 48.52it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  85% 5020/5899 [01:43<00:18, 48.64it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  85% 5040/5899 [01:43<00:17, 48.75it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  86% 5060/5899 [01:43<00:17, 48.86it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  86% 5080/5899 [01:43<00:16, 48.97it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  86% 5100/5899 [01:43<00:16, 49.08it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  87% 5120/5899 [01:44<00:15, 49.19it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  87% 5140/5899 [01:44<00:15, 49.30it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  87% 5160/5899 [01:44<00:14, 49.41it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  88% 5180/5899 [01:44<00:14, 49.51it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  88% 5200/5899 [01:44<00:14, 49.62it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  88% 5220/5899 [01:44<00:13, 49.72it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  89% 5240/5899 [01:45<00:13, 49.83it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  89% 5260/5899 [01:45<00:12, 49.94it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  90% 5280/5899 [01:45<00:12, 50.04it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  90% 5300/5899 [01:45<00:11, 50.15it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  90% 5320/5899 [01:45<00:11, 50.25it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  91% 5340/5899 [01:46<00:11, 50.35it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  91% 5360/5899 [01:46<00:10, 50.45it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  91% 5380/5899 [01:46<00:10, 50.56it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  92% 5400/5899 [01:46<00:09, 50.67it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  92% 5420/5899 [01:46<00:09, 50.78it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  92% 5440/5899 [01:46<00:09, 50.89it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  93% 5460/5899 [01:47<00:08, 50.99it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  93% 5480/5899 [01:47<00:08, 51.10it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  93% 5500/5899 [01:47<00:07, 51.21it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  94% 5520/5899 [01:47<00:07, 51.31it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  94% 5540/5899 [01:47<00:06, 51.42it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  94% 5560/5899 [01:47<00:06, 51.52it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  95% 5580/5899 [01:48<00:06, 51.63it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  95% 5600/5899 [01:48<00:05, 51.73it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  95% 5620/5899 [01:48<00:05, 51.84it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  96% 5640/5899 [01:48<00:04, 51.94it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  96% 5660/5899 [01:48<00:04, 52.04it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  96% 5680/5899 [01:48<00:04, 52.14it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  97% 5700/5899 [01:49<00:03, 52.24it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  97% 5720/5899 [01:49<00:03, 52.34it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  97% 5740/5899 [01:49<00:03, 52.44it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  98% 5760/5899 [01:49<00:02, 52.54it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  98% 5780/5899 [01:49<00:02, 52.65it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  98% 5800/5899 [01:49<00:01, 52.75it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  99% 5820/5899 [01:50<00:01, 52.84it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  99% 5840/5899 [01:50<00:01, 52.94it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12:  99% 5860/5899 [01:50<00:00, 53.04it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12: 100% 5880/5899 [01:50<00:00, 53.14it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.370, avg_val_loss=4.370, train_loss_epoch=4.420]\n",
            "Epoch 12: 100% 5899/5899 [01:50<00:00, 53.24it/s, loss=4.42, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.004 >= min_delta = 0.001. New best score: 4.362\n",
            "Epoch 13:  80% 4700/5899 [01:35<00:24, 49.18it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  80% 4720/5899 [01:39<00:24, 47.52it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  80% 4740/5899 [01:39<00:24, 47.63it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  81% 4760/5899 [01:39<00:23, 47.74it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  81% 4780/5899 [01:39<00:23, 47.85it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  81% 4800/5899 [01:40<00:22, 47.97it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  82% 4820/5899 [01:40<00:22, 48.09it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  82% 4840/5899 [01:40<00:21, 48.20it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  82% 4860/5899 [01:40<00:21, 48.31it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  83% 4880/5899 [01:40<00:21, 48.43it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  83% 4900/5899 [01:40<00:20, 48.55it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  83% 4920/5899 [01:41<00:20, 48.66it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  84% 4940/5899 [01:41<00:19, 48.77it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  84% 4960/5899 [01:41<00:19, 48.89it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  84% 4980/5899 [01:41<00:18, 49.00it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  85% 5000/5899 [01:41<00:18, 49.11it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  85% 5020/5899 [01:42<00:17, 49.22it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  85% 5040/5899 [01:42<00:17, 49.32it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  86% 5060/5899 [01:42<00:16, 49.43it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  86% 5080/5899 [01:42<00:16, 49.54it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  86% 5100/5899 [01:42<00:16, 49.64it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  87% 5120/5899 [01:42<00:15, 49.75it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  87% 5140/5899 [01:43<00:15, 49.85it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  87% 5160/5899 [01:43<00:14, 49.96it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  88% 5180/5899 [01:43<00:14, 50.06it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  88% 5200/5899 [01:43<00:13, 50.16it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  88% 5220/5899 [01:43<00:13, 50.27it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  89% 5240/5899 [01:44<00:13, 50.37it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  89% 5260/5899 [01:44<00:12, 50.48it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  90% 5280/5899 [01:44<00:12, 50.58it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  90% 5300/5899 [01:44<00:11, 50.68it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  90% 5320/5899 [01:44<00:11, 50.78it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  91% 5340/5899 [01:44<00:10, 50.87it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  91% 5360/5899 [01:45<00:10, 50.97it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  91% 5380/5899 [01:45<00:10, 51.08it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  92% 5400/5899 [01:45<00:09, 51.18it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  92% 5420/5899 [01:45<00:09, 51.29it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  92% 5440/5899 [01:45<00:08, 51.39it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  93% 5460/5899 [01:46<00:08, 51.50it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  93% 5480/5899 [01:46<00:08, 51.61it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  93% 5500/5899 [01:46<00:07, 51.71it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  94% 5520/5899 [01:46<00:07, 51.82it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  94% 5540/5899 [01:46<00:06, 51.93it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  94% 5560/5899 [01:46<00:06, 52.03it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  95% 5580/5899 [01:47<00:06, 52.13it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  95% 5600/5899 [01:47<00:05, 52.23it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  95% 5620/5899 [01:47<00:05, 52.33it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  96% 5640/5899 [01:47<00:04, 52.43it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  96% 5660/5899 [01:47<00:04, 52.53it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  96% 5680/5899 [01:47<00:04, 52.63it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  97% 5700/5899 [01:48<00:03, 52.73it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  97% 5720/5899 [01:48<00:03, 52.83it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  97% 5740/5899 [01:48<00:03, 52.94it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  98% 5760/5899 [01:48<00:02, 53.04it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  98% 5780/5899 [01:48<00:02, 53.14it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  98% 5800/5899 [01:48<00:01, 53.24it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  99% 5820/5899 [01:49<00:01, 53.34it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  99% 5840/5899 [01:49<00:01, 53.43it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13:  99% 5860/5899 [01:49<00:00, 53.53it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13: 100% 5880/5899 [01:49<00:00, 53.63it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "Epoch 13: 100% 5899/5899 [01:49<00:00, 53.72it/s, loss=4.41, v_num=6, train_loss_step=4.410, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.420]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.003 >= min_delta = 0.001. New best score: 4.359\n",
            "Epoch 14:  80% 4700/5899 [01:35<00:24, 49.00it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  80% 4720/5899 [01:39<00:24, 47.32it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  80% 4740/5899 [01:39<00:24, 47.43it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  81% 4760/5899 [01:40<00:23, 47.54it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  81% 4780/5899 [01:40<00:23, 47.66it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  81% 4800/5899 [01:40<00:23, 47.77it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  82% 4820/5899 [01:40<00:22, 47.89it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  82% 4840/5899 [01:40<00:22, 48.00it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  82% 4860/5899 [01:40<00:21, 48.12it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  83% 4880/5899 [01:41<00:21, 48.24it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  83% 4900/5899 [01:41<00:20, 48.35it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  83% 4920/5899 [01:41<00:20, 48.47it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  84% 4940/5899 [01:41<00:19, 48.59it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  84% 4960/5899 [01:41<00:19, 48.70it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  84% 4980/5899 [01:42<00:18, 48.82it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  85% 5000/5899 [01:42<00:18, 48.93it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  85% 5020/5899 [01:42<00:17, 49.05it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  85% 5040/5899 [01:42<00:17, 49.16it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  86% 5060/5899 [01:42<00:17, 49.28it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  86% 5080/5899 [01:42<00:16, 49.39it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  86% 5100/5899 [01:43<00:16, 49.51it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  87% 5120/5899 [01:43<00:15, 49.61it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  87% 5140/5899 [01:43<00:15, 49.72it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  87% 5160/5899 [01:43<00:14, 49.83it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  88% 5180/5899 [01:43<00:14, 49.95it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  88% 5200/5899 [01:43<00:13, 50.06it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  88% 5220/5899 [01:44<00:13, 50.16it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  89% 5240/5899 [01:44<00:13, 50.27it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  89% 5260/5899 [01:44<00:12, 50.38it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  90% 5280/5899 [01:44<00:12, 50.49it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  90% 5300/5899 [01:44<00:11, 50.59it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  90% 5320/5899 [01:44<00:11, 50.69it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  91% 5340/5899 [01:45<00:11, 50.80it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  91% 5360/5899 [01:45<00:10, 50.90it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  91% 5380/5899 [01:45<00:10, 51.00it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  92% 5400/5899 [01:45<00:09, 51.10it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  92% 5420/5899 [01:45<00:09, 51.21it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  92% 5440/5899 [01:45<00:08, 51.32it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  93% 5460/5899 [01:46<00:08, 51.43it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  93% 5480/5899 [01:46<00:08, 51.54it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  93% 5500/5899 [01:46<00:07, 51.64it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  94% 5520/5899 [01:46<00:07, 51.75it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  94% 5540/5899 [01:46<00:06, 51.86it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  94% 5560/5899 [01:46<00:06, 51.97it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  95% 5580/5899 [01:47<00:06, 52.07it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  95% 5600/5899 [01:47<00:05, 52.18it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  95% 5620/5899 [01:47<00:05, 52.29it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  96% 5640/5899 [01:47<00:04, 52.39it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  96% 5660/5899 [01:47<00:04, 52.49it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  96% 5680/5899 [01:47<00:04, 52.59it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  97% 5700/5899 [01:48<00:03, 52.69it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  97% 5720/5899 [01:48<00:03, 52.79it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  97% 5740/5899 [01:48<00:03, 52.90it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  98% 5760/5899 [01:48<00:02, 53.00it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  98% 5780/5899 [01:48<00:02, 53.10it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  98% 5800/5899 [01:49<00:01, 53.20it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  99% 5820/5899 [01:49<00:01, 53.30it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  99% 5840/5899 [01:49<00:01, 53.39it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14:  99% 5860/5899 [01:49<00:00, 53.49it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14: 100% 5880/5899 [01:49<00:00, 53.59it/s, loss=4.41, v_num=6, train_loss_step=4.430, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 14: 100% 5899/5899 [01:49<00:00, 53.69it/s, loss=4.41, v_num=6, train_loss_step=4.410, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.357\n",
            "Epoch 15:  80% 4700/5899 [01:34<00:24, 49.50it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  80% 4720/5899 [01:38<00:24, 47.81it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  80% 4740/5899 [01:38<00:24, 47.92it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  81% 4760/5899 [01:39<00:23, 48.03it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  81% 4780/5899 [01:39<00:23, 48.15it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  81% 4800/5899 [01:39<00:22, 48.26it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  82% 4820/5899 [01:39<00:22, 48.38it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  82% 4840/5899 [01:39<00:21, 48.50it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  82% 4860/5899 [01:39<00:21, 48.62it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  83% 4880/5899 [01:40<00:20, 48.73it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  83% 4900/5899 [01:40<00:20, 48.85it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  83% 4920/5899 [01:40<00:19, 48.96it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  84% 4940/5899 [01:40<00:19, 49.08it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  84% 4960/5899 [01:40<00:19, 49.19it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  84% 4980/5899 [01:41<00:18, 49.30it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  85% 5000/5899 [01:41<00:18, 49.42it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  85% 5020/5899 [01:41<00:17, 49.53it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  85% 5040/5899 [01:41<00:17, 49.65it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  86% 5060/5899 [01:41<00:16, 49.76it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  86% 5080/5899 [01:41<00:16, 49.87it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  86% 5100/5899 [01:42<00:15, 49.99it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  87% 5120/5899 [01:42<00:15, 50.10it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  87% 5140/5899 [01:42<00:15, 50.21it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  87% 5160/5899 [01:42<00:14, 50.32it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  88% 5180/5899 [01:42<00:14, 50.43it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  88% 5200/5899 [01:42<00:13, 50.53it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  88% 5220/5899 [01:43<00:13, 50.64it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  89% 5240/5899 [01:43<00:12, 50.75it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  89% 5260/5899 [01:43<00:12, 50.86it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  90% 5280/5899 [01:43<00:12, 50.96it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  90% 5300/5899 [01:43<00:11, 51.06it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  90% 5320/5899 [01:43<00:11, 51.16it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  91% 5340/5899 [01:44<00:10, 51.26it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  91% 5360/5899 [01:44<00:10, 51.36it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  91% 5380/5899 [01:44<00:10, 51.46it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  92% 5400/5899 [01:44<00:09, 51.57it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  92% 5420/5899 [01:44<00:09, 51.67it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  92% 5440/5899 [01:45<00:08, 51.78it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  93% 5460/5899 [01:45<00:08, 51.89it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  93% 5480/5899 [01:45<00:08, 51.99it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  93% 5500/5899 [01:45<00:07, 52.09it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  94% 5520/5899 [01:45<00:07, 52.18it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  94% 5540/5899 [01:45<00:06, 52.29it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  94% 5560/5899 [01:46<00:06, 52.39it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  95% 5580/5899 [01:46<00:06, 52.49it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  95% 5600/5899 [01:46<00:05, 52.60it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  95% 5620/5899 [01:46<00:05, 52.70it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  96% 5640/5899 [01:46<00:04, 52.81it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  96% 5660/5899 [01:46<00:04, 52.91it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  96% 5680/5899 [01:47<00:04, 53.02it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  97% 5700/5899 [01:47<00:03, 53.12it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  97% 5720/5899 [01:47<00:03, 53.22it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  97% 5740/5899 [01:47<00:02, 53.33it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  98% 5760/5899 [01:47<00:02, 53.43it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  98% 5780/5899 [01:47<00:02, 53.53it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  98% 5800/5899 [01:48<00:01, 53.64it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  99% 5820/5899 [01:48<00:01, 53.74it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  99% 5840/5899 [01:48<00:01, 53.83it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15:  99% 5860/5899 [01:48<00:00, 53.94it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15: 100% 5880/5899 [01:48<00:00, 54.04it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 15: 100% 5899/5899 [01:48<00:00, 54.13it/s, loss=4.41, v_num=6, train_loss_step=4.410, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.355\n",
            "Epoch 16:  80% 4700/5899 [01:35<00:24, 49.29it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  80% 4720/5899 [01:39<00:24, 47.60it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  80% 4740/5899 [01:39<00:24, 47.72it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  81% 4760/5899 [01:39<00:23, 47.83it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  81% 4780/5899 [01:39<00:23, 47.94it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  81% 4800/5899 [01:39<00:22, 48.06it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  82% 4820/5899 [01:40<00:22, 48.18it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  82% 4840/5899 [01:40<00:21, 48.29it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  82% 4860/5899 [01:40<00:21, 48.40it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  83% 4880/5899 [01:40<00:21, 48.52it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  83% 4900/5899 [01:40<00:20, 48.63it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  83% 4920/5899 [01:40<00:20, 48.74it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  84% 4940/5899 [01:41<00:19, 48.86it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  84% 4960/5899 [01:41<00:19, 48.97it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  84% 4980/5899 [01:41<00:18, 49.08it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  85% 5000/5899 [01:41<00:18, 49.20it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  85% 5020/5899 [01:41<00:17, 49.31it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  85% 5040/5899 [01:41<00:17, 49.42it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  86% 5060/5899 [01:42<00:16, 49.54it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  86% 5080/5899 [01:42<00:16, 49.65it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  86% 5100/5899 [01:42<00:16, 49.76it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  87% 5120/5899 [01:42<00:15, 49.87it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  87% 5140/5899 [01:42<00:15, 49.98it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  87% 5160/5899 [01:43<00:14, 50.09it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  88% 5180/5899 [01:43<00:14, 50.20it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  88% 5200/5899 [01:43<00:13, 50.31it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  88% 5220/5899 [01:43<00:13, 50.42it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  89% 5240/5899 [01:43<00:13, 50.53it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  89% 5260/5899 [01:43<00:12, 50.64it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  90% 5280/5899 [01:44<00:12, 50.75it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  90% 5300/5899 [01:44<00:11, 50.86it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  90% 5320/5899 [01:44<00:11, 50.97it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  91% 5340/5899 [01:44<00:10, 51.08it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  91% 5360/5899 [01:44<00:10, 51.19it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  91% 5380/5899 [01:44<00:10, 51.30it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  92% 5400/5899 [01:45<00:09, 51.40it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  92% 5420/5899 [01:45<00:09, 51.50it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  92% 5440/5899 [01:45<00:08, 51.61it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  93% 5460/5899 [01:45<00:08, 51.72it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  93% 5480/5899 [01:45<00:08, 51.82it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  93% 5500/5899 [01:45<00:07, 51.92it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  94% 5520/5899 [01:46<00:07, 52.01it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  94% 5540/5899 [01:46<00:06, 52.12it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  94% 5560/5899 [01:46<00:06, 52.22it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  95% 5580/5899 [01:46<00:06, 52.32it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  95% 5600/5899 [01:46<00:05, 52.42it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  95% 5620/5899 [01:47<00:05, 52.52it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  96% 5640/5899 [01:47<00:04, 52.61it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  96% 5660/5899 [01:47<00:04, 52.71it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  96% 5680/5899 [01:47<00:04, 52.82it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  97% 5700/5899 [01:47<00:03, 52.91it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  97% 5720/5899 [01:47<00:03, 53.01it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  97% 5740/5899 [01:48<00:02, 53.11it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  98% 5760/5899 [01:48<00:02, 53.21it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  98% 5780/5899 [01:48<00:02, 53.31it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  98% 5800/5899 [01:48<00:01, 53.40it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  99% 5820/5899 [01:48<00:01, 53.50it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  99% 5840/5899 [01:48<00:01, 53.60it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16:  99% 5860/5899 [01:49<00:00, 53.70it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16: 100% 5880/5899 [01:49<00:00, 53.80it/s, loss=4.41, v_num=6, train_loss_step=4.420, val_loss=4.360, avg_val_loss=4.360, train_loss_epoch=4.410]\n",
            "Epoch 16: 100% 5899/5899 [01:49<00:00, 53.89it/s, loss=4.41, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.353\n",
            "Epoch 17:  80% 4700/5899 [01:34<00:24, 49.59it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  80% 4720/5899 [01:38<00:24, 47.83it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  80% 4740/5899 [01:38<00:24, 47.94it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  81% 4760/5899 [01:39<00:23, 48.05it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  81% 4780/5899 [01:39<00:23, 48.16it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  81% 4800/5899 [01:39<00:22, 48.28it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  82% 4820/5899 [01:39<00:22, 48.39it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  82% 4840/5899 [01:39<00:21, 48.50it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  82% 4860/5899 [01:39<00:21, 48.61it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  83% 4880/5899 [01:40<00:20, 48.72it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  83% 4900/5899 [01:40<00:20, 48.84it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  83% 4920/5899 [01:40<00:19, 48.95it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  84% 4940/5899 [01:40<00:19, 49.06it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  84% 4960/5899 [01:40<00:19, 49.18it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  84% 4980/5899 [01:41<00:18, 49.29it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  85% 5000/5899 [01:41<00:18, 49.40it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  85% 5020/5899 [01:41<00:17, 49.51it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  85% 5040/5899 [01:41<00:17, 49.62it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  86% 5060/5899 [01:41<00:16, 49.73it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  86% 5080/5899 [01:41<00:16, 49.84it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  86% 5100/5899 [01:42<00:15, 49.95it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  87% 5120/5899 [01:42<00:15, 50.05it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  87% 5140/5899 [01:42<00:15, 50.16it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  87% 5160/5899 [01:42<00:14, 50.27it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  88% 5180/5899 [01:42<00:14, 50.37it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  88% 5200/5899 [01:43<00:13, 50.48it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  88% 5220/5899 [01:43<00:13, 50.59it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  89% 5240/5899 [01:43<00:12, 50.70it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  89% 5260/5899 [01:43<00:12, 50.81it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  90% 5280/5899 [01:43<00:12, 50.92it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  90% 5300/5899 [01:43<00:11, 51.03it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  90% 5320/5899 [01:44<00:11, 51.14it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  91% 5340/5899 [01:44<00:10, 51.25it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  91% 5360/5899 [01:44<00:10, 51.35it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  91% 5380/5899 [01:44<00:10, 51.46it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  92% 5400/5899 [01:44<00:09, 51.56it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  92% 5420/5899 [01:44<00:09, 51.66it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  92% 5440/5899 [01:45<00:08, 51.77it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  93% 5460/5899 [01:45<00:08, 51.87it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  93% 5480/5899 [01:45<00:08, 51.97it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  93% 5500/5899 [01:45<00:07, 52.07it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  94% 5520/5899 [01:45<00:07, 52.17it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  94% 5540/5899 [01:45<00:06, 52.27it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  94% 5560/5899 [01:46<00:06, 52.37it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  95% 5580/5899 [01:46<00:06, 52.47it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  95% 5600/5899 [01:46<00:05, 52.57it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  95% 5620/5899 [01:46<00:05, 52.67it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  96% 5640/5899 [01:46<00:04, 52.78it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  96% 5660/5899 [01:47<00:04, 52.88it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  96% 5680/5899 [01:47<00:04, 52.97it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  97% 5700/5899 [01:47<00:03, 53.07it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  97% 5720/5899 [01:47<00:03, 53.16it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  97% 5740/5899 [01:47<00:02, 53.25it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  98% 5760/5899 [01:47<00:02, 53.35it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  98% 5780/5899 [01:48<00:02, 53.45it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  98% 5800/5899 [01:48<00:01, 53.55it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  99% 5820/5899 [01:48<00:01, 53.64it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  99% 5840/5899 [01:48<00:01, 53.74it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17:  99% 5860/5899 [01:48<00:00, 53.84it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17: 100% 5880/5899 [01:49<00:00, 53.94it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 17: 100% 5899/5899 [01:49<00:00, 54.03it/s, loss=4.41, v_num=6, train_loss_step=4.400, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.352\n",
            "Epoch 18:  80% 4700/5899 [01:34<00:24, 49.51it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  80% 4720/5899 [01:39<00:24, 47.65it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  80% 4740/5899 [01:39<00:24, 47.75it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  81% 4760/5899 [01:39<00:23, 47.86it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  81% 4780/5899 [01:39<00:23, 47.97it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  81% 4800/5899 [01:39<00:22, 48.09it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  82% 4820/5899 [01:39<00:22, 48.20it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  82% 4840/5899 [01:40<00:21, 48.32it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  82% 4860/5899 [01:40<00:21, 48.43it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  83% 4880/5899 [01:40<00:20, 48.55it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  83% 4900/5899 [01:40<00:20, 48.66it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  83% 4920/5899 [01:40<00:20, 48.78it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  84% 4940/5899 [01:41<00:19, 48.89it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  84% 4960/5899 [01:41<00:19, 49.01it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  84% 4980/5899 [01:41<00:18, 49.12it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  85% 5000/5899 [01:41<00:18, 49.24it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  85% 5020/5899 [01:41<00:17, 49.35it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  85% 5040/5899 [01:41<00:17, 49.46it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  86% 5060/5899 [01:42<00:16, 49.57it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  86% 5080/5899 [01:42<00:16, 49.69it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  86% 5100/5899 [01:42<00:16, 49.80it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  87% 5120/5899 [01:42<00:15, 49.91it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  87% 5140/5899 [01:42<00:15, 50.02it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  87% 5160/5899 [01:42<00:14, 50.14it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  88% 5180/5899 [01:43<00:14, 50.25it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  88% 5200/5899 [01:43<00:13, 50.36it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  88% 5220/5899 [01:43<00:13, 50.47it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  89% 5240/5899 [01:43<00:13, 50.58it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  89% 5260/5899 [01:43<00:12, 50.69it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  90% 5280/5899 [01:43<00:12, 50.79it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  90% 5300/5899 [01:44<00:11, 50.90it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  90% 5320/5899 [01:44<00:11, 51.01it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  91% 5340/5899 [01:44<00:10, 51.12it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  91% 5360/5899 [01:44<00:10, 51.23it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  91% 5380/5899 [01:44<00:10, 51.34it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  92% 5400/5899 [01:44<00:09, 51.45it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  92% 5420/5899 [01:45<00:09, 51.56it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  92% 5440/5899 [01:45<00:08, 51.67it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  93% 5460/5899 [01:45<00:08, 51.78it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  93% 5480/5899 [01:45<00:08, 51.89it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  93% 5500/5899 [01:45<00:07, 52.00it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  94% 5520/5899 [01:45<00:07, 52.10it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  94% 5540/5899 [01:46<00:06, 52.21it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  94% 5560/5899 [01:46<00:06, 52.31it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  95% 5580/5899 [01:46<00:06, 52.42it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  95% 5600/5899 [01:46<00:05, 52.52it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  95% 5620/5899 [01:46<00:05, 52.62it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  96% 5640/5899 [01:46<00:04, 52.72it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  96% 5660/5899 [01:47<00:04, 52.82it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  96% 5680/5899 [01:47<00:04, 52.92it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  97% 5700/5899 [01:47<00:03, 53.02it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  97% 5720/5899 [01:47<00:03, 53.12it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  97% 5740/5899 [01:47<00:02, 53.22it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  98% 5760/5899 [01:48<00:02, 53.32it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  98% 5780/5899 [01:48<00:02, 53.42it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  98% 5800/5899 [01:48<00:01, 53.51it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  99% 5820/5899 [01:48<00:01, 53.62it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  99% 5840/5899 [01:48<00:01, 53.72it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18:  99% 5860/5899 [01:48<00:00, 53.82it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18: 100% 5880/5899 [01:49<00:00, 53.92it/s, loss=4.4, v_num=6, train_loss_step=4.420, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "Epoch 18: 100% 5899/5899 [01:49<00:00, 54.02it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.410]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.350\n",
            "Epoch 19:  80% 4700/5899 [01:35<00:24, 49.36it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  80% 4720/5899 [01:39<00:24, 47.65it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  80% 4740/5899 [01:39<00:24, 47.78it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  81% 4760/5899 [01:39<00:23, 47.89it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  81% 4780/5899 [01:39<00:23, 48.00it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  81% 4800/5899 [01:39<00:22, 48.12it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  82% 4820/5899 [01:39<00:22, 48.23it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  82% 4840/5899 [01:40<00:21, 48.35it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  82% 4860/5899 [01:40<00:21, 48.46it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  83% 4880/5899 [01:40<00:20, 48.58it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  83% 4900/5899 [01:40<00:20, 48.68it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  83% 4920/5899 [01:40<00:20, 48.79it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  84% 4940/5899 [01:41<00:19, 48.90it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  84% 4960/5899 [01:41<00:19, 49.01it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  84% 4980/5899 [01:41<00:18, 49.11it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  85% 5000/5899 [01:41<00:18, 49.21it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  85% 5020/5899 [01:41<00:17, 49.31it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  85% 5040/5899 [01:41<00:17, 49.41it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  86% 5060/5899 [01:42<00:16, 49.52it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  86% 5080/5899 [01:42<00:16, 49.63it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  86% 5100/5899 [01:42<00:16, 49.74it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  87% 5120/5899 [01:42<00:15, 49.85it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  87% 5140/5899 [01:42<00:15, 49.96it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  87% 5160/5899 [01:43<00:14, 50.07it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  88% 5180/5899 [01:43<00:14, 50.18it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  88% 5200/5899 [01:43<00:13, 50.29it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  88% 5220/5899 [01:43<00:13, 50.40it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  89% 5240/5899 [01:43<00:13, 50.50it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  89% 5260/5899 [01:43<00:12, 50.61it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  90% 5280/5899 [01:44<00:12, 50.72it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  90% 5300/5899 [01:44<00:11, 50.82it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  90% 5320/5899 [01:44<00:11, 50.93it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  91% 5340/5899 [01:44<00:10, 51.04it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  91% 5360/5899 [01:44<00:10, 51.14it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  91% 5380/5899 [01:44<00:10, 51.25it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  92% 5400/5899 [01:45<00:09, 51.36it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  92% 5420/5899 [01:45<00:09, 51.46it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  92% 5440/5899 [01:45<00:08, 51.57it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  93% 5460/5899 [01:45<00:08, 51.67it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  93% 5480/5899 [01:45<00:08, 51.78it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  93% 5500/5899 [01:46<00:07, 51.88it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  94% 5520/5899 [01:46<00:07, 51.99it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  94% 5540/5899 [01:46<00:06, 52.09it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  94% 5560/5899 [01:46<00:06, 52.19it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  95% 5580/5899 [01:46<00:06, 52.29it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  95% 5600/5899 [01:46<00:05, 52.40it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  95% 5620/5899 [01:47<00:05, 52.50it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  96% 5640/5899 [01:47<00:04, 52.60it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  96% 5660/5899 [01:47<00:04, 52.71it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  96% 5680/5899 [01:47<00:04, 52.81it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  97% 5700/5899 [01:47<00:03, 52.91it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  97% 5720/5899 [01:47<00:03, 53.01it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  97% 5740/5899 [01:48<00:02, 53.12it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  98% 5760/5899 [01:48<00:02, 53.22it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  98% 5780/5899 [01:48<00:02, 53.31it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  98% 5800/5899 [01:48<00:01, 53.41it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  99% 5820/5899 [01:48<00:01, 53.50it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  99% 5840/5899 [01:48<00:01, 53.60it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19:  99% 5860/5899 [01:49<00:00, 53.70it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19: 100% 5880/5899 [01:49<00:00, 53.80it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 19: 100% 5899/5899 [01:49<00:00, 53.90it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.349\n",
            "Epoch 20:  80% 4700/5899 [01:35<00:24, 49.00it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  80% 4720/5899 [01:39<00:24, 47.31it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  80% 4740/5899 [01:39<00:24, 47.42it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  81% 4760/5899 [01:40<00:23, 47.53it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  81% 4780/5899 [01:40<00:23, 47.64it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  81% 4800/5899 [01:40<00:23, 47.75it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  82% 4820/5899 [01:40<00:22, 47.85it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  82% 4840/5899 [01:40<00:22, 47.95it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  82% 4860/5899 [01:41<00:21, 48.07it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  83% 4880/5899 [01:41<00:21, 48.18it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  83% 4900/5899 [01:41<00:20, 48.29it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  83% 4920/5899 [01:41<00:20, 48.40it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  84% 4940/5899 [01:41<00:19, 48.51it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  84% 4960/5899 [01:42<00:19, 48.62it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  84% 4980/5899 [01:42<00:18, 48.73it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  85% 5000/5899 [01:42<00:18, 48.83it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  85% 5020/5899 [01:42<00:17, 48.93it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  85% 5040/5899 [01:42<00:17, 49.04it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  86% 5060/5899 [01:42<00:17, 49.15it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  86% 5080/5899 [01:43<00:16, 49.26it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  86% 5100/5899 [01:43<00:16, 49.37it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  87% 5120/5899 [01:43<00:15, 49.48it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  87% 5140/5899 [01:43<00:15, 49.59it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  87% 5160/5899 [01:43<00:14, 49.71it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  88% 5180/5899 [01:43<00:14, 49.81it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  88% 5200/5899 [01:44<00:14, 49.93it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  88% 5220/5899 [01:44<00:13, 50.04it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  89% 5240/5899 [01:44<00:13, 50.15it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  89% 5260/5899 [01:44<00:12, 50.26it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  90% 5280/5899 [01:44<00:12, 50.37it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  90% 5300/5899 [01:44<00:11, 50.48it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  90% 5320/5899 [01:45<00:11, 50.59it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  91% 5340/5899 [01:45<00:11, 50.70it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  91% 5360/5899 [01:45<00:10, 50.80it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  91% 5380/5899 [01:45<00:10, 50.91it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  92% 5400/5899 [01:45<00:09, 51.02it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  92% 5420/5899 [01:46<00:09, 51.13it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  92% 5440/5899 [01:46<00:08, 51.24it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  93% 5460/5899 [01:46<00:08, 51.34it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  93% 5480/5899 [01:46<00:08, 51.45it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  93% 5500/5899 [01:46<00:07, 51.56it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  94% 5520/5899 [01:46<00:07, 51.66it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  94% 5540/5899 [01:47<00:06, 51.77it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  94% 5560/5899 [01:47<00:06, 51.88it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  95% 5580/5899 [01:47<00:06, 51.98it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  95% 5600/5899 [01:47<00:05, 52.09it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  95% 5620/5899 [01:47<00:05, 52.19it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  96% 5640/5899 [01:47<00:04, 52.30it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  96% 5660/5899 [01:48<00:04, 52.40it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  96% 5680/5899 [01:48<00:04, 52.50it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  97% 5700/5899 [01:48<00:03, 52.60it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  97% 5720/5899 [01:48<00:03, 52.70it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  97% 5740/5899 [01:48<00:03, 52.80it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  98% 5760/5899 [01:48<00:02, 52.91it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  98% 5780/5899 [01:49<00:02, 53.01it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  98% 5800/5899 [01:49<00:01, 53.11it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  99% 5820/5899 [01:49<00:01, 53.21it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  99% 5840/5899 [01:49<00:01, 53.31it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20:  99% 5860/5899 [01:49<00:00, 53.42it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20: 100% 5880/5899 [01:49<00:00, 53.52it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 20: 100% 5899/5899 [01:50<00:00, 53.61it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 21:  80% 4700/5899 [01:35<00:24, 49.23it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  80% 4720/5899 [01:39<00:24, 47.50it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  80% 4740/5899 [01:39<00:24, 47.60it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  81% 4760/5899 [01:39<00:23, 47.71it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  81% 4780/5899 [01:39<00:23, 47.83it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  81% 4800/5899 [01:40<00:22, 47.94it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  82% 4820/5899 [01:40<00:22, 48.06it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  82% 4840/5899 [01:40<00:21, 48.17it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  82% 4860/5899 [01:40<00:21, 48.29it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  83% 4880/5899 [01:40<00:21, 48.40it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  83% 4900/5899 [01:41<00:20, 48.51it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  83% 4920/5899 [01:41<00:20, 48.62it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  84% 4940/5899 [01:41<00:19, 48.73it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  84% 4960/5899 [01:41<00:19, 48.84it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  84% 4980/5899 [01:41<00:18, 48.95it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  85% 5000/5899 [01:41<00:18, 49.06it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  85% 5020/5899 [01:42<00:17, 49.17it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  85% 5040/5899 [01:42<00:17, 49.28it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  86% 5060/5899 [01:42<00:16, 49.38it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  86% 5080/5899 [01:42<00:16, 49.49it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  86% 5100/5899 [01:42<00:16, 49.60it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  87% 5120/5899 [01:42<00:15, 49.71it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  87% 5140/5899 [01:43<00:15, 49.82it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  87% 5160/5899 [01:43<00:14, 49.92it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  88% 5180/5899 [01:43<00:14, 50.03it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  88% 5200/5899 [01:43<00:13, 50.14it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  88% 5220/5899 [01:43<00:13, 50.25it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  89% 5240/5899 [01:44<00:13, 50.36it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  89% 5260/5899 [01:44<00:12, 50.47it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  90% 5280/5899 [01:44<00:12, 50.58it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  90% 5300/5899 [01:44<00:11, 50.69it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  90% 5320/5899 [01:44<00:11, 50.80it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  91% 5340/5899 [01:44<00:10, 50.91it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  91% 5360/5899 [01:45<00:10, 51.01it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  91% 5380/5899 [01:45<00:10, 51.12it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  92% 5400/5899 [01:45<00:09, 51.23it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  92% 5420/5899 [01:45<00:09, 51.33it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  92% 5440/5899 [01:45<00:08, 51.44it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  93% 5460/5899 [01:45<00:08, 51.54it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  93% 5480/5899 [01:46<00:08, 51.65it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  93% 5500/5899 [01:46<00:07, 51.75it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  94% 5520/5899 [01:46<00:07, 51.86it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  94% 5540/5899 [01:46<00:06, 51.96it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  94% 5560/5899 [01:46<00:06, 52.07it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  95% 5580/5899 [01:46<00:06, 52.17it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  95% 5600/5899 [01:47<00:05, 52.28it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  95% 5620/5899 [01:47<00:05, 52.38it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  96% 5640/5899 [01:47<00:04, 52.49it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  96% 5660/5899 [01:47<00:04, 52.59it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  96% 5680/5899 [01:47<00:04, 52.70it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  97% 5700/5899 [01:47<00:03, 52.80it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  97% 5720/5899 [01:48<00:03, 52.91it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  97% 5740/5899 [01:48<00:02, 53.01it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  98% 5760/5899 [01:48<00:02, 53.11it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  98% 5780/5899 [01:48<00:02, 53.20it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  98% 5800/5899 [01:48<00:01, 53.31it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  99% 5820/5899 [01:48<00:01, 53.41it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  99% 5840/5899 [01:49<00:01, 53.51it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21:  99% 5860/5899 [01:49<00:00, 53.61it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21: 100% 5880/5899 [01:49<00:00, 53.72it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 21: 100% 5899/5899 [01:49<00:00, 53.81it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.002 >= min_delta = 0.001. New best score: 4.347\n",
            "Epoch 22:  80% 4700/5899 [01:35<00:24, 49.17it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  80% 4720/5899 [01:39<00:24, 47.42it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  80% 4740/5899 [01:39<00:24, 47.53it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  81% 4760/5899 [01:39<00:23, 47.64it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  81% 4780/5899 [01:40<00:23, 47.75it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  81% 4800/5899 [01:40<00:22, 47.86it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  82% 4820/5899 [01:40<00:22, 47.98it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  82% 4840/5899 [01:40<00:22, 48.08it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  82% 4860/5899 [01:40<00:21, 48.19it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  83% 4880/5899 [01:41<00:21, 48.30it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  83% 4900/5899 [01:41<00:20, 48.41it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  83% 4920/5899 [01:41<00:20, 48.52it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  84% 4940/5899 [01:41<00:19, 48.63it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  84% 4960/5899 [01:41<00:19, 48.73it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  84% 4980/5899 [01:41<00:18, 48.85it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  85% 5000/5899 [01:42<00:18, 48.95it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  85% 5020/5899 [01:42<00:17, 49.06it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  85% 5040/5899 [01:42<00:17, 49.16it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  86% 5060/5899 [01:42<00:17, 49.26it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  86% 5080/5899 [01:42<00:16, 49.37it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  86% 5100/5899 [01:43<00:16, 49.48it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  87% 5120/5899 [01:43<00:15, 49.58it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  87% 5140/5899 [01:43<00:15, 49.68it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  87% 5160/5899 [01:43<00:14, 49.79it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  88% 5180/5899 [01:43<00:14, 49.89it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  88% 5200/5899 [01:44<00:13, 50.00it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  88% 5220/5899 [01:44<00:13, 50.11it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  89% 5240/5899 [01:44<00:13, 50.22it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  89% 5260/5899 [01:44<00:12, 50.33it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  90% 5280/5899 [01:44<00:12, 50.44it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  90% 5300/5899 [01:44<00:11, 50.55it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  90% 5320/5899 [01:45<00:11, 50.66it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  91% 5340/5899 [01:45<00:11, 50.77it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  91% 5360/5899 [01:45<00:10, 50.88it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  91% 5380/5899 [01:45<00:10, 50.99it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  92% 5400/5899 [01:45<00:09, 51.10it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  92% 5420/5899 [01:45<00:09, 51.21it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  92% 5440/5899 [01:46<00:08, 51.31it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  93% 5460/5899 [01:46<00:08, 51.42it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  93% 5480/5899 [01:46<00:08, 51.52it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  93% 5500/5899 [01:46<00:07, 51.63it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  94% 5520/5899 [01:46<00:07, 51.74it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  94% 5540/5899 [01:46<00:06, 51.84it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  94% 5560/5899 [01:47<00:06, 51.94it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  95% 5580/5899 [01:47<00:06, 52.05it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  95% 5600/5899 [01:47<00:05, 52.15it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  95% 5620/5899 [01:47<00:05, 52.26it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  96% 5640/5899 [01:47<00:04, 52.36it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  96% 5660/5899 [01:47<00:04, 52.46it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  96% 5680/5899 [01:48<00:04, 52.57it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  97% 5700/5899 [01:48<00:03, 52.67it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  97% 5720/5899 [01:48<00:03, 52.78it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  97% 5740/5899 [01:48<00:03, 52.88it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  98% 5760/5899 [01:48<00:02, 52.98it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  98% 5780/5899 [01:48<00:02, 53.09it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  98% 5800/5899 [01:49<00:01, 53.19it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  99% 5820/5899 [01:49<00:01, 53.29it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  99% 5840/5899 [01:49<00:01, 53.39it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22:  99% 5860/5899 [01:49<00:00, 53.49it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22: 100% 5880/5899 [01:49<00:00, 53.59it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 22: 100% 5899/5899 [01:49<00:00, 53.68it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.346\n",
            "Epoch 23:  80% 4700/5899 [01:35<00:24, 49.39it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  80% 4720/5899 [01:39<00:24, 47.65it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  80% 4740/5899 [01:39<00:24, 47.75it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  81% 4760/5899 [01:39<00:23, 47.86it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  81% 4780/5899 [01:39<00:23, 47.97it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  81% 4800/5899 [01:39<00:22, 48.08it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  82% 4820/5899 [01:40<00:22, 48.19it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  82% 4840/5899 [01:40<00:21, 48.30it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  82% 4860/5899 [01:40<00:21, 48.41it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  83% 4880/5899 [01:40<00:21, 48.52it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  83% 4900/5899 [01:40<00:20, 48.63it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  83% 4920/5899 [01:40<00:20, 48.74it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  84% 4940/5899 [01:41<00:19, 48.85it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  84% 4960/5899 [01:41<00:19, 48.95it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  84% 4980/5899 [01:41<00:18, 49.06it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  85% 5000/5899 [01:41<00:18, 49.17it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  85% 5020/5899 [01:41<00:17, 49.28it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  85% 5040/5899 [01:42<00:17, 49.39it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  86% 5060/5899 [01:42<00:16, 49.49it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  86% 5080/5899 [01:42<00:16, 49.60it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  86% 5100/5899 [01:42<00:16, 49.70it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  87% 5120/5899 [01:42<00:15, 49.80it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  87% 5140/5899 [01:42<00:15, 49.91it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  87% 5160/5899 [01:43<00:14, 50.02it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  88% 5180/5899 [01:43<00:14, 50.13it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  88% 5200/5899 [01:43<00:13, 50.24it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  88% 5220/5899 [01:43<00:13, 50.34it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  89% 5240/5899 [01:43<00:13, 50.44it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  89% 5260/5899 [01:44<00:12, 50.55it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  90% 5280/5899 [01:44<00:12, 50.65it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  90% 5300/5899 [01:44<00:11, 50.76it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  90% 5320/5899 [01:44<00:11, 50.86it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  91% 5340/5899 [01:44<00:10, 50.97it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  91% 5360/5899 [01:44<00:10, 51.08it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  91% 5380/5899 [01:45<00:10, 51.18it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  92% 5400/5899 [01:45<00:09, 51.29it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  92% 5420/5899 [01:45<00:09, 51.39it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  92% 5440/5899 [01:45<00:08, 51.50it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  93% 5460/5899 [01:45<00:08, 51.60it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  93% 5480/5899 [01:45<00:08, 51.71it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  93% 5500/5899 [01:46<00:07, 51.82it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  94% 5520/5899 [01:46<00:07, 51.92it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  94% 5540/5899 [01:46<00:06, 52.03it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  94% 5560/5899 [01:46<00:06, 52.13it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  95% 5580/5899 [01:46<00:06, 52.23it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  95% 5600/5899 [01:47<00:05, 52.33it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  95% 5620/5899 [01:47<00:05, 52.44it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  96% 5640/5899 [01:47<00:04, 52.54it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  96% 5660/5899 [01:47<00:04, 52.64it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  96% 5680/5899 [01:47<00:04, 52.74it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  97% 5700/5899 [01:47<00:03, 52.85it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  97% 5720/5899 [01:48<00:03, 52.95it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  97% 5740/5899 [01:48<00:02, 53.05it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  98% 5760/5899 [01:48<00:02, 53.15it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  98% 5780/5899 [01:48<00:02, 53.25it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  98% 5800/5899 [01:48<00:01, 53.35it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  99% 5820/5899 [01:48<00:01, 53.45it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  99% 5840/5899 [01:49<00:01, 53.56it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23:  99% 5860/5899 [01:49<00:00, 53.66it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23: 100% 5880/5899 [01:49<00:00, 53.76it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.350, avg_val_loss=4.350, train_loss_epoch=4.400]\n",
            "Epoch 23: 100% 5899/5899 [01:49<00:00, 53.85it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 24:  80% 4700/5899 [01:34<00:24, 49.57it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  80% 4720/5899 [01:38<00:24, 47.79it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  80% 4740/5899 [01:38<00:24, 47.90it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  81% 4760/5899 [01:39<00:23, 48.01it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  81% 4780/5899 [01:39<00:23, 48.12it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  81% 4800/5899 [01:39<00:22, 48.23it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  82% 4820/5899 [01:39<00:22, 48.34it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  82% 4840/5899 [01:39<00:21, 48.46it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  82% 4860/5899 [01:40<00:21, 48.57it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  83% 4880/5899 [01:40<00:20, 48.68it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  83% 4900/5899 [01:40<00:20, 48.80it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  83% 4920/5899 [01:40<00:20, 48.91it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  84% 4940/5899 [01:40<00:19, 49.02it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  84% 4960/5899 [01:40<00:19, 49.13it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  84% 4980/5899 [01:41<00:18, 49.24it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  85% 5000/5899 [01:41<00:18, 49.35it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  85% 5020/5899 [01:41<00:17, 49.46it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  85% 5040/5899 [01:41<00:17, 49.58it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  86% 5060/5899 [01:41<00:16, 49.68it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  86% 5080/5899 [01:42<00:16, 49.79it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  86% 5100/5899 [01:42<00:16, 49.90it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  87% 5120/5899 [01:42<00:15, 50.01it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  87% 5140/5899 [01:42<00:15, 50.12it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  87% 5160/5899 [01:42<00:14, 50.23it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  88% 5180/5899 [01:42<00:14, 50.34it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  88% 5200/5899 [01:43<00:13, 50.44it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  88% 5220/5899 [01:43<00:13, 50.55it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  89% 5240/5899 [01:43<00:13, 50.66it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  89% 5260/5899 [01:43<00:12, 50.76it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  90% 5280/5899 [01:43<00:12, 50.86it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  90% 5300/5899 [01:43<00:11, 50.97it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  90% 5320/5899 [01:44<00:11, 51.07it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  91% 5340/5899 [01:44<00:10, 51.18it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  91% 5360/5899 [01:44<00:10, 51.29it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  91% 5380/5899 [01:44<00:10, 51.40it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  92% 5400/5899 [01:44<00:09, 51.51it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  92% 5420/5899 [01:45<00:09, 51.61it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  92% 5440/5899 [01:45<00:08, 51.72it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  93% 5460/5899 [01:45<00:08, 51.83it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  93% 5480/5899 [01:45<00:08, 51.94it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  93% 5500/5899 [01:45<00:07, 52.03it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  94% 5520/5899 [01:45<00:07, 52.13it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  94% 5540/5899 [01:46<00:06, 52.22it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  94% 5560/5899 [01:46<00:06, 52.32it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  95% 5580/5899 [01:46<00:06, 52.43it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  95% 5600/5899 [01:46<00:05, 52.53it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  95% 5620/5899 [01:46<00:05, 52.64it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  96% 5640/5899 [01:46<00:04, 52.74it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  96% 5660/5899 [01:47<00:04, 52.84it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  96% 5680/5899 [01:47<00:04, 52.95it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  97% 5700/5899 [01:47<00:03, 53.05it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  97% 5720/5899 [01:47<00:03, 53.15it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  97% 5740/5899 [01:47<00:02, 53.24it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  98% 5760/5899 [01:47<00:02, 53.34it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  98% 5780/5899 [01:48<00:02, 53.44it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  98% 5800/5899 [01:48<00:01, 53.54it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  99% 5820/5899 [01:48<00:01, 53.64it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  99% 5840/5899 [01:48<00:01, 53.74it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24:  99% 5860/5899 [01:48<00:00, 53.85it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24: 100% 5880/5899 [01:48<00:00, 53.95it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 24: 100% 5899/5899 [01:49<00:00, 54.04it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.345\n",
            "Epoch 25:  80% 4700/5899 [01:35<00:24, 49.05it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  80% 4720/5899 [01:39<00:24, 47.25it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  80% 4740/5899 [01:40<00:24, 47.36it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  81% 4760/5899 [01:40<00:23, 47.48it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  81% 4780/5899 [01:40<00:23, 47.59it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  81% 4800/5899 [01:40<00:23, 47.71it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  82% 4820/5899 [01:40<00:22, 47.82it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  82% 4840/5899 [01:40<00:22, 47.92it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  82% 4860/5899 [01:41<00:21, 48.04it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  83% 4880/5899 [01:41<00:21, 48.15it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  83% 4900/5899 [01:41<00:20, 48.26it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  83% 4920/5899 [01:41<00:20, 48.37it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  84% 4940/5899 [01:41<00:19, 48.47it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  84% 4960/5899 [01:42<00:19, 48.57it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  84% 4980/5899 [01:42<00:18, 48.69it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  85% 5000/5899 [01:42<00:18, 48.79it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  85% 5020/5899 [01:42<00:17, 48.90it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  85% 5040/5899 [01:42<00:17, 49.01it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  86% 5060/5899 [01:43<00:17, 49.11it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  86% 5080/5899 [01:43<00:16, 49.22it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  86% 5100/5899 [01:43<00:16, 49.33it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  87% 5120/5899 [01:43<00:15, 49.45it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  87% 5140/5899 [01:43<00:15, 49.55it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  87% 5160/5899 [01:43<00:14, 49.66it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  88% 5180/5899 [01:44<00:14, 49.77it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  88% 5200/5899 [01:44<00:14, 49.89it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  88% 5220/5899 [01:44<00:13, 50.00it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  89% 5240/5899 [01:44<00:13, 50.11it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  89% 5260/5899 [01:44<00:12, 50.22it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  90% 5280/5899 [01:44<00:12, 50.32it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  90% 5300/5899 [01:45<00:11, 50.43it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  90% 5320/5899 [01:45<00:11, 50.53it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  91% 5340/5899 [01:45<00:11, 50.62it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  91% 5360/5899 [01:45<00:10, 50.73it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  91% 5380/5899 [01:45<00:10, 50.83it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  92% 5400/5899 [01:46<00:09, 50.93it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  92% 5420/5899 [01:46<00:09, 51.04it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  92% 5440/5899 [01:46<00:08, 51.15it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  93% 5460/5899 [01:46<00:08, 51.25it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  93% 5480/5899 [01:46<00:08, 51.36it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  93% 5500/5899 [01:46<00:07, 51.47it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  94% 5520/5899 [01:47<00:07, 51.57it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  94% 5540/5899 [01:47<00:06, 51.67it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  94% 5560/5899 [01:47<00:06, 51.78it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  95% 5580/5899 [01:47<00:06, 51.88it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  95% 5600/5899 [01:47<00:05, 51.99it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  95% 5620/5899 [01:47<00:05, 52.09it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  96% 5640/5899 [01:48<00:04, 52.19it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  96% 5660/5899 [01:48<00:04, 52.29it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  96% 5680/5899 [01:48<00:04, 52.39it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  97% 5700/5899 [01:48<00:03, 52.50it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  97% 5720/5899 [01:48<00:03, 52.60it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  97% 5740/5899 [01:48<00:03, 52.71it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  98% 5760/5899 [01:49<00:02, 52.81it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  98% 5780/5899 [01:49<00:02, 52.92it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  98% 5800/5899 [01:49<00:01, 53.02it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  99% 5820/5899 [01:49<00:01, 53.12it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  99% 5840/5899 [01:49<00:01, 53.22it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25:  99% 5860/5899 [01:49<00:00, 53.32it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25: 100% 5880/5899 [01:50<00:00, 53.42it/s, loss=4.4, v_num=6, train_loss_step=4.410, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 25: 100% 5899/5899 [01:50<00:00, 53.52it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 26:  80% 4700/5899 [01:35<00:24, 49.35it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  80% 4720/5899 [01:39<00:24, 47.55it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  80% 4740/5899 [01:39<00:24, 47.66it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  81% 4760/5899 [01:39<00:23, 47.77it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  81% 4780/5899 [01:39<00:23, 47.88it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  81% 4800/5899 [01:40<00:22, 47.99it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  82% 4820/5899 [01:40<00:22, 48.11it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  82% 4840/5899 [01:40<00:21, 48.22it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  82% 4860/5899 [01:40<00:21, 48.34it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  83% 4880/5899 [01:40<00:21, 48.45it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  83% 4900/5899 [01:40<00:20, 48.56it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  83% 4920/5899 [01:41<00:20, 48.68it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  84% 4940/5899 [01:41<00:19, 48.80it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  84% 4960/5899 [01:41<00:19, 48.91it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  84% 4980/5899 [01:41<00:18, 49.03it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  85% 5000/5899 [01:41<00:18, 49.14it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  85% 5020/5899 [01:41<00:17, 49.26it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  85% 5040/5899 [01:42<00:17, 49.37it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  86% 5060/5899 [01:42<00:16, 49.49it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  86% 5080/5899 [01:42<00:16, 49.60it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  86% 5100/5899 [01:42<00:16, 49.72it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  87% 5120/5899 [01:42<00:15, 49.83it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  87% 5140/5899 [01:42<00:15, 49.94it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  87% 5160/5899 [01:43<00:14, 50.06it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  88% 5180/5899 [01:43<00:14, 50.17it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  88% 5200/5899 [01:43<00:13, 50.27it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  88% 5220/5899 [01:43<00:13, 50.38it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  89% 5240/5899 [01:43<00:13, 50.49it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  89% 5260/5899 [01:43<00:12, 50.60it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  90% 5280/5899 [01:44<00:12, 50.71it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  90% 5300/5899 [01:44<00:11, 50.82it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  90% 5320/5899 [01:44<00:11, 50.92it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  91% 5340/5899 [01:44<00:10, 51.03it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  91% 5360/5899 [01:44<00:10, 51.14it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  91% 5380/5899 [01:44<00:10, 51.24it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  92% 5400/5899 [01:45<00:09, 51.34it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  92% 5420/5899 [01:45<00:09, 51.45it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  92% 5440/5899 [01:45<00:08, 51.55it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  93% 5460/5899 [01:45<00:08, 51.65it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  93% 5480/5899 [01:45<00:08, 51.76it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  93% 5500/5899 [01:46<00:07, 51.87it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  94% 5520/5899 [01:46<00:07, 51.98it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  94% 5540/5899 [01:46<00:06, 52.09it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  94% 5560/5899 [01:46<00:06, 52.19it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  95% 5580/5899 [01:46<00:06, 52.30it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  95% 5600/5899 [01:46<00:05, 52.40it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  95% 5620/5899 [01:47<00:05, 52.50it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  96% 5640/5899 [01:47<00:04, 52.60it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  96% 5660/5899 [01:47<00:04, 52.71it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  96% 5680/5899 [01:47<00:04, 52.81it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  97% 5700/5899 [01:47<00:03, 52.92it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  97% 5720/5899 [01:47<00:03, 53.01it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  97% 5740/5899 [01:48<00:02, 53.11it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  98% 5760/5899 [01:48<00:02, 53.21it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  98% 5780/5899 [01:48<00:02, 53.31it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  98% 5800/5899 [01:48<00:01, 53.41it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  99% 5820/5899 [01:48<00:01, 53.51it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  99% 5840/5899 [01:48<00:01, 53.61it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26:  99% 5860/5899 [01:49<00:00, 53.71it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26: 100% 5880/5899 [01:49<00:00, 53.81it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 26: 100% 5899/5899 [01:49<00:00, 53.90it/s, loss=4.4, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.343\n",
            "Epoch 27:  80% 4700/5899 [01:35<00:24, 49.46it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  80% 4720/5899 [01:39<00:24, 47.67it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  80% 4740/5899 [01:39<00:24, 47.78it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  81% 4760/5899 [01:39<00:23, 47.90it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  81% 4780/5899 [01:39<00:23, 48.01it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  81% 4800/5899 [01:39<00:22, 48.13it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  82% 4820/5899 [01:39<00:22, 48.24it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  82% 4840/5899 [01:40<00:21, 48.35it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  82% 4860/5899 [01:40<00:21, 48.46it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  83% 4880/5899 [01:40<00:20, 48.58it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  83% 4900/5899 [01:40<00:20, 48.69it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  83% 4920/5899 [01:40<00:20, 48.80it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  84% 4940/5899 [01:40<00:19, 48.92it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  84% 4960/5899 [01:41<00:19, 49.03it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  84% 4980/5899 [01:41<00:18, 49.14it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  85% 5000/5899 [01:41<00:18, 49.25it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  85% 5020/5899 [01:41<00:17, 49.36it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  85% 5040/5899 [01:41<00:17, 49.47it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  86% 5060/5899 [01:42<00:16, 49.58it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  86% 5080/5899 [01:42<00:16, 49.69it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  86% 5100/5899 [01:42<00:16, 49.80it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  87% 5120/5899 [01:42<00:15, 49.91it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  87% 5140/5899 [01:42<00:15, 50.02it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  87% 5160/5899 [01:42<00:14, 50.13it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  88% 5180/5899 [01:43<00:14, 50.24it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  88% 5200/5899 [01:43<00:13, 50.34it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  88% 5220/5899 [01:43<00:13, 50.45it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  89% 5240/5899 [01:43<00:13, 50.55it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  89% 5260/5899 [01:43<00:12, 50.65it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  90% 5280/5899 [01:44<00:12, 50.76it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  90% 5300/5899 [01:44<00:11, 50.87it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  90% 5320/5899 [01:44<00:11, 50.97it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  91% 5340/5899 [01:44<00:10, 51.07it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  91% 5360/5899 [01:44<00:10, 51.18it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  91% 5380/5899 [01:44<00:10, 51.29it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  92% 5400/5899 [01:45<00:09, 51.40it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  92% 5420/5899 [01:45<00:09, 51.50it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  92% 5440/5899 [01:45<00:08, 51.60it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  93% 5460/5899 [01:45<00:08, 51.71it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  93% 5480/5899 [01:45<00:08, 51.81it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  93% 5500/5899 [01:45<00:07, 51.91it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  94% 5520/5899 [01:46<00:07, 52.02it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  94% 5540/5899 [01:46<00:06, 52.12it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  94% 5560/5899 [01:46<00:06, 52.22it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  95% 5580/5899 [01:46<00:06, 52.33it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  95% 5600/5899 [01:46<00:05, 52.43it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  95% 5620/5899 [01:46<00:05, 52.53it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  96% 5640/5899 [01:47<00:04, 52.64it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  96% 5660/5899 [01:47<00:04, 52.74it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  96% 5680/5899 [01:47<00:04, 52.85it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  97% 5700/5899 [01:47<00:03, 52.95it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  97% 5720/5899 [01:47<00:03, 53.05it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  97% 5740/5899 [01:47<00:02, 53.16it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  98% 5760/5899 [01:48<00:02, 53.26it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  98% 5780/5899 [01:48<00:02, 53.36it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  98% 5800/5899 [01:48<00:01, 53.46it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  99% 5820/5899 [01:48<00:01, 53.57it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  99% 5840/5899 [01:48<00:01, 53.67it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27:  99% 5860/5899 [01:48<00:00, 53.77it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27: 100% 5880/5899 [01:49<00:00, 53.87it/s, loss=4.4, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 27: 100% 5899/5899 [01:49<00:00, 53.97it/s, loss=4.4, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 28:  80% 4700/5899 [01:35<00:24, 49.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  80% 4720/5899 [01:39<00:24, 47.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  80% 4740/5899 [01:39<00:24, 47.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  81% 4760/5899 [01:39<00:23, 47.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  81% 4780/5899 [01:39<00:23, 47.85it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  81% 4800/5899 [01:40<00:22, 47.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  82% 4820/5899 [01:40<00:22, 48.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  82% 4840/5899 [01:40<00:21, 48.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  82% 4860/5899 [01:40<00:21, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  83% 4880/5899 [01:40<00:21, 48.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  83% 4900/5899 [01:40<00:20, 48.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  83% 4920/5899 [01:41<00:20, 48.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  84% 4940/5899 [01:41<00:19, 48.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  84% 4960/5899 [01:41<00:19, 48.88it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  84% 4980/5899 [01:41<00:18, 48.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  85% 5000/5899 [01:41<00:18, 49.09it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  85% 5020/5899 [01:42<00:17, 49.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  85% 5040/5899 [01:42<00:17, 49.32it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  86% 5060/5899 [01:42<00:16, 49.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  86% 5080/5899 [01:42<00:16, 49.55it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  86% 5100/5899 [01:42<00:16, 49.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  87% 5120/5899 [01:42<00:15, 49.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  87% 5140/5899 [01:43<00:15, 49.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  87% 5160/5899 [01:43<00:14, 50.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  88% 5180/5899 [01:43<00:14, 50.12it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  88% 5200/5899 [01:43<00:13, 50.23it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  88% 5220/5899 [01:43<00:13, 50.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  89% 5240/5899 [01:43<00:13, 50.45it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  89% 5260/5899 [01:44<00:12, 50.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  90% 5280/5899 [01:44<00:12, 50.67it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  90% 5300/5899 [01:44<00:11, 50.79it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  90% 5320/5899 [01:44<00:11, 50.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  91% 5340/5899 [01:44<00:10, 50.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  91% 5360/5899 [01:44<00:10, 51.10it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  91% 5380/5899 [01:45<00:10, 51.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  92% 5400/5899 [01:45<00:09, 51.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  92% 5420/5899 [01:45<00:09, 51.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  92% 5440/5899 [01:45<00:08, 51.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  93% 5460/5899 [01:45<00:08, 51.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  93% 5480/5899 [01:45<00:08, 51.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  93% 5500/5899 [01:46<00:07, 51.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  94% 5520/5899 [01:46<00:07, 51.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  94% 5540/5899 [01:46<00:06, 52.05it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  94% 5560/5899 [01:46<00:06, 52.16it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  95% 5580/5899 [01:46<00:06, 52.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  95% 5600/5899 [01:46<00:05, 52.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  95% 5620/5899 [01:47<00:05, 52.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  96% 5640/5899 [01:47<00:04, 52.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  96% 5660/5899 [01:47<00:04, 52.67it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  96% 5680/5899 [01:47<00:04, 52.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  97% 5700/5899 [01:47<00:03, 52.88it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  97% 5720/5899 [01:47<00:03, 52.98it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  97% 5740/5899 [01:48<00:02, 53.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  98% 5760/5899 [01:48<00:02, 53.18it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  98% 5780/5899 [01:48<00:02, 53.29it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  98% 5800/5899 [01:48<00:01, 53.38it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  99% 5820/5899 [01:48<00:01, 53.49it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  99% 5840/5899 [01:48<00:01, 53.59it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28:  99% 5860/5899 [01:49<00:00, 53.69it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28: 100% 5880/5899 [01:49<00:00, 53.79it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 28: 100% 5899/5899 [01:49<00:00, 53.89it/s, loss=4.4, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400] \n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 29:  80% 4700/5899 [01:35<00:24, 49.29it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  80% 4720/5899 [01:39<00:24, 47.50it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  80% 4740/5899 [01:39<00:24, 47.61it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  81% 4760/5899 [01:39<00:23, 47.72it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  81% 4780/5899 [01:39<00:23, 47.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  81% 4800/5899 [01:40<00:22, 47.95it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  82% 4820/5899 [01:40<00:22, 48.07it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  82% 4840/5899 [01:40<00:21, 48.18it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  82% 4860/5899 [01:40<00:21, 48.30it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  83% 4880/5899 [01:40<00:21, 48.41it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  83% 4900/5899 [01:40<00:20, 48.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  83% 4920/5899 [01:41<00:20, 48.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  84% 4940/5899 [01:41<00:19, 48.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  84% 4960/5899 [01:41<00:19, 48.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  84% 4980/5899 [01:41<00:18, 48.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  85% 5000/5899 [01:41<00:18, 49.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  85% 5020/5899 [01:42<00:17, 49.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  85% 5040/5899 [01:42<00:17, 49.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  86% 5060/5899 [01:42<00:16, 49.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  86% 5080/5899 [01:42<00:16, 49.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  86% 5100/5899 [01:42<00:16, 49.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  87% 5120/5899 [01:42<00:15, 49.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  87% 5140/5899 [01:43<00:15, 49.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  87% 5160/5899 [01:43<00:14, 49.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  88% 5180/5899 [01:43<00:14, 50.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  88% 5200/5899 [01:43<00:13, 50.18it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  88% 5220/5899 [01:43<00:13, 50.28it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  89% 5240/5899 [01:43<00:13, 50.39it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  89% 5260/5899 [01:44<00:12, 50.50it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  90% 5280/5899 [01:44<00:12, 50.61it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  90% 5300/5899 [01:44<00:11, 50.71it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  90% 5320/5899 [01:44<00:11, 50.82it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  91% 5340/5899 [01:44<00:10, 50.92it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  91% 5360/5899 [01:45<00:10, 51.03it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  91% 5380/5899 [01:45<00:10, 51.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  92% 5400/5899 [01:45<00:09, 51.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  92% 5420/5899 [01:45<00:09, 51.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  92% 5440/5899 [01:45<00:08, 51.44it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  93% 5460/5899 [01:45<00:08, 51.55it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  93% 5480/5899 [01:46<00:08, 51.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  93% 5500/5899 [01:46<00:07, 51.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  94% 5520/5899 [01:46<00:07, 51.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  94% 5540/5899 [01:46<00:06, 51.96it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  94% 5560/5899 [01:46<00:06, 52.06it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  95% 5580/5899 [01:46<00:06, 52.17it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  95% 5600/5899 [01:47<00:05, 52.27it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  95% 5620/5899 [01:47<00:05, 52.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  96% 5640/5899 [01:47<00:04, 52.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  96% 5660/5899 [01:47<00:04, 52.55it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  96% 5680/5899 [01:47<00:04, 52.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  97% 5700/5899 [01:48<00:03, 52.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  97% 5720/5899 [01:48<00:03, 52.83it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  97% 5740/5899 [01:48<00:03, 52.92it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  98% 5760/5899 [01:48<00:02, 53.01it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  98% 5780/5899 [01:48<00:02, 53.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  98% 5800/5899 [01:48<00:01, 53.21it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  99% 5820/5899 [01:49<00:01, 53.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  99% 5840/5899 [01:49<00:01, 53.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29:  99% 5860/5899 [01:49<00:00, 53.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29: 100% 5880/5899 [01:49<00:00, 53.62it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "Epoch 29: 100% 5899/5899 [01:49<00:00, 53.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.400]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.342\n",
            "Epoch 30:  80% 4700/5899 [01:35<00:24, 48.98it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  80% 4720/5899 [01:40<00:24, 47.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  80% 4740/5899 [01:40<00:24, 47.30it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  81% 4760/5899 [01:40<00:24, 47.41it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  81% 4780/5899 [01:40<00:23, 47.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  81% 4800/5899 [01:40<00:23, 47.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  82% 4820/5899 [01:40<00:22, 47.76it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  82% 4840/5899 [01:41<00:22, 47.88it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  82% 4860/5899 [01:41<00:21, 47.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  83% 4880/5899 [01:41<00:21, 48.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  83% 4900/5899 [01:41<00:20, 48.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  83% 4920/5899 [01:41<00:20, 48.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  84% 4940/5899 [01:41<00:19, 48.45it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  84% 4960/5899 [01:42<00:19, 48.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  84% 4980/5899 [01:42<00:18, 48.68it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  85% 5000/5899 [01:42<00:18, 48.79it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  85% 5020/5899 [01:42<00:17, 48.90it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  85% 5040/5899 [01:42<00:17, 49.01it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  86% 5060/5899 [01:42<00:17, 49.13it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  86% 5080/5899 [01:43<00:16, 49.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  86% 5100/5899 [01:43<00:16, 49.35it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  87% 5120/5899 [01:43<00:15, 49.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  87% 5140/5899 [01:43<00:15, 49.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  87% 5160/5899 [01:43<00:14, 49.68it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  88% 5180/5899 [01:44<00:14, 49.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  88% 5200/5899 [01:44<00:14, 49.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  88% 5220/5899 [01:44<00:13, 49.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  89% 5240/5899 [01:44<00:13, 50.10it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  89% 5260/5899 [01:44<00:12, 50.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  90% 5280/5899 [01:44<00:12, 50.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  90% 5300/5899 [01:45<00:11, 50.41it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  90% 5320/5899 [01:45<00:11, 50.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  91% 5340/5899 [01:45<00:11, 50.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  91% 5360/5899 [01:45<00:10, 50.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  91% 5380/5899 [01:45<00:10, 50.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  92% 5400/5899 [01:46<00:09, 50.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  92% 5420/5899 [01:46<00:09, 51.03it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  92% 5440/5899 [01:46<00:08, 51.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  93% 5460/5899 [01:46<00:08, 51.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  93% 5480/5899 [01:46<00:08, 51.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  93% 5500/5899 [01:46<00:07, 51.45it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  94% 5520/5899 [01:47<00:07, 51.55it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  94% 5540/5899 [01:47<00:06, 51.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  94% 5560/5899 [01:47<00:06, 51.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  95% 5580/5899 [01:47<00:06, 51.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  95% 5600/5899 [01:47<00:05, 51.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  95% 5620/5899 [01:47<00:05, 52.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  96% 5640/5899 [01:48<00:04, 52.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  96% 5660/5899 [01:48<00:04, 52.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  96% 5680/5899 [01:48<00:04, 52.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  97% 5700/5899 [01:48<00:03, 52.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  97% 5720/5899 [01:48<00:03, 52.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  97% 5740/5899 [01:49<00:03, 52.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  98% 5760/5899 [01:49<00:02, 52.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  98% 5780/5899 [01:49<00:02, 52.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  98% 5800/5899 [01:49<00:01, 52.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  99% 5820/5899 [01:49<00:01, 53.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  99% 5840/5899 [01:49<00:01, 53.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30:  99% 5860/5899 [01:50<00:00, 53.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30: 100% 5880/5899 [01:50<00:00, 53.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 30: 100% 5899/5899 [01:50<00:00, 53.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 31:  80% 4700/5899 [01:36<00:24, 48.80it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  80% 4720/5899 [01:40<00:25, 47.07it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  80% 4740/5899 [01:40<00:24, 47.18it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  81% 4760/5899 [01:40<00:24, 47.29it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  81% 4780/5899 [01:40<00:23, 47.40it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  81% 4800/5899 [01:41<00:23, 47.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  82% 4820/5899 [01:41<00:22, 47.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  82% 4840/5899 [01:41<00:22, 47.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  82% 4860/5899 [01:41<00:21, 47.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  83% 4880/5899 [01:41<00:21, 47.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  83% 4900/5899 [01:41<00:20, 48.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  83% 4920/5899 [01:42<00:20, 48.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  84% 4940/5899 [01:42<00:19, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  84% 4960/5899 [01:42<00:19, 48.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  84% 4980/5899 [01:42<00:18, 48.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  85% 5000/5899 [01:42<00:18, 48.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  85% 5020/5899 [01:42<00:18, 48.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  85% 5040/5899 [01:43<00:17, 48.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  86% 5060/5899 [01:43<00:17, 49.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  86% 5080/5899 [01:43<00:16, 49.12it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  86% 5100/5899 [01:43<00:16, 49.23it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  87% 5120/5899 [01:43<00:15, 49.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  87% 5140/5899 [01:43<00:15, 49.45it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  87% 5160/5899 [01:44<00:14, 49.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  88% 5180/5899 [01:44<00:14, 49.67it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  88% 5200/5899 [01:44<00:14, 49.79it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  88% 5220/5899 [01:44<00:13, 49.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  89% 5240/5899 [01:44<00:13, 50.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  89% 5260/5899 [01:44<00:12, 50.10it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  90% 5280/5899 [01:45<00:12, 50.21it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  90% 5300/5899 [01:45<00:11, 50.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  90% 5320/5899 [01:45<00:11, 50.41it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  91% 5340/5899 [01:45<00:11, 50.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  91% 5360/5899 [01:45<00:10, 50.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  91% 5380/5899 [01:46<00:10, 50.72it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  92% 5400/5899 [01:46<00:09, 50.83it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  92% 5420/5899 [01:46<00:09, 50.93it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  92% 5440/5899 [01:46<00:08, 51.03it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  93% 5460/5899 [01:46<00:08, 51.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  93% 5480/5899 [01:46<00:08, 51.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  93% 5500/5899 [01:47<00:07, 51.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  94% 5520/5899 [01:47<00:07, 51.44it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  94% 5540/5899 [01:47<00:06, 51.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  94% 5560/5899 [01:47<00:06, 51.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  95% 5580/5899 [01:47<00:06, 51.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  95% 5600/5899 [01:47<00:05, 51.85it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  95% 5620/5899 [01:48<00:05, 51.96it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  96% 5640/5899 [01:48<00:04, 52.05it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  96% 5660/5899 [01:48<00:04, 52.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  96% 5680/5899 [01:48<00:04, 52.25it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  97% 5700/5899 [01:48<00:03, 52.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  97% 5720/5899 [01:49<00:03, 52.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  97% 5740/5899 [01:49<00:03, 52.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  98% 5760/5899 [01:49<00:02, 52.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  98% 5780/5899 [01:49<00:02, 52.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  98% 5800/5899 [01:49<00:01, 52.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  99% 5820/5899 [01:49<00:01, 52.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  99% 5840/5899 [01:50<00:01, 53.07it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31:  99% 5860/5899 [01:50<00:00, 53.17it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31: 100% 5880/5899 [01:50<00:00, 53.27it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 31: 100% 5899/5899 [01:50<00:00, 53.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.341\n",
            "Epoch 32:  80% 4700/5899 [01:35<00:24, 49.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  80% 4720/5899 [01:39<00:24, 47.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  80% 4740/5899 [01:40<00:24, 47.32it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  81% 4760/5899 [01:40<00:24, 47.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  81% 4780/5899 [01:40<00:23, 47.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  81% 4800/5899 [01:40<00:23, 47.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  82% 4820/5899 [01:40<00:22, 47.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  82% 4840/5899 [01:41<00:22, 47.88it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  82% 4860/5899 [01:41<00:21, 48.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  83% 4880/5899 [01:41<00:21, 48.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  83% 4900/5899 [01:41<00:20, 48.23it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  83% 4920/5899 [01:41<00:20, 48.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  84% 4940/5899 [01:41<00:19, 48.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  84% 4960/5899 [01:42<00:19, 48.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  84% 4980/5899 [01:42<00:18, 48.69it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  85% 5000/5899 [01:42<00:18, 48.80it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  85% 5020/5899 [01:42<00:17, 48.91it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  85% 5040/5899 [01:42<00:17, 49.02it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  86% 5060/5899 [01:42<00:17, 49.13it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  86% 5080/5899 [01:43<00:16, 49.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  86% 5100/5899 [01:43<00:16, 49.35it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  87% 5120/5899 [01:43<00:15, 49.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  87% 5140/5899 [01:43<00:15, 49.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  87% 5160/5899 [01:43<00:14, 49.67it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  88% 5180/5899 [01:44<00:14, 49.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  88% 5200/5899 [01:44<00:14, 49.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  88% 5220/5899 [01:44<00:13, 50.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  89% 5240/5899 [01:44<00:13, 50.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  89% 5260/5899 [01:44<00:12, 50.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  90% 5280/5899 [01:44<00:12, 50.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  90% 5300/5899 [01:45<00:11, 50.44it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  90% 5320/5899 [01:45<00:11, 50.55it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  91% 5340/5899 [01:45<00:11, 50.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  91% 5360/5899 [01:45<00:10, 50.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  91% 5380/5899 [01:45<00:10, 50.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  92% 5400/5899 [01:45<00:09, 50.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  92% 5420/5899 [01:46<00:09, 51.07it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  92% 5440/5899 [01:46<00:08, 51.17it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  93% 5460/5899 [01:46<00:08, 51.28it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  93% 5480/5899 [01:46<00:08, 51.38it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  93% 5500/5899 [01:46<00:07, 51.48it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  94% 5520/5899 [01:47<00:07, 51.58it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  94% 5540/5899 [01:47<00:06, 51.68it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  94% 5560/5899 [01:47<00:06, 51.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  95% 5580/5899 [01:47<00:06, 51.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  95% 5600/5899 [01:47<00:05, 51.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  95% 5620/5899 [01:47<00:05, 52.09it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  96% 5640/5899 [01:48<00:04, 52.18it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  96% 5660/5899 [01:48<00:04, 52.28it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  96% 5680/5899 [01:48<00:04, 52.38it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  97% 5700/5899 [01:48<00:03, 52.48it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  97% 5720/5899 [01:48<00:03, 52.58it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  97% 5740/5899 [01:48<00:03, 52.68it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  98% 5760/5899 [01:49<00:02, 52.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  98% 5780/5899 [01:49<00:02, 52.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  98% 5800/5899 [01:49<00:01, 52.96it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  99% 5820/5899 [01:49<00:01, 53.06it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  99% 5840/5899 [01:49<00:01, 53.16it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32:  99% 5860/5899 [01:50<00:00, 53.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32: 100% 5880/5899 [01:50<00:00, 53.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 32: 100% 5899/5899 [01:50<00:00, 53.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 33:  80% 4700/5899 [01:36<00:24, 48.95it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  80% 4720/5899 [01:40<00:24, 47.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  80% 4740/5899 [01:40<00:24, 47.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  81% 4760/5899 [01:40<00:24, 47.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  81% 4780/5899 [01:40<00:23, 47.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  81% 4800/5899 [01:40<00:23, 47.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  82% 4820/5899 [01:40<00:22, 47.76it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  82% 4840/5899 [01:41<00:22, 47.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  82% 4860/5899 [01:41<00:21, 47.98it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  83% 4880/5899 [01:41<00:21, 48.09it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  83% 4900/5899 [01:41<00:20, 48.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  83% 4920/5899 [01:41<00:20, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  84% 4940/5899 [01:42<00:19, 48.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  84% 4960/5899 [01:42<00:19, 48.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  84% 4980/5899 [01:42<00:18, 48.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  85% 5000/5899 [01:42<00:18, 48.76it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  85% 5020/5899 [01:42<00:17, 48.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  85% 5040/5899 [01:42<00:17, 48.98it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  86% 5060/5899 [01:43<00:17, 49.09it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  86% 5080/5899 [01:43<00:16, 49.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  86% 5100/5899 [01:43<00:16, 49.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  87% 5120/5899 [01:43<00:15, 49.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  87% 5140/5899 [01:43<00:15, 49.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  87% 5160/5899 [01:43<00:14, 49.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  88% 5180/5899 [01:44<00:14, 49.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  88% 5200/5899 [01:44<00:14, 49.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  88% 5220/5899 [01:44<00:13, 49.95it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  89% 5240/5899 [01:44<00:13, 50.06it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  89% 5260/5899 [01:44<00:12, 50.17it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  90% 5280/5899 [01:45<00:12, 50.27it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  90% 5300/5899 [01:45<00:11, 50.38it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  90% 5320/5899 [01:45<00:11, 50.49it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  91% 5340/5899 [01:45<00:11, 50.60it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  91% 5360/5899 [01:45<00:10, 50.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  91% 5380/5899 [01:45<00:10, 50.80it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  92% 5400/5899 [01:46<00:09, 50.91it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  92% 5420/5899 [01:46<00:09, 51.01it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  92% 5440/5899 [01:46<00:08, 51.10it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  93% 5460/5899 [01:46<00:08, 51.21it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  93% 5480/5899 [01:46<00:08, 51.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  93% 5500/5899 [01:46<00:07, 51.41it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  94% 5520/5899 [01:47<00:07, 51.51it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  94% 5540/5899 [01:47<00:06, 51.61it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  94% 5560/5899 [01:47<00:06, 51.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  95% 5580/5899 [01:47<00:06, 51.79it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  95% 5600/5899 [01:47<00:05, 51.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  95% 5620/5899 [01:48<00:05, 51.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  96% 5640/5899 [01:48<00:04, 52.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  96% 5660/5899 [01:48<00:04, 52.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  96% 5680/5899 [01:48<00:04, 52.29it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  97% 5700/5899 [01:48<00:03, 52.39it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  97% 5720/5899 [01:48<00:03, 52.50it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  97% 5740/5899 [01:49<00:03, 52.60it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  98% 5760/5899 [01:49<00:02, 52.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  98% 5780/5899 [01:49<00:02, 52.80it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  98% 5800/5899 [01:49<00:01, 52.90it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  99% 5820/5899 [01:49<00:01, 53.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  99% 5840/5899 [01:49<00:01, 53.10it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33:  99% 5860/5899 [01:50<00:00, 53.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33: 100% 5880/5899 [01:50<00:00, 53.30it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 33: 100% 5899/5899 [01:50<00:00, 53.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 34:  80% 4700/5899 [01:34<00:24, 49.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  80% 4720/5899 [01:38<00:24, 47.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  80% 4740/5899 [01:39<00:24, 47.82it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  81% 4760/5899 [01:39<00:23, 47.93it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  81% 4780/5899 [01:39<00:23, 48.05it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  81% 4800/5899 [01:39<00:22, 48.16it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  82% 4820/5899 [01:39<00:22, 48.27it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  82% 4840/5899 [01:40<00:21, 48.38it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  82% 4860/5899 [01:40<00:21, 48.50it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  83% 4880/5899 [01:40<00:20, 48.61it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  83% 4900/5899 [01:40<00:20, 48.72it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  83% 4920/5899 [01:40<00:20, 48.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  84% 4940/5899 [01:40<00:19, 48.96it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  84% 4960/5899 [01:41<00:19, 49.07it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  84% 4980/5899 [01:41<00:18, 49.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  85% 5000/5899 [01:41<00:18, 49.30it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  85% 5020/5899 [01:41<00:17, 49.41it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  85% 5040/5899 [01:41<00:17, 49.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  86% 5060/5899 [01:41<00:16, 49.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  86% 5080/5899 [01:42<00:16, 49.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  86% 5100/5899 [01:42<00:16, 49.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  87% 5120/5899 [01:42<00:15, 49.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  87% 5140/5899 [01:42<00:15, 50.09it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  87% 5160/5899 [01:42<00:14, 50.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  88% 5180/5899 [01:42<00:14, 50.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  88% 5200/5899 [01:43<00:13, 50.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  88% 5220/5899 [01:43<00:13, 50.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  89% 5240/5899 [01:43<00:13, 50.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  89% 5260/5899 [01:43<00:12, 50.76it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  90% 5280/5899 [01:43<00:12, 50.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  90% 5300/5899 [01:43<00:11, 50.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  90% 5320/5899 [01:44<00:11, 51.09it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  91% 5340/5899 [01:44<00:10, 51.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  91% 5360/5899 [01:44<00:10, 51.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  91% 5380/5899 [01:44<00:10, 51.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  92% 5400/5899 [01:44<00:09, 51.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  92% 5420/5899 [01:44<00:09, 51.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  92% 5440/5899 [01:45<00:08, 51.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  93% 5460/5899 [01:45<00:08, 51.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  93% 5480/5899 [01:45<00:08, 51.95it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  93% 5500/5899 [01:45<00:07, 52.06it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  94% 5520/5899 [01:45<00:07, 52.16it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  94% 5540/5899 [01:45<00:06, 52.27it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  94% 5560/5899 [01:46<00:06, 52.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  95% 5580/5899 [01:46<00:06, 52.47it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  95% 5600/5899 [01:46<00:05, 52.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  95% 5620/5899 [01:46<00:05, 52.67it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  96% 5640/5899 [01:46<00:04, 52.76it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  96% 5660/5899 [01:47<00:04, 52.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  96% 5680/5899 [01:47<00:04, 52.95it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  97% 5700/5899 [01:47<00:03, 53.06it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  97% 5720/5899 [01:47<00:03, 53.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  97% 5740/5899 [01:47<00:02, 53.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  98% 5760/5899 [01:47<00:02, 53.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  98% 5780/5899 [01:48<00:02, 53.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  98% 5800/5899 [01:48<00:01, 53.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  99% 5820/5899 [01:48<00:01, 53.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  99% 5840/5899 [01:48<00:01, 53.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34:  99% 5860/5899 [01:48<00:00, 53.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34: 100% 5880/5899 [01:49<00:00, 53.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 34: 100% 5899/5899 [01:49<00:00, 54.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 35:  80% 4700/5899 [01:35<00:24, 49.35it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  80% 4720/5899 [01:39<00:24, 47.51it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  80% 4740/5899 [01:39<00:24, 47.62it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  81% 4760/5899 [01:39<00:23, 47.73it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  81% 4780/5899 [01:39<00:23, 47.85it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  81% 4800/5899 [01:40<00:22, 47.96it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  82% 4820/5899 [01:40<00:22, 48.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  82% 4840/5899 [01:40<00:21, 48.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  82% 4860/5899 [01:40<00:21, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  83% 4880/5899 [01:40<00:21, 48.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  83% 4900/5899 [01:40<00:20, 48.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  83% 4920/5899 [01:41<00:20, 48.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  84% 4940/5899 [01:41<00:19, 48.76it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  84% 4960/5899 [01:41<00:19, 48.88it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  84% 4980/5899 [01:41<00:18, 48.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  85% 5000/5899 [01:41<00:18, 49.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  85% 5020/5899 [01:41<00:17, 49.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  85% 5040/5899 [01:42<00:17, 49.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  86% 5060/5899 [01:42<00:16, 49.44it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  86% 5080/5899 [01:42<00:16, 49.55it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  86% 5100/5899 [01:42<00:16, 49.67it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  87% 5120/5899 [01:42<00:15, 49.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  87% 5140/5899 [01:43<00:15, 49.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  87% 5160/5899 [01:43<00:14, 50.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  88% 5180/5899 [01:43<00:14, 50.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  88% 5200/5899 [01:43<00:13, 50.21it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  88% 5220/5899 [01:43<00:13, 50.32it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  89% 5240/5899 [01:43<00:13, 50.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  89% 5260/5899 [01:44<00:12, 50.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  90% 5280/5899 [01:44<00:12, 50.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  90% 5300/5899 [01:44<00:11, 50.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  90% 5320/5899 [01:44<00:11, 50.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  91% 5340/5899 [01:44<00:10, 50.96it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  91% 5360/5899 [01:44<00:10, 51.07it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  91% 5380/5899 [01:45<00:10, 51.18it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  92% 5400/5899 [01:45<00:09, 51.29it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  92% 5420/5899 [01:45<00:09, 51.40it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  92% 5440/5899 [01:45<00:08, 51.50it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  93% 5460/5899 [01:45<00:08, 51.61it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  93% 5480/5899 [01:45<00:08, 51.72it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  93% 5500/5899 [01:46<00:07, 51.82it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  94% 5520/5899 [01:46<00:07, 51.92it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  94% 5540/5899 [01:46<00:06, 52.02it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  94% 5560/5899 [01:46<00:06, 52.12it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  95% 5580/5899 [01:46<00:06, 52.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  95% 5600/5899 [01:47<00:05, 52.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  95% 5620/5899 [01:47<00:05, 52.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  96% 5640/5899 [01:47<00:04, 52.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  96% 5660/5899 [01:47<00:04, 52.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  96% 5680/5899 [01:47<00:04, 52.73it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  97% 5700/5899 [01:47<00:03, 52.83it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  97% 5720/5899 [01:48<00:03, 52.93it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  97% 5740/5899 [01:48<00:02, 53.03it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  98% 5760/5899 [01:48<00:02, 53.13it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  98% 5780/5899 [01:48<00:02, 53.23it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  98% 5800/5899 [01:48<00:01, 53.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  99% 5820/5899 [01:48<00:01, 53.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  99% 5840/5899 [01:49<00:01, 53.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35:  99% 5860/5899 [01:49<00:00, 53.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35: 100% 5880/5899 [01:49<00:00, 53.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 35: 100% 5899/5899 [01:49<00:00, 53.83it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.340\n",
            "Epoch 36:  80% 4700/5899 [01:35<00:24, 49.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  80% 4720/5899 [01:39<00:24, 47.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  80% 4740/5899 [01:40<00:24, 47.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  81% 4760/5899 [01:40<00:24, 47.45it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  81% 4780/5899 [01:40<00:23, 47.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  81% 4800/5899 [01:40<00:23, 47.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  82% 4820/5899 [01:40<00:22, 47.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  82% 4840/5899 [01:41<00:22, 47.88it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  82% 4860/5899 [01:41<00:21, 47.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  83% 4880/5899 [01:41<00:21, 48.10it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  83% 4900/5899 [01:41<00:20, 48.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  83% 4920/5899 [01:41<00:20, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  84% 4940/5899 [01:42<00:19, 48.41it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  84% 4960/5899 [01:42<00:19, 48.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  84% 4980/5899 [01:42<00:18, 48.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  85% 5000/5899 [01:42<00:18, 48.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  85% 5020/5899 [01:42<00:17, 48.85it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  85% 5040/5899 [01:42<00:17, 48.95it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  86% 5060/5899 [01:43<00:17, 49.06it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  86% 5080/5899 [01:43<00:16, 49.17it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  86% 5100/5899 [01:43<00:16, 49.28it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  87% 5120/5899 [01:43<00:15, 49.39it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  87% 5140/5899 [01:43<00:15, 49.50it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  87% 5160/5899 [01:44<00:14, 49.61it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  88% 5180/5899 [01:44<00:14, 49.72it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  88% 5200/5899 [01:44<00:14, 49.82it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  88% 5220/5899 [01:44<00:13, 49.93it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  89% 5240/5899 [01:44<00:13, 50.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  89% 5260/5899 [01:44<00:12, 50.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  90% 5280/5899 [01:45<00:12, 50.25it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  90% 5300/5899 [01:45<00:11, 50.35it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  90% 5320/5899 [01:45<00:11, 50.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  91% 5340/5899 [01:45<00:11, 50.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  91% 5360/5899 [01:45<00:10, 50.68it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  91% 5380/5899 [01:45<00:10, 50.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  92% 5400/5899 [01:46<00:09, 50.89it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  92% 5420/5899 [01:46<00:09, 50.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  92% 5440/5899 [01:46<00:08, 51.10it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  93% 5460/5899 [01:46<00:08, 51.21it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  93% 5480/5899 [01:46<00:08, 51.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  93% 5500/5899 [01:46<00:07, 51.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  94% 5520/5899 [01:47<00:07, 51.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  94% 5540/5899 [01:47<00:06, 51.62it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  94% 5560/5899 [01:47<00:06, 51.73it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  95% 5580/5899 [01:47<00:06, 51.83it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  95% 5600/5899 [01:47<00:05, 51.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  95% 5620/5899 [01:47<00:05, 52.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  96% 5640/5899 [01:48<00:04, 52.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  96% 5660/5899 [01:48<00:04, 52.25it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  96% 5680/5899 [01:48<00:04, 52.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  97% 5700/5899 [01:48<00:03, 52.47it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  97% 5720/5899 [01:48<00:03, 52.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  97% 5740/5899 [01:48<00:03, 52.67it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  98% 5760/5899 [01:49<00:02, 52.78it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  98% 5780/5899 [01:49<00:02, 52.88it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  98% 5800/5899 [01:49<00:01, 52.98it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  99% 5820/5899 [01:49<00:01, 53.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  99% 5840/5899 [01:49<00:01, 53.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36:  99% 5860/5899 [01:49<00:00, 53.29it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36: 100% 5880/5899 [01:50<00:00, 53.40it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 36: 100% 5899/5899 [01:50<00:00, 53.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 37:  80% 4700/5899 [01:35<00:24, 49.28it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  80% 4720/5899 [01:39<00:24, 47.48it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  80% 4740/5899 [01:39<00:24, 47.58it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  81% 4760/5899 [01:39<00:23, 47.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  81% 4780/5899 [01:39<00:23, 47.81it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  81% 4800/5899 [01:40<00:22, 47.92it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  82% 4820/5899 [01:40<00:22, 48.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  82% 4840/5899 [01:40<00:21, 48.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  82% 4860/5899 [01:40<00:21, 48.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  83% 4880/5899 [01:40<00:21, 48.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  83% 4900/5899 [01:41<00:20, 48.48it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  83% 4920/5899 [01:41<00:20, 48.60it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  84% 4940/5899 [01:41<00:19, 48.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  84% 4960/5899 [01:41<00:19, 48.81it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  84% 4980/5899 [01:41<00:18, 48.93it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  85% 5000/5899 [01:41<00:18, 49.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  85% 5020/5899 [01:42<00:17, 49.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  85% 5040/5899 [01:42<00:17, 49.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  86% 5060/5899 [01:42<00:16, 49.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  86% 5080/5899 [01:42<00:16, 49.48it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  86% 5100/5899 [01:42<00:16, 49.59it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  87% 5120/5899 [01:43<00:15, 49.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  87% 5140/5899 [01:43<00:15, 49.81it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  87% 5160/5899 [01:43<00:14, 49.92it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  88% 5180/5899 [01:43<00:14, 50.03it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  88% 5200/5899 [01:43<00:13, 50.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  88% 5220/5899 [01:43<00:13, 50.25it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  89% 5240/5899 [01:44<00:13, 50.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  89% 5260/5899 [01:44<00:12, 50.47it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  90% 5280/5899 [01:44<00:12, 50.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  90% 5300/5899 [01:44<00:11, 50.68it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  90% 5320/5899 [01:44<00:11, 50.79it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  91% 5340/5899 [01:44<00:10, 50.90it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  91% 5360/5899 [01:45<00:10, 51.00it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  91% 5380/5899 [01:45<00:10, 51.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  92% 5400/5899 [01:45<00:09, 51.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  92% 5420/5899 [01:45<00:09, 51.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  92% 5440/5899 [01:45<00:08, 51.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  93% 5460/5899 [01:45<00:08, 51.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  93% 5480/5899 [01:46<00:08, 51.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  93% 5500/5899 [01:46<00:07, 51.75it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  94% 5520/5899 [01:46<00:07, 51.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  94% 5540/5899 [01:46<00:06, 51.96it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  94% 5560/5899 [01:46<00:06, 52.07it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  95% 5580/5899 [01:46<00:06, 52.18it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  95% 5600/5899 [01:47<00:05, 52.28it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  95% 5620/5899 [01:47<00:05, 52.39it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  96% 5640/5899 [01:47<00:04, 52.49it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  96% 5660/5899 [01:47<00:04, 52.60it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  96% 5680/5899 [01:47<00:04, 52.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  97% 5700/5899 [01:47<00:03, 52.81it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  97% 5720/5899 [01:48<00:03, 52.91it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  97% 5740/5899 [01:48<00:02, 53.02it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  98% 5760/5899 [01:48<00:02, 53.12it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  98% 5780/5899 [01:48<00:02, 53.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  98% 5800/5899 [01:48<00:01, 53.32it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  99% 5820/5899 [01:48<00:01, 53.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  99% 5840/5899 [01:49<00:01, 53.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37:  99% 5860/5899 [01:49<00:00, 53.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37: 100% 5880/5899 [01:49<00:00, 53.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 37: 100% 5899/5899 [01:49<00:00, 53.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 38:  80% 4700/5899 [01:35<00:24, 49.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  80% 4720/5899 [01:39<00:24, 47.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  80% 4740/5899 [01:39<00:24, 47.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  81% 4760/5899 [01:39<00:23, 47.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  81% 4780/5899 [01:40<00:23, 47.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  81% 4800/5899 [01:40<00:22, 47.85it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  82% 4820/5899 [01:40<00:22, 47.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  82% 4840/5899 [01:40<00:22, 48.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  82% 4860/5899 [01:40<00:21, 48.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  83% 4880/5899 [01:41<00:21, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  83% 4900/5899 [01:41<00:20, 48.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  83% 4920/5899 [01:41<00:20, 48.52it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  84% 4940/5899 [01:41<00:19, 48.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  84% 4960/5899 [01:41<00:19, 48.73it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  84% 4980/5899 [01:41<00:18, 48.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  85% 5000/5899 [01:42<00:18, 48.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  85% 5020/5899 [01:42<00:17, 49.05it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  85% 5040/5899 [01:42<00:17, 49.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  86% 5060/5899 [01:42<00:17, 49.27it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  86% 5080/5899 [01:42<00:16, 49.38it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  86% 5100/5899 [01:43<00:16, 49.50it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  87% 5120/5899 [01:43<00:15, 49.61it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  87% 5140/5899 [01:43<00:15, 49.72it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  87% 5160/5899 [01:43<00:14, 49.83it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  88% 5180/5899 [01:43<00:14, 49.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  88% 5200/5899 [01:43<00:13, 50.05it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  88% 5220/5899 [01:44<00:13, 50.16it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  89% 5240/5899 [01:44<00:13, 50.28it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  89% 5260/5899 [01:44<00:12, 50.39it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  90% 5280/5899 [01:44<00:12, 50.50it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  90% 5300/5899 [01:44<00:11, 50.60it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  90% 5320/5899 [01:44<00:11, 50.71it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  91% 5340/5899 [01:45<00:10, 50.82it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  91% 5360/5899 [01:45<00:10, 50.93it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  91% 5380/5899 [01:45<00:10, 51.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  92% 5400/5899 [01:45<00:09, 51.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  92% 5420/5899 [01:45<00:09, 51.25it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  92% 5440/5899 [01:45<00:08, 51.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  93% 5460/5899 [01:46<00:08, 51.47it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  93% 5480/5899 [01:46<00:08, 51.58it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  93% 5500/5899 [01:46<00:07, 51.69it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  94% 5520/5899 [01:46<00:07, 51.79it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  94% 5540/5899 [01:46<00:06, 51.90it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  94% 5560/5899 [01:46<00:06, 52.01it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  95% 5580/5899 [01:47<00:06, 52.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  95% 5600/5899 [01:47<00:05, 52.21it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  95% 5620/5899 [01:47<00:05, 52.32it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  96% 5640/5899 [01:47<00:04, 52.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  96% 5660/5899 [01:47<00:04, 52.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  96% 5680/5899 [01:47<00:04, 52.63it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  97% 5700/5899 [01:48<00:03, 52.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  97% 5720/5899 [01:48<00:03, 52.84it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  97% 5740/5899 [01:48<00:03, 52.95it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  98% 5760/5899 [01:48<00:02, 53.06it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  98% 5780/5899 [01:48<00:02, 53.16it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  98% 5800/5899 [01:48<00:01, 53.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  99% 5820/5899 [01:49<00:01, 53.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  99% 5840/5899 [01:49<00:01, 53.47it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38:  99% 5860/5899 [01:49<00:00, 53.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38: 100% 5880/5899 [01:49<00:00, 53.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 38: 100% 5899/5899 [01:49<00:00, 53.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.339\n",
            "Epoch 39:  80% 4700/5899 [01:35<00:24, 49.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  80% 4720/5899 [01:39<00:24, 47.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  80% 4740/5899 [01:39<00:24, 47.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  81% 4760/5899 [01:39<00:23, 47.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  81% 4780/5899 [01:39<00:23, 47.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  81% 4800/5899 [01:39<00:22, 48.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  82% 4820/5899 [01:40<00:22, 48.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  82% 4840/5899 [01:40<00:21, 48.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  82% 4860/5899 [01:40<00:21, 48.34it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  83% 4880/5899 [01:40<00:21, 48.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  83% 4900/5899 [01:40<00:20, 48.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  83% 4920/5899 [01:41<00:20, 48.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  84% 4940/5899 [01:41<00:19, 48.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  84% 4960/5899 [01:41<00:19, 48.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  84% 4980/5899 [01:41<00:18, 48.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  85% 5000/5899 [01:41<00:18, 49.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  85% 5020/5899 [01:42<00:17, 49.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  85% 5040/5899 [01:42<00:17, 49.30it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  86% 5060/5899 [01:42<00:16, 49.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  86% 5080/5899 [01:42<00:16, 49.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  86% 5100/5899 [01:42<00:16, 49.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  87% 5120/5899 [01:43<00:15, 49.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  87% 5140/5899 [01:43<00:15, 49.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  87% 5160/5899 [01:43<00:14, 49.92it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  88% 5180/5899 [01:43<00:14, 50.03it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  88% 5200/5899 [01:43<00:13, 50.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  88% 5220/5899 [01:43<00:13, 50.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  89% 5240/5899 [01:44<00:13, 50.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  89% 5260/5899 [01:44<00:12, 50.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  90% 5280/5899 [01:44<00:12, 50.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  90% 5300/5899 [01:44<00:11, 50.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  90% 5320/5899 [01:44<00:11, 50.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  91% 5340/5899 [01:44<00:10, 50.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  91% 5360/5899 [01:45<00:10, 51.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  91% 5380/5899 [01:45<00:10, 51.10it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  92% 5400/5899 [01:45<00:09, 51.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  92% 5420/5899 [01:45<00:09, 51.32it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  92% 5440/5899 [01:45<00:08, 51.42it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  93% 5460/5899 [01:45<00:08, 51.53it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  93% 5480/5899 [01:46<00:08, 51.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  93% 5500/5899 [01:46<00:07, 51.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  94% 5520/5899 [01:46<00:07, 51.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  94% 5540/5899 [01:46<00:06, 51.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  94% 5560/5899 [01:46<00:06, 52.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  95% 5580/5899 [01:46<00:06, 52.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  95% 5600/5899 [01:47<00:05, 52.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  95% 5620/5899 [01:47<00:05, 52.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  96% 5640/5899 [01:47<00:04, 52.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  96% 5660/5899 [01:47<00:04, 52.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  96% 5680/5899 [01:47<00:04, 52.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  97% 5700/5899 [01:47<00:03, 52.79it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  97% 5720/5899 [01:48<00:03, 52.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  97% 5740/5899 [01:48<00:03, 52.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  98% 5760/5899 [01:48<00:02, 53.10it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  98% 5780/5899 [01:48<00:02, 53.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  98% 5800/5899 [01:48<00:01, 53.30it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  99% 5820/5899 [01:48<00:01, 53.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  99% 5840/5899 [01:49<00:01, 53.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39:  99% 5860/5899 [01:49<00:00, 53.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39: 100% 5880/5899 [01:49<00:00, 53.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 39: 100% 5899/5899 [01:49<00:00, 53.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 40:  80% 4700/5899 [01:35<00:24, 49.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  80% 4720/5899 [01:39<00:24, 47.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  80% 4740/5899 [01:39<00:24, 47.47it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  81% 4760/5899 [01:40<00:23, 47.58it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  81% 4780/5899 [01:40<00:23, 47.69it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  81% 4800/5899 [01:40<00:22, 47.81it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  82% 4820/5899 [01:40<00:22, 47.92it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  82% 4840/5899 [01:40<00:22, 48.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  82% 4860/5899 [01:40<00:21, 48.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  83% 4880/5899 [01:41<00:21, 48.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  83% 4900/5899 [01:41<00:20, 48.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  83% 4920/5899 [01:41<00:20, 48.48it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  84% 4940/5899 [01:41<00:19, 48.59it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  84% 4960/5899 [01:41<00:19, 48.70it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  84% 4980/5899 [01:42<00:18, 48.81it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  85% 5000/5899 [01:42<00:18, 48.92it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  85% 5020/5899 [01:42<00:17, 49.03it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  85% 5040/5899 [01:42<00:17, 49.13it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  86% 5060/5899 [01:42<00:17, 49.24it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  86% 5080/5899 [01:42<00:16, 49.34it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  86% 5100/5899 [01:43<00:16, 49.45it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  87% 5120/5899 [01:43<00:15, 49.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  87% 5140/5899 [01:43<00:15, 49.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  87% 5160/5899 [01:43<00:14, 49.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  88% 5180/5899 [01:43<00:14, 49.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  88% 5200/5899 [01:44<00:13, 49.98it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  88% 5220/5899 [01:44<00:13, 50.09it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  89% 5240/5899 [01:44<00:13, 50.20it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  89% 5260/5899 [01:44<00:12, 50.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  90% 5280/5899 [01:44<00:12, 50.42it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  90% 5300/5899 [01:44<00:11, 50.51it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  90% 5320/5899 [01:45<00:11, 50.61it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  91% 5340/5899 [01:45<00:11, 50.71it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  91% 5360/5899 [01:45<00:10, 50.82it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  91% 5380/5899 [01:45<00:10, 50.93it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  92% 5400/5899 [01:45<00:09, 51.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  92% 5420/5899 [01:45<00:09, 51.14it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  92% 5440/5899 [01:46<00:08, 51.25it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  93% 5460/5899 [01:46<00:08, 51.35it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  93% 5480/5899 [01:46<00:08, 51.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  93% 5500/5899 [01:46<00:07, 51.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  94% 5520/5899 [01:46<00:07, 51.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  94% 5540/5899 [01:47<00:06, 51.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  94% 5560/5899 [01:47<00:06, 51.86it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  95% 5580/5899 [01:47<00:06, 51.96it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  95% 5600/5899 [01:47<00:05, 52.06it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  95% 5620/5899 [01:47<00:05, 52.16it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  96% 5640/5899 [01:47<00:04, 52.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  96% 5660/5899 [01:48<00:04, 52.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  96% 5680/5899 [01:48<00:04, 52.47it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  97% 5700/5899 [01:48<00:03, 52.57it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  97% 5720/5899 [01:48<00:03, 52.67it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  97% 5740/5899 [01:48<00:03, 52.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  98% 5760/5899 [01:48<00:02, 52.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  98% 5780/5899 [01:49<00:02, 52.98it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  98% 5800/5899 [01:49<00:01, 53.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  99% 5820/5899 [01:49<00:01, 53.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  99% 5840/5899 [01:49<00:01, 53.29it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40:  99% 5860/5899 [01:49<00:00, 53.39it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40: 100% 5880/5899 [01:49<00:00, 53.49it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 40: 100% 5899/5899 [01:50<00:00, 53.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 41:  80% 4700/5899 [01:35<00:24, 49.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  80% 4720/5899 [01:39<00:24, 47.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  80% 4740/5899 [01:39<00:24, 47.65it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  81% 4760/5899 [01:39<00:23, 47.76it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  81% 4780/5899 [01:39<00:23, 47.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  81% 4800/5899 [01:40<00:22, 47.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  82% 4820/5899 [01:40<00:22, 48.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  82% 4840/5899 [01:40<00:21, 48.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  82% 4860/5899 [01:40<00:21, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  83% 4880/5899 [01:40<00:21, 48.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  83% 4900/5899 [01:40<00:20, 48.54it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  83% 4920/5899 [01:41<00:20, 48.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  84% 4940/5899 [01:41<00:19, 48.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  84% 4960/5899 [01:41<00:19, 48.88it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  84% 4980/5899 [01:41<00:18, 48.99it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  85% 5000/5899 [01:41<00:18, 49.11it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  85% 5020/5899 [01:41<00:17, 49.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  85% 5040/5899 [01:42<00:17, 49.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  86% 5060/5899 [01:42<00:16, 49.44it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  86% 5080/5899 [01:42<00:16, 49.55it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  86% 5100/5899 [01:42<00:16, 49.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  87% 5120/5899 [01:42<00:15, 49.77it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  87% 5140/5899 [01:43<00:15, 49.87it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  87% 5160/5899 [01:43<00:14, 49.97it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  88% 5180/5899 [01:43<00:14, 50.08it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  88% 5200/5899 [01:43<00:13, 50.19it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  88% 5220/5899 [01:43<00:13, 50.29it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  89% 5240/5899 [01:43<00:13, 50.40it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  89% 5260/5899 [01:44<00:12, 50.51it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  90% 5280/5899 [01:44<00:12, 50.62it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  90% 5300/5899 [01:44<00:11, 50.72it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  90% 5320/5899 [01:44<00:11, 50.83it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  91% 5340/5899 [01:44<00:10, 50.94it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  91% 5360/5899 [01:45<00:10, 51.04it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  91% 5380/5899 [01:45<00:10, 51.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  92% 5400/5899 [01:45<00:09, 51.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  92% 5420/5899 [01:45<00:09, 51.37it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  92% 5440/5899 [01:45<00:08, 51.48it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  93% 5460/5899 [01:45<00:08, 51.58it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  93% 5480/5899 [01:46<00:08, 51.69it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  93% 5500/5899 [01:46<00:07, 51.80it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  94% 5520/5899 [01:46<00:07, 51.90it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  94% 5540/5899 [01:46<00:06, 52.01it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  94% 5560/5899 [01:46<00:06, 52.12it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  95% 5580/5899 [01:46<00:06, 52.22it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  95% 5600/5899 [01:47<00:05, 52.33it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  95% 5620/5899 [01:47<00:05, 52.43it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  96% 5640/5899 [01:47<00:04, 52.53it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  96% 5660/5899 [01:47<00:04, 52.64it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  96% 5680/5899 [01:47<00:04, 52.74it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  97% 5700/5899 [01:47<00:03, 52.85it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  97% 5720/5899 [01:48<00:03, 52.95it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  97% 5740/5899 [01:48<00:02, 53.05it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  98% 5760/5899 [01:48<00:02, 53.15it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  98% 5780/5899 [01:48<00:02, 53.26it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  98% 5800/5899 [01:48<00:01, 53.36it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  99% 5820/5899 [01:48<00:01, 53.46it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  99% 5840/5899 [01:49<00:01, 53.56it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41:  99% 5860/5899 [01:49<00:00, 53.66it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41: 100% 5880/5899 [01:49<00:00, 53.76it/s, loss=4.39, v_num=6, train_loss_step=4.400, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 41: 100% 5899/5899 [01:49<00:00, 53.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 42:  80% 4700/5899 [01:35<00:24, 49.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  80% 4720/5899 [01:39<00:24, 47.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  80% 4740/5899 [01:39<00:24, 47.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  81% 4760/5899 [01:39<00:23, 47.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  81% 4780/5899 [01:39<00:23, 47.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  81% 4800/5899 [01:39<00:22, 48.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  82% 4820/5899 [01:39<00:22, 48.22it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  82% 4840/5899 [01:40<00:21, 48.34it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  82% 4860/5899 [01:40<00:21, 48.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  83% 4880/5899 [01:40<00:20, 48.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  83% 4900/5899 [01:40<00:20, 48.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  83% 4920/5899 [01:40<00:20, 48.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  84% 4940/5899 [01:41<00:19, 48.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  84% 4960/5899 [01:41<00:19, 49.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  84% 4980/5899 [01:41<00:18, 49.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  85% 5000/5899 [01:41<00:18, 49.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  85% 5020/5899 [01:41<00:17, 49.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  85% 5040/5899 [01:41<00:17, 49.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  86% 5060/5899 [01:42<00:16, 49.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  86% 5080/5899 [01:42<00:16, 49.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  86% 5100/5899 [01:42<00:16, 49.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  87% 5120/5899 [01:42<00:15, 49.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  87% 5140/5899 [01:42<00:15, 49.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  87% 5160/5899 [01:43<00:14, 50.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  88% 5180/5899 [01:43<00:14, 50.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  88% 5200/5899 [01:43<00:13, 50.30it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  88% 5220/5899 [01:43<00:13, 50.42it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  89% 5240/5899 [01:43<00:13, 50.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  89% 5260/5899 [01:43<00:12, 50.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  90% 5280/5899 [01:44<00:12, 50.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  90% 5300/5899 [01:44<00:11, 50.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  90% 5320/5899 [01:44<00:11, 50.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  91% 5340/5899 [01:44<00:10, 51.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  91% 5360/5899 [01:44<00:10, 51.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  91% 5380/5899 [01:44<00:10, 51.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  92% 5400/5899 [01:45<00:09, 51.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  92% 5420/5899 [01:45<00:09, 51.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  92% 5440/5899 [01:45<00:08, 51.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  93% 5460/5899 [01:45<00:08, 51.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  93% 5480/5899 [01:45<00:08, 51.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  93% 5500/5899 [01:45<00:07, 51.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  94% 5520/5899 [01:46<00:07, 52.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  94% 5540/5899 [01:46<00:06, 52.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  94% 5560/5899 [01:46<00:06, 52.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  95% 5580/5899 [01:46<00:06, 52.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  95% 5600/5899 [01:46<00:05, 52.42it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  95% 5620/5899 [01:47<00:05, 52.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  96% 5640/5899 [01:47<00:04, 52.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  96% 5660/5899 [01:47<00:04, 52.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  96% 5680/5899 [01:47<00:04, 52.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  97% 5700/5899 [01:47<00:03, 52.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  97% 5720/5899 [01:47<00:03, 53.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  97% 5740/5899 [01:48<00:02, 53.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  98% 5760/5899 [01:48<00:02, 53.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  98% 5780/5899 [01:48<00:02, 53.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  98% 5800/5899 [01:48<00:01, 53.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  99% 5820/5899 [01:48<00:01, 53.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  99% 5840/5899 [01:48<00:01, 53.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42:  99% 5860/5899 [01:49<00:00, 53.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42: 100% 5880/5899 [01:49<00:00, 53.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 42: 100% 5899/5899 [01:49<00:00, 53.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 43:  80% 4700/5899 [01:35<00:24, 49.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  80% 4720/5899 [01:39<00:24, 47.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  80% 4740/5899 [01:39<00:24, 47.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  81% 4760/5899 [01:39<00:23, 47.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  81% 4780/5899 [01:40<00:23, 47.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  81% 4800/5899 [01:40<00:22, 47.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  82% 4820/5899 [01:40<00:22, 47.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  82% 4840/5899 [01:40<00:22, 48.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  82% 4860/5899 [01:40<00:21, 48.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  83% 4880/5899 [01:41<00:21, 48.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  83% 4900/5899 [01:41<00:20, 48.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  83% 4920/5899 [01:41<00:20, 48.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  84% 4940/5899 [01:41<00:19, 48.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  84% 4960/5899 [01:41<00:19, 48.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  84% 4980/5899 [01:41<00:18, 48.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  85% 5000/5899 [01:42<00:18, 48.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  85% 5020/5899 [01:42<00:17, 49.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  85% 5040/5899 [01:42<00:17, 49.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  86% 5060/5899 [01:42<00:17, 49.30it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  86% 5080/5899 [01:42<00:16, 49.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  86% 5100/5899 [01:42<00:16, 49.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  87% 5120/5899 [01:43<00:15, 49.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  87% 5140/5899 [01:43<00:15, 49.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  87% 5160/5899 [01:43<00:14, 49.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  88% 5180/5899 [01:43<00:14, 49.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  88% 5200/5899 [01:43<00:13, 50.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  88% 5220/5899 [01:44<00:13, 50.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  89% 5240/5899 [01:44<00:13, 50.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  89% 5260/5899 [01:44<00:12, 50.34it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  90% 5280/5899 [01:44<00:12, 50.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  90% 5300/5899 [01:44<00:11, 50.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  90% 5320/5899 [01:45<00:11, 50.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  91% 5340/5899 [01:45<00:11, 50.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  91% 5360/5899 [01:45<00:10, 50.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  91% 5380/5899 [01:45<00:10, 50.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  92% 5400/5899 [01:45<00:09, 51.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  92% 5420/5899 [01:45<00:09, 51.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  92% 5440/5899 [01:46<00:08, 51.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  93% 5460/5899 [01:46<00:08, 51.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  93% 5480/5899 [01:46<00:08, 51.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  93% 5500/5899 [01:46<00:07, 51.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  94% 5520/5899 [01:46<00:07, 51.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  94% 5540/5899 [01:46<00:06, 51.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  94% 5560/5899 [01:47<00:06, 51.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  95% 5580/5899 [01:47<00:06, 52.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  95% 5600/5899 [01:47<00:05, 52.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  95% 5620/5899 [01:47<00:05, 52.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  96% 5640/5899 [01:47<00:04, 52.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  96% 5660/5899 [01:47<00:04, 52.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  96% 5680/5899 [01:48<00:04, 52.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  97% 5700/5899 [01:48<00:03, 52.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  97% 5720/5899 [01:48<00:03, 52.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  97% 5740/5899 [01:48<00:03, 52.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  98% 5760/5899 [01:48<00:02, 52.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  98% 5780/5899 [01:48<00:02, 53.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  98% 5800/5899 [01:49<00:01, 53.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  99% 5820/5899 [01:49<00:01, 53.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  99% 5840/5899 [01:49<00:01, 53.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43:  99% 5860/5899 [01:49<00:00, 53.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43: 100% 5880/5899 [01:49<00:00, 53.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 43: 100% 5899/5899 [01:49<00:00, 53.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.338\n",
            "Epoch 44:  80% 4700/5899 [01:35<00:24, 49.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  80% 4720/5899 [01:39<00:24, 47.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  80% 4740/5899 [01:39<00:24, 47.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  81% 4760/5899 [01:39<00:23, 47.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  81% 4780/5899 [01:39<00:23, 47.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  81% 4800/5899 [01:40<00:22, 47.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  82% 4820/5899 [01:40<00:22, 48.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  82% 4840/5899 [01:40<00:21, 48.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  82% 4860/5899 [01:40<00:21, 48.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  83% 4880/5899 [01:40<00:21, 48.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  83% 4900/5899 [01:40<00:20, 48.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  83% 4920/5899 [01:41<00:20, 48.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  84% 4940/5899 [01:41<00:19, 48.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  84% 4960/5899 [01:41<00:19, 48.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  84% 4980/5899 [01:41<00:18, 48.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  85% 5000/5899 [01:41<00:18, 49.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  85% 5020/5899 [01:42<00:17, 49.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  85% 5040/5899 [01:42<00:17, 49.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  86% 5060/5899 [01:42<00:16, 49.42it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  86% 5080/5899 [01:42<00:16, 49.53it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  86% 5100/5899 [01:42<00:16, 49.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  87% 5120/5899 [01:42<00:15, 49.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  87% 5140/5899 [01:43<00:15, 49.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  87% 5160/5899 [01:43<00:14, 49.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  88% 5180/5899 [01:43<00:14, 50.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  88% 5200/5899 [01:43<00:13, 50.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  88% 5220/5899 [01:43<00:13, 50.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  89% 5240/5899 [01:43<00:13, 50.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  89% 5260/5899 [01:44<00:12, 50.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  90% 5280/5899 [01:44<00:12, 50.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  90% 5300/5899 [01:44<00:11, 50.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  90% 5320/5899 [01:44<00:11, 50.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  91% 5340/5899 [01:44<00:10, 50.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  91% 5360/5899 [01:44<00:10, 51.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  91% 5380/5899 [01:45<00:10, 51.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  92% 5400/5899 [01:45<00:09, 51.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  92% 5420/5899 [01:45<00:09, 51.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  92% 5440/5899 [01:45<00:08, 51.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  93% 5460/5899 [01:45<00:08, 51.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  93% 5480/5899 [01:45<00:08, 51.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  93% 5500/5899 [01:46<00:07, 51.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  94% 5520/5899 [01:46<00:07, 51.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  94% 5540/5899 [01:46<00:06, 52.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  94% 5560/5899 [01:46<00:06, 52.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  95% 5580/5899 [01:46<00:06, 52.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  95% 5600/5899 [01:46<00:05, 52.34it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  95% 5620/5899 [01:47<00:05, 52.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  96% 5640/5899 [01:47<00:04, 52.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  96% 5660/5899 [01:47<00:04, 52.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  96% 5680/5899 [01:47<00:04, 52.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  97% 5700/5899 [01:47<00:03, 52.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  97% 5720/5899 [01:48<00:03, 52.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  97% 5740/5899 [01:48<00:02, 53.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  98% 5760/5899 [01:48<00:02, 53.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  98% 5780/5899 [01:48<00:02, 53.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  98% 5800/5899 [01:48<00:01, 53.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  99% 5820/5899 [01:48<00:01, 53.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  99% 5840/5899 [01:49<00:01, 53.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44:  99% 5860/5899 [01:49<00:00, 53.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44: 100% 5880/5899 [01:49<00:00, 53.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 44: 100% 5899/5899 [01:49<00:00, 53.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 45:  80% 4700/5899 [01:35<00:24, 49.10it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  80% 4720/5899 [01:39<00:24, 47.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  80% 4740/5899 [01:40<00:24, 47.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  81% 4760/5899 [01:40<00:23, 47.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  81% 4780/5899 [01:40<00:23, 47.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  81% 4800/5899 [01:40<00:23, 47.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  82% 4820/5899 [01:40<00:22, 47.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  82% 4840/5899 [01:40<00:22, 47.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  82% 4860/5899 [01:41<00:21, 48.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  83% 4880/5899 [01:41<00:21, 48.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  83% 4900/5899 [01:41<00:20, 48.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  83% 4920/5899 [01:41<00:20, 48.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  84% 4940/5899 [01:41<00:19, 48.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  84% 4960/5899 [01:41<00:19, 48.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  84% 4980/5899 [01:42<00:18, 48.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  85% 5000/5899 [01:42<00:18, 48.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  85% 5020/5899 [01:42<00:17, 48.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  85% 5040/5899 [01:42<00:17, 49.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  86% 5060/5899 [01:42<00:17, 49.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  86% 5080/5899 [01:43<00:16, 49.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  86% 5100/5899 [01:43<00:16, 49.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  87% 5120/5899 [01:43<00:15, 49.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  87% 5140/5899 [01:43<00:15, 49.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  87% 5160/5899 [01:43<00:14, 49.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  88% 5180/5899 [01:43<00:14, 49.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  88% 5200/5899 [01:44<00:13, 49.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  88% 5220/5899 [01:44<00:13, 50.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  89% 5240/5899 [01:44<00:13, 50.22it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  89% 5260/5899 [01:44<00:12, 50.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  90% 5280/5899 [01:44<00:12, 50.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  90% 5300/5899 [01:44<00:11, 50.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  90% 5320/5899 [01:45<00:11, 50.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  91% 5340/5899 [01:45<00:11, 50.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  91% 5360/5899 [01:45<00:10, 50.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  91% 5380/5899 [01:45<00:10, 50.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  92% 5400/5899 [01:45<00:09, 51.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  92% 5420/5899 [01:45<00:09, 51.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  92% 5440/5899 [01:46<00:08, 51.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  93% 5460/5899 [01:46<00:08, 51.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  93% 5480/5899 [01:46<00:08, 51.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  93% 5500/5899 [01:46<00:07, 51.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  94% 5520/5899 [01:46<00:07, 51.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  94% 5540/5899 [01:46<00:06, 51.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  94% 5560/5899 [01:47<00:06, 51.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  95% 5580/5899 [01:47<00:06, 52.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  95% 5600/5899 [01:47<00:05, 52.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  95% 5620/5899 [01:47<00:05, 52.22it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  96% 5640/5899 [01:47<00:04, 52.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  96% 5660/5899 [01:47<00:04, 52.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  96% 5680/5899 [01:48<00:04, 52.53it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  97% 5700/5899 [01:48<00:03, 52.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  97% 5720/5899 [01:48<00:03, 52.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  97% 5740/5899 [01:48<00:03, 52.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  98% 5760/5899 [01:48<00:02, 52.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  98% 5780/5899 [01:48<00:02, 53.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  98% 5800/5899 [01:49<00:01, 53.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  99% 5820/5899 [01:49<00:01, 53.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  99% 5840/5899 [01:49<00:01, 53.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45:  99% 5860/5899 [01:49<00:00, 53.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45: 100% 5880/5899 [01:49<00:00, 53.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 45: 100% 5899/5899 [01:49<00:00, 53.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 46:  80% 4700/5899 [01:35<00:24, 49.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  80% 4720/5899 [01:39<00:24, 47.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  80% 4740/5899 [01:40<00:24, 47.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  81% 4760/5899 [01:40<00:23, 47.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  81% 4780/5899 [01:40<00:23, 47.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  81% 4800/5899 [01:40<00:23, 47.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  82% 4820/5899 [01:40<00:22, 47.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  82% 4840/5899 [01:40<00:22, 47.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  82% 4860/5899 [01:41<00:21, 48.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  83% 4880/5899 [01:41<00:21, 48.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  83% 4900/5899 [01:41<00:20, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  83% 4920/5899 [01:41<00:20, 48.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  84% 4940/5899 [01:41<00:19, 48.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  84% 4960/5899 [01:41<00:19, 48.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  84% 4980/5899 [01:42<00:18, 48.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  85% 5000/5899 [01:42<00:18, 48.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  85% 5020/5899 [01:42<00:17, 49.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  85% 5040/5899 [01:42<00:17, 49.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  86% 5060/5899 [01:42<00:17, 49.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  86% 5080/5899 [01:42<00:16, 49.32it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  86% 5100/5899 [01:43<00:16, 49.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  87% 5120/5899 [01:43<00:15, 49.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  87% 5140/5899 [01:43<00:15, 49.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  87% 5160/5899 [01:43<00:14, 49.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  88% 5180/5899 [01:43<00:14, 49.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  88% 5200/5899 [01:44<00:13, 50.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  88% 5220/5899 [01:44<00:13, 50.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  89% 5240/5899 [01:44<00:13, 50.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  89% 5260/5899 [01:44<00:12, 50.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  90% 5280/5899 [01:44<00:12, 50.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  90% 5300/5899 [01:44<00:11, 50.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  90% 5320/5899 [01:45<00:11, 50.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  91% 5340/5899 [01:45<00:11, 50.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  91% 5360/5899 [01:45<00:10, 50.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  91% 5380/5899 [01:45<00:10, 50.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  92% 5400/5899 [01:45<00:09, 51.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  92% 5420/5899 [01:45<00:09, 51.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  92% 5440/5899 [01:46<00:08, 51.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  93% 5460/5899 [01:46<00:08, 51.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  93% 5480/5899 [01:46<00:08, 51.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  93% 5500/5899 [01:46<00:07, 51.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  94% 5520/5899 [01:46<00:07, 51.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  94% 5540/5899 [01:46<00:06, 51.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  94% 5560/5899 [01:47<00:06, 51.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  95% 5580/5899 [01:47<00:06, 52.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  95% 5600/5899 [01:47<00:05, 52.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  95% 5620/5899 [01:47<00:05, 52.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  96% 5640/5899 [01:47<00:04, 52.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  96% 5660/5899 [01:47<00:04, 52.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  96% 5680/5899 [01:48<00:04, 52.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  97% 5700/5899 [01:48<00:03, 52.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  97% 5720/5899 [01:48<00:03, 52.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  97% 5740/5899 [01:48<00:03, 52.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  98% 5760/5899 [01:48<00:02, 52.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  98% 5780/5899 [01:48<00:02, 53.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  98% 5800/5899 [01:49<00:01, 53.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  99% 5820/5899 [01:49<00:01, 53.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  99% 5840/5899 [01:49<00:01, 53.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46:  99% 5860/5899 [01:49<00:00, 53.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46: 100% 5880/5899 [01:49<00:00, 53.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 46: 100% 5899/5899 [01:49<00:00, 53.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 47:  80% 4700/5899 [01:35<00:24, 49.34it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  80% 4720/5899 [01:39<00:24, 47.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  80% 4740/5899 [01:39<00:24, 47.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  81% 4760/5899 [01:39<00:23, 47.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  81% 4780/5899 [01:39<00:23, 47.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  81% 4800/5899 [01:40<00:22, 47.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  82% 4820/5899 [01:40<00:22, 48.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  82% 4840/5899 [01:40<00:21, 48.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  82% 4860/5899 [01:40<00:21, 48.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  83% 4880/5899 [01:40<00:21, 48.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  83% 4900/5899 [01:41<00:20, 48.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  83% 4920/5899 [01:41<00:20, 48.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  84% 4940/5899 [01:41<00:19, 48.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  84% 4960/5899 [01:41<00:19, 48.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  84% 4980/5899 [01:41<00:18, 48.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  85% 5000/5899 [01:41<00:18, 49.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  85% 5020/5899 [01:42<00:17, 49.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  85% 5040/5899 [01:42<00:17, 49.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  86% 5060/5899 [01:42<00:16, 49.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  86% 5080/5899 [01:42<00:16, 49.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  86% 5100/5899 [01:42<00:16, 49.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  87% 5120/5899 [01:42<00:15, 49.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  87% 5140/5899 [01:43<00:15, 49.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  87% 5160/5899 [01:43<00:14, 49.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  88% 5180/5899 [01:43<00:14, 50.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  88% 5200/5899 [01:43<00:13, 50.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  88% 5220/5899 [01:43<00:13, 50.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  89% 5240/5899 [01:43<00:13, 50.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  89% 5260/5899 [01:44<00:12, 50.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  90% 5280/5899 [01:44<00:12, 50.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  90% 5300/5899 [01:44<00:11, 50.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  90% 5320/5899 [01:44<00:11, 50.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  91% 5340/5899 [01:44<00:10, 50.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  91% 5360/5899 [01:45<00:10, 51.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  91% 5380/5899 [01:45<00:10, 51.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  92% 5400/5899 [01:45<00:09, 51.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  92% 5420/5899 [01:45<00:09, 51.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  92% 5440/5899 [01:45<00:08, 51.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  93% 5460/5899 [01:45<00:08, 51.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  93% 5480/5899 [01:46<00:08, 51.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  93% 5500/5899 [01:46<00:07, 51.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  94% 5520/5899 [01:46<00:07, 51.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  94% 5540/5899 [01:46<00:06, 51.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  94% 5560/5899 [01:46<00:06, 52.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  95% 5580/5899 [01:46<00:06, 52.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  95% 5600/5899 [01:47<00:05, 52.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  95% 5620/5899 [01:47<00:05, 52.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  96% 5640/5899 [01:47<00:04, 52.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  96% 5660/5899 [01:47<00:04, 52.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  96% 5680/5899 [01:47<00:04, 52.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  97% 5700/5899 [01:47<00:03, 52.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  97% 5720/5899 [01:48<00:03, 52.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  97% 5740/5899 [01:48<00:02, 53.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  98% 5760/5899 [01:48<00:02, 53.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  98% 5780/5899 [01:48<00:02, 53.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  98% 5800/5899 [01:48<00:01, 53.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  99% 5820/5899 [01:48<00:01, 53.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  99% 5840/5899 [01:49<00:01, 53.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47:  99% 5860/5899 [01:49<00:00, 53.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47: 100% 5880/5899 [01:49<00:00, 53.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 47: 100% 5899/5899 [01:49<00:00, 53.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 48:  80% 4700/5899 [01:36<00:24, 48.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  80% 4720/5899 [01:40<00:25, 46.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  80% 4740/5899 [01:40<00:24, 47.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  81% 4760/5899 [01:40<00:24, 47.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  81% 4780/5899 [01:41<00:23, 47.32it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  81% 4800/5899 [01:41<00:23, 47.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  82% 4820/5899 [01:41<00:22, 47.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  82% 4840/5899 [01:41<00:22, 47.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  82% 4860/5899 [01:41<00:21, 47.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  83% 4880/5899 [01:41<00:21, 47.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  83% 4900/5899 [01:42<00:20, 48.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  83% 4920/5899 [01:42<00:20, 48.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  84% 4940/5899 [01:42<00:19, 48.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  84% 4960/5899 [01:42<00:19, 48.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  84% 4980/5899 [01:42<00:18, 48.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  85% 5000/5899 [01:42<00:18, 48.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  85% 5020/5899 [01:43<00:18, 48.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  85% 5040/5899 [01:43<00:17, 48.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  86% 5060/5899 [01:43<00:17, 48.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  86% 5080/5899 [01:43<00:16, 49.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  86% 5100/5899 [01:43<00:16, 49.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  87% 5120/5899 [01:43<00:15, 49.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  87% 5140/5899 [01:44<00:15, 49.34it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  87% 5160/5899 [01:44<00:14, 49.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  88% 5180/5899 [01:44<00:14, 49.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  88% 5200/5899 [01:44<00:14, 49.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  88% 5220/5899 [01:44<00:13, 49.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  89% 5240/5899 [01:45<00:13, 49.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  89% 5260/5899 [01:45<00:12, 49.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  90% 5280/5899 [01:45<00:12, 50.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  90% 5300/5899 [01:45<00:11, 50.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  90% 5320/5899 [01:45<00:11, 50.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  91% 5340/5899 [01:46<00:11, 50.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  91% 5360/5899 [01:46<00:10, 50.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  91% 5380/5899 [01:46<00:10, 50.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  92% 5400/5899 [01:46<00:09, 50.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  92% 5420/5899 [01:46<00:09, 50.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  92% 5440/5899 [01:46<00:09, 50.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  93% 5460/5899 [01:47<00:08, 50.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  93% 5480/5899 [01:47<00:08, 51.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  93% 5500/5899 [01:47<00:07, 51.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  94% 5520/5899 [01:47<00:07, 51.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  94% 5540/5899 [01:47<00:06, 51.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  94% 5560/5899 [01:48<00:06, 51.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  95% 5580/5899 [01:48<00:06, 51.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  95% 5600/5899 [01:48<00:05, 51.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  95% 5620/5899 [01:48<00:05, 51.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  96% 5640/5899 [01:48<00:04, 51.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  96% 5660/5899 [01:48<00:04, 51.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  96% 5680/5899 [01:49<00:04, 52.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  97% 5700/5899 [01:49<00:03, 52.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  97% 5720/5899 [01:49<00:03, 52.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  97% 5740/5899 [01:49<00:03, 52.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  98% 5760/5899 [01:49<00:02, 52.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  98% 5780/5899 [01:49<00:02, 52.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  98% 5800/5899 [01:50<00:01, 52.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  99% 5820/5899 [01:50<00:01, 52.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  99% 5840/5899 [01:50<00:01, 52.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48:  99% 5860/5899 [01:50<00:00, 52.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48: 100% 5880/5899 [01:50<00:00, 53.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 48: 100% 5899/5899 [01:51<00:00, 53.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 49:  80% 4700/5899 [01:36<00:24, 48.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  80% 4720/5899 [01:40<00:25, 47.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  80% 4740/5899 [01:40<00:24, 47.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  81% 4760/5899 [01:40<00:24, 47.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  81% 4780/5899 [01:40<00:23, 47.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  81% 4800/5899 [01:41<00:23, 47.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  82% 4820/5899 [01:41<00:22, 47.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  82% 4840/5899 [01:41<00:22, 47.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  82% 4860/5899 [01:41<00:21, 47.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  83% 4880/5899 [01:41<00:21, 47.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  83% 4900/5899 [01:41<00:20, 48.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  83% 4920/5899 [01:42<00:20, 48.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  84% 4940/5899 [01:42<00:19, 48.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  84% 4960/5899 [01:42<00:19, 48.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  84% 4980/5899 [01:42<00:18, 48.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  85% 5000/5899 [01:42<00:18, 48.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  85% 5020/5899 [01:43<00:18, 48.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  85% 5040/5899 [01:43<00:17, 48.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  86% 5060/5899 [01:43<00:17, 48.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  86% 5080/5899 [01:43<00:16, 49.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  86% 5100/5899 [01:43<00:16, 49.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  87% 5120/5899 [01:43<00:15, 49.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  87% 5140/5899 [01:44<00:15, 49.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  87% 5160/5899 [01:44<00:14, 49.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  88% 5180/5899 [01:44<00:14, 49.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  88% 5200/5899 [01:44<00:14, 49.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  88% 5220/5899 [01:44<00:13, 49.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  89% 5240/5899 [01:44<00:13, 49.92it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  89% 5260/5899 [01:45<00:12, 50.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  90% 5280/5899 [01:45<00:12, 50.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  90% 5300/5899 [01:45<00:11, 50.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  90% 5320/5899 [01:45<00:11, 50.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  91% 5340/5899 [01:45<00:11, 50.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  91% 5360/5899 [01:46<00:10, 50.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  91% 5380/5899 [01:46<00:10, 50.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  92% 5400/5899 [01:46<00:09, 50.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  92% 5420/5899 [01:46<00:09, 50.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  92% 5440/5899 [01:46<00:09, 50.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  93% 5460/5899 [01:46<00:08, 51.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  93% 5480/5899 [01:47<00:08, 51.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  93% 5500/5899 [01:47<00:07, 51.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  94% 5520/5899 [01:47<00:07, 51.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  94% 5540/5899 [01:47<00:06, 51.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  94% 5560/5899 [01:47<00:06, 51.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  95% 5580/5899 [01:48<00:06, 51.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  95% 5600/5899 [01:48<00:05, 51.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  95% 5620/5899 [01:48<00:05, 51.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  96% 5640/5899 [01:48<00:04, 51.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  96% 5660/5899 [01:48<00:04, 52.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  96% 5680/5899 [01:48<00:04, 52.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  97% 5700/5899 [01:49<00:03, 52.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  97% 5720/5899 [01:49<00:03, 52.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  97% 5740/5899 [01:49<00:03, 52.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  98% 5760/5899 [01:49<00:02, 52.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  98% 5780/5899 [01:49<00:02, 52.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  98% 5800/5899 [01:49<00:01, 52.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  99% 5820/5899 [01:50<00:01, 52.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  99% 5840/5899 [01:50<00:01, 52.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49:  99% 5860/5899 [01:50<00:00, 53.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49: 100% 5880/5899 [01:50<00:00, 53.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 49: 100% 5899/5899 [01:50<00:00, 53.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 50:  80% 4700/5899 [01:35<00:24, 49.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 50:  80% 4720/5899 [01:39<00:24, 47.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  80% 4740/5899 [01:39<00:24, 47.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  81% 4760/5899 [01:40<00:23, 47.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  81% 4780/5899 [01:40<00:23, 47.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  81% 4800/5899 [01:40<00:22, 47.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  82% 4820/5899 [01:40<00:22, 47.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  82% 4840/5899 [01:40<00:22, 48.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  82% 4860/5899 [01:40<00:21, 48.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  83% 4880/5899 [01:41<00:21, 48.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  83% 4900/5899 [01:41<00:20, 48.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  83% 4920/5899 [01:41<00:20, 48.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  84% 4940/5899 [01:41<00:19, 48.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  84% 4960/5899 [01:41<00:19, 48.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  84% 4980/5899 [01:41<00:18, 48.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  85% 5000/5899 [01:42<00:18, 48.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  85% 5020/5899 [01:42<00:17, 49.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  85% 5040/5899 [01:42<00:17, 49.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  86% 5060/5899 [01:42<00:17, 49.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  86% 5080/5899 [01:42<00:16, 49.42it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  86% 5100/5899 [01:42<00:16, 49.53it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  87% 5120/5899 [01:43<00:15, 49.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  87% 5140/5899 [01:43<00:15, 49.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  87% 5160/5899 [01:43<00:14, 49.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  88% 5180/5899 [01:43<00:14, 49.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  88% 5200/5899 [01:43<00:13, 50.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  88% 5220/5899 [01:43<00:13, 50.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  89% 5240/5899 [01:44<00:13, 50.30it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  89% 5260/5899 [01:44<00:12, 50.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  90% 5280/5899 [01:44<00:12, 50.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  90% 5300/5899 [01:44<00:11, 50.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  90% 5320/5899 [01:44<00:11, 50.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  91% 5340/5899 [01:45<00:10, 50.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  91% 5360/5899 [01:45<00:10, 50.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  91% 5380/5899 [01:45<00:10, 51.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  92% 5400/5899 [01:45<00:09, 51.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  92% 5420/5899 [01:45<00:09, 51.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  92% 5440/5899 [01:45<00:08, 51.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  93% 5460/5899 [01:46<00:08, 51.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  93% 5480/5899 [01:46<00:08, 51.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  93% 5500/5899 [01:46<00:07, 51.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  94% 5520/5899 [01:46<00:07, 51.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  94% 5540/5899 [01:46<00:06, 51.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  94% 5560/5899 [01:47<00:06, 51.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  95% 5580/5899 [01:47<00:06, 52.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  95% 5600/5899 [01:47<00:05, 52.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  95% 5620/5899 [01:47<00:05, 52.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  96% 5640/5899 [01:47<00:04, 52.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  96% 5660/5899 [01:47<00:04, 52.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  96% 5680/5899 [01:48<00:04, 52.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  97% 5700/5899 [01:48<00:03, 52.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  97% 5720/5899 [01:48<00:03, 52.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  97% 5740/5899 [01:48<00:03, 52.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  98% 5760/5899 [01:48<00:02, 52.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  98% 5780/5899 [01:48<00:02, 53.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  98% 5800/5899 [01:49<00:01, 53.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  99% 5820/5899 [01:49<00:01, 53.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  99% 5840/5899 [01:49<00:01, 53.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50:  99% 5860/5899 [01:49<00:00, 53.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50: 100% 5880/5899 [01:49<00:00, 53.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 50: 100% 5899/5899 [01:49<00:00, 53.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 51:  80% 4700/5899 [01:35<00:24, 49.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 51:  80% 4720/5899 [01:39<00:24, 47.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  80% 4740/5899 [01:39<00:24, 47.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  81% 4760/5899 [01:39<00:23, 47.79it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  81% 4780/5899 [01:39<00:23, 47.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  81% 4800/5899 [01:39<00:22, 48.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  82% 4820/5899 [01:40<00:22, 48.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  82% 4840/5899 [01:40<00:21, 48.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  82% 4860/5899 [01:40<00:21, 48.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  83% 4880/5899 [01:40<00:21, 48.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  83% 4900/5899 [01:40<00:20, 48.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  83% 4920/5899 [01:41<00:20, 48.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  84% 4940/5899 [01:41<00:19, 48.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  84% 4960/5899 [01:41<00:19, 48.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  84% 4980/5899 [01:41<00:18, 49.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  85% 5000/5899 [01:41<00:18, 49.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  85% 5020/5899 [01:41<00:17, 49.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  85% 5040/5899 [01:42<00:17, 49.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  86% 5060/5899 [01:42<00:16, 49.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  86% 5080/5899 [01:42<00:16, 49.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  86% 5100/5899 [01:42<00:16, 49.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  87% 5120/5899 [01:42<00:15, 49.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  87% 5140/5899 [01:42<00:15, 49.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  87% 5160/5899 [01:43<00:14, 50.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  88% 5180/5899 [01:43<00:14, 50.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  88% 5200/5899 [01:43<00:13, 50.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  88% 5220/5899 [01:43<00:13, 50.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  89% 5240/5899 [01:43<00:13, 50.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  89% 5260/5899 [01:43<00:12, 50.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  90% 5280/5899 [01:44<00:12, 50.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  90% 5300/5899 [01:44<00:11, 50.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  90% 5320/5899 [01:44<00:11, 50.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  91% 5340/5899 [01:44<00:10, 51.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  91% 5360/5899 [01:44<00:10, 51.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  91% 5380/5899 [01:44<00:10, 51.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  92% 5400/5899 [01:45<00:09, 51.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  92% 5420/5899 [01:45<00:09, 51.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  92% 5440/5899 [01:45<00:08, 51.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  93% 5460/5899 [01:45<00:08, 51.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  93% 5480/5899 [01:45<00:08, 51.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  93% 5500/5899 [01:45<00:07, 51.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  94% 5520/5899 [01:46<00:07, 52.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  94% 5540/5899 [01:46<00:06, 52.10it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  94% 5560/5899 [01:46<00:06, 52.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  95% 5580/5899 [01:46<00:06, 52.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  95% 5600/5899 [01:46<00:05, 52.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  95% 5620/5899 [01:47<00:05, 52.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  96% 5640/5899 [01:47<00:04, 52.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  96% 5660/5899 [01:47<00:04, 52.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  96% 5680/5899 [01:47<00:04, 52.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  97% 5700/5899 [01:47<00:03, 52.92it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  97% 5720/5899 [01:47<00:03, 53.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  97% 5740/5899 [01:48<00:02, 53.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  98% 5760/5899 [01:48<00:02, 53.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  98% 5780/5899 [01:48<00:02, 53.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  98% 5800/5899 [01:48<00:01, 53.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  99% 5820/5899 [01:48<00:01, 53.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  99% 5840/5899 [01:48<00:01, 53.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51:  99% 5860/5899 [01:49<00:00, 53.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51: 100% 5880/5899 [01:49<00:00, 53.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 51: 100% 5899/5899 [01:49<00:00, 53.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Metric avg_val_loss improved by 0.001 >= min_delta = 0.001. New best score: 4.337\n",
            "Epoch 52:  80% 4700/5899 [01:35<00:24, 49.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 52:  80% 4720/5899 [01:39<00:24, 47.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  80% 4740/5899 [01:39<00:24, 47.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  81% 4760/5899 [01:39<00:23, 47.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  81% 4780/5899 [01:40<00:23, 47.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  81% 4800/5899 [01:40<00:22, 47.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  82% 4820/5899 [01:40<00:22, 48.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  82% 4840/5899 [01:40<00:22, 48.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  82% 4860/5899 [01:40<00:21, 48.22it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  83% 4880/5899 [01:40<00:21, 48.34it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  83% 4900/5899 [01:41<00:20, 48.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  83% 4920/5899 [01:41<00:20, 48.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  84% 4940/5899 [01:41<00:19, 48.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  84% 4960/5899 [01:41<00:19, 48.79it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  84% 4980/5899 [01:41<00:18, 48.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  85% 5000/5899 [01:42<00:18, 49.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  85% 5020/5899 [01:42<00:17, 49.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  85% 5040/5899 [01:42<00:17, 49.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  86% 5060/5899 [01:42<00:16, 49.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  86% 5080/5899 [01:42<00:16, 49.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  86% 5100/5899 [01:42<00:16, 49.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  87% 5120/5899 [01:43<00:15, 49.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  87% 5140/5899 [01:43<00:15, 49.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  87% 5160/5899 [01:43<00:14, 49.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  88% 5180/5899 [01:43<00:14, 50.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  88% 5200/5899 [01:43<00:13, 50.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  88% 5220/5899 [01:43<00:13, 50.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  89% 5240/5899 [01:44<00:13, 50.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  89% 5260/5899 [01:44<00:12, 50.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  90% 5280/5899 [01:44<00:12, 50.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  90% 5300/5899 [01:44<00:11, 50.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  90% 5320/5899 [01:44<00:11, 50.79it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  91% 5340/5899 [01:44<00:10, 50.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  91% 5360/5899 [01:45<00:10, 51.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  91% 5380/5899 [01:45<00:10, 51.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  92% 5400/5899 [01:45<00:09, 51.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  92% 5420/5899 [01:45<00:09, 51.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  92% 5440/5899 [01:45<00:08, 51.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  93% 5460/5899 [01:45<00:08, 51.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  93% 5480/5899 [01:46<00:08, 51.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  93% 5500/5899 [01:46<00:07, 51.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  94% 5520/5899 [01:46<00:07, 51.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  94% 5540/5899 [01:46<00:06, 51.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  94% 5560/5899 [01:46<00:06, 52.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  95% 5580/5899 [01:46<00:06, 52.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  95% 5600/5899 [01:47<00:05, 52.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  95% 5620/5899 [01:47<00:05, 52.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  96% 5640/5899 [01:47<00:04, 52.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  96% 5660/5899 [01:47<00:04, 52.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  96% 5680/5899 [01:47<00:04, 52.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  97% 5700/5899 [01:48<00:03, 52.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  97% 5720/5899 [01:48<00:03, 52.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  97% 5740/5899 [01:48<00:03, 52.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  98% 5760/5899 [01:48<00:02, 53.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  98% 5780/5899 [01:48<00:02, 53.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  98% 5800/5899 [01:48<00:01, 53.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  99% 5820/5899 [01:49<00:01, 53.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  99% 5840/5899 [01:49<00:01, 53.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52:  99% 5860/5899 [01:49<00:00, 53.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52: 100% 5880/5899 [01:49<00:00, 53.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 52: 100% 5899/5899 [01:49<00:00, 53.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 53:  80% 4700/5899 [01:35<00:24, 49.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 53:  80% 4720/5899 [01:39<00:24, 47.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  80% 4740/5899 [01:40<00:24, 47.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  81% 4760/5899 [01:40<00:23, 47.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  81% 4780/5899 [01:40<00:23, 47.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  81% 4800/5899 [01:40<00:23, 47.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  82% 4820/5899 [01:40<00:22, 47.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  82% 4840/5899 [01:40<00:22, 47.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  82% 4860/5899 [01:41<00:21, 48.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  83% 4880/5899 [01:41<00:21, 48.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  83% 4900/5899 [01:41<00:20, 48.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  83% 4920/5899 [01:41<00:20, 48.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  84% 4940/5899 [01:41<00:19, 48.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  84% 4960/5899 [01:42<00:19, 48.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  84% 4980/5899 [01:42<00:18, 48.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  85% 5000/5899 [01:42<00:18, 48.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  85% 5020/5899 [01:42<00:17, 48.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  85% 5040/5899 [01:42<00:17, 49.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  86% 5060/5899 [01:42<00:17, 49.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  86% 5080/5899 [01:43<00:16, 49.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  86% 5100/5899 [01:43<00:16, 49.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  87% 5120/5899 [01:43<00:15, 49.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  87% 5140/5899 [01:43<00:15, 49.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  87% 5160/5899 [01:43<00:14, 49.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  88% 5180/5899 [01:43<00:14, 49.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  88% 5200/5899 [01:44<00:13, 49.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  88% 5220/5899 [01:44<00:13, 50.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  89% 5240/5899 [01:44<00:13, 50.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  89% 5260/5899 [01:44<00:12, 50.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  90% 5280/5899 [01:44<00:12, 50.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  90% 5300/5899 [01:44<00:11, 50.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  90% 5320/5899 [01:45<00:11, 50.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  91% 5340/5899 [01:45<00:11, 50.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  91% 5360/5899 [01:45<00:10, 50.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  91% 5380/5899 [01:45<00:10, 50.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  92% 5400/5899 [01:45<00:09, 51.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  92% 5420/5899 [01:46<00:09, 51.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  92% 5440/5899 [01:46<00:08, 51.22it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  93% 5460/5899 [01:46<00:08, 51.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  93% 5480/5899 [01:46<00:08, 51.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  93% 5500/5899 [01:46<00:07, 51.53it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  94% 5520/5899 [01:46<00:07, 51.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  94% 5540/5899 [01:47<00:06, 51.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  94% 5560/5899 [01:47<00:06, 51.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  95% 5580/5899 [01:47<00:06, 51.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  95% 5600/5899 [01:47<00:05, 52.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  95% 5620/5899 [01:47<00:05, 52.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  96% 5640/5899 [01:48<00:04, 52.22it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  96% 5660/5899 [01:48<00:04, 52.32it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  96% 5680/5899 [01:48<00:04, 52.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  97% 5700/5899 [01:48<00:03, 52.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  97% 5720/5899 [01:48<00:03, 52.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  97% 5740/5899 [01:48<00:03, 52.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  98% 5760/5899 [01:49<00:02, 52.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  98% 5780/5899 [01:49<00:02, 52.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  98% 5800/5899 [01:49<00:01, 53.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  99% 5820/5899 [01:49<00:01, 53.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  99% 5840/5899 [01:49<00:01, 53.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53:  99% 5860/5899 [01:49<00:00, 53.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53: 100% 5880/5899 [01:50<00:00, 53.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 53: 100% 5899/5899 [01:50<00:00, 53.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 54:  80% 4700/5899 [01:35<00:24, 49.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 54:  80% 4720/5899 [01:40<00:25, 47.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  80% 4740/5899 [01:40<00:24, 47.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  81% 4760/5899 [01:40<00:24, 47.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  81% 4780/5899 [01:40<00:23, 47.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  81% 4800/5899 [01:40<00:23, 47.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  82% 4820/5899 [01:41<00:22, 47.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  82% 4840/5899 [01:41<00:22, 47.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  82% 4860/5899 [01:41<00:21, 47.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  83% 4880/5899 [01:41<00:21, 48.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  83% 4900/5899 [01:41<00:20, 48.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  83% 4920/5899 [01:41<00:20, 48.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  84% 4940/5899 [01:42<00:19, 48.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  84% 4960/5899 [01:42<00:19, 48.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  84% 4980/5899 [01:42<00:18, 48.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  85% 5000/5899 [01:42<00:18, 48.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  85% 5020/5899 [01:42<00:17, 48.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  85% 5040/5899 [01:42<00:17, 48.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  86% 5060/5899 [01:43<00:17, 49.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  86% 5080/5899 [01:43<00:16, 49.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  86% 5100/5899 [01:43<00:16, 49.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  87% 5120/5899 [01:43<00:15, 49.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  87% 5140/5899 [01:43<00:15, 49.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  87% 5160/5899 [01:43<00:14, 49.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  88% 5180/5899 [01:44<00:14, 49.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  88% 5200/5899 [01:44<00:14, 49.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  88% 5220/5899 [01:44<00:13, 49.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  89% 5240/5899 [01:44<00:13, 50.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  89% 5260/5899 [01:44<00:12, 50.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  90% 5280/5899 [01:45<00:12, 50.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  90% 5300/5899 [01:45<00:11, 50.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  90% 5320/5899 [01:45<00:11, 50.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  91% 5340/5899 [01:45<00:11, 50.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  91% 5360/5899 [01:45<00:10, 50.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  91% 5380/5899 [01:45<00:10, 50.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  92% 5400/5899 [01:46<00:09, 50.92it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  92% 5420/5899 [01:46<00:09, 51.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  92% 5440/5899 [01:46<00:08, 51.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  93% 5460/5899 [01:46<00:08, 51.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  93% 5480/5899 [01:46<00:08, 51.34it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  93% 5500/5899 [01:46<00:07, 51.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  94% 5520/5899 [01:47<00:07, 51.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  94% 5540/5899 [01:47<00:06, 51.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  94% 5560/5899 [01:47<00:06, 51.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  95% 5580/5899 [01:47<00:06, 51.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  95% 5600/5899 [01:47<00:05, 51.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  95% 5620/5899 [01:47<00:05, 52.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  96% 5640/5899 [01:48<00:04, 52.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  96% 5660/5899 [01:48<00:04, 52.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  96% 5680/5899 [01:48<00:04, 52.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  97% 5700/5899 [01:48<00:03, 52.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  97% 5720/5899 [01:48<00:03, 52.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  97% 5740/5899 [01:48<00:03, 52.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  98% 5760/5899 [01:49<00:02, 52.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  98% 5780/5899 [01:49<00:02, 52.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  98% 5800/5899 [01:49<00:01, 52.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  99% 5820/5899 [01:49<00:01, 53.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  99% 5840/5899 [01:49<00:01, 53.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54:  99% 5860/5899 [01:50<00:00, 53.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54: 100% 5880/5899 [01:50<00:00, 53.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 54: 100% 5899/5899 [01:50<00:00, 53.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 55:  80% 4700/5899 [01:36<00:24, 48.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 55:  80% 4720/5899 [01:40<00:25, 46.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  80% 4740/5899 [01:40<00:24, 46.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  81% 4760/5899 [01:41<00:24, 47.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  81% 4780/5899 [01:41<00:23, 47.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  81% 4800/5899 [01:41<00:23, 47.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  82% 4820/5899 [01:41<00:22, 47.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  82% 4840/5899 [01:41<00:22, 47.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  82% 4860/5899 [01:42<00:21, 47.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  83% 4880/5899 [01:42<00:21, 47.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  83% 4900/5899 [01:42<00:20, 47.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  83% 4920/5899 [01:42<00:20, 47.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  84% 4940/5899 [01:42<00:19, 48.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  84% 4960/5899 [01:42<00:19, 48.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  84% 4980/5899 [01:43<00:19, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  85% 5000/5899 [01:43<00:18, 48.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  85% 5020/5899 [01:43<00:18, 48.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  85% 5040/5899 [01:43<00:17, 48.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  86% 5060/5899 [01:43<00:17, 48.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  86% 5080/5899 [01:43<00:16, 48.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  86% 5100/5899 [01:44<00:16, 48.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  87% 5120/5899 [01:44<00:15, 49.10it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  87% 5140/5899 [01:44<00:15, 49.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  87% 5160/5899 [01:44<00:14, 49.32it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  88% 5180/5899 [01:44<00:14, 49.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  88% 5200/5899 [01:44<00:14, 49.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  88% 5220/5899 [01:45<00:13, 49.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  89% 5240/5899 [01:45<00:13, 49.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  89% 5260/5899 [01:45<00:12, 49.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  90% 5280/5899 [01:45<00:12, 49.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  90% 5300/5899 [01:45<00:11, 50.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  90% 5320/5899 [01:45<00:11, 50.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  91% 5340/5899 [01:46<00:11, 50.32it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  91% 5360/5899 [01:46<00:10, 50.42it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  91% 5380/5899 [01:46<00:10, 50.53it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  92% 5400/5899 [01:46<00:09, 50.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  92% 5420/5899 [01:46<00:09, 50.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  92% 5440/5899 [01:47<00:09, 50.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  93% 5460/5899 [01:47<00:08, 50.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  93% 5480/5899 [01:47<00:08, 51.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  93% 5500/5899 [01:47<00:07, 51.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  94% 5520/5899 [01:47<00:07, 51.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  94% 5540/5899 [01:47<00:06, 51.36it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  94% 5560/5899 [01:48<00:06, 51.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  95% 5580/5899 [01:48<00:06, 51.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  95% 5600/5899 [01:48<00:05, 51.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  95% 5620/5899 [01:48<00:05, 51.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  96% 5640/5899 [01:48<00:04, 51.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  96% 5660/5899 [01:48<00:04, 51.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  96% 5680/5899 [01:49<00:04, 52.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  97% 5700/5899 [01:49<00:03, 52.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  97% 5720/5899 [01:49<00:03, 52.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  97% 5740/5899 [01:49<00:03, 52.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  98% 5760/5899 [01:49<00:02, 52.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  98% 5780/5899 [01:49<00:02, 52.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  98% 5800/5899 [01:50<00:01, 52.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  99% 5820/5899 [01:50<00:01, 52.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  99% 5840/5899 [01:50<00:01, 52.90it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55:  99% 5860/5899 [01:50<00:00, 53.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55: 100% 5880/5899 [01:50<00:00, 53.10it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 55: 100% 5899/5899 [01:50<00:00, 53.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 56:  80% 4700/5899 [01:35<00:24, 49.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 56:  80% 4720/5899 [01:39<00:24, 47.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  80% 4740/5899 [01:39<00:24, 47.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  81% 4760/5899 [01:40<00:23, 47.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  81% 4780/5899 [01:40<00:23, 47.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  81% 4800/5899 [01:40<00:22, 47.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  82% 4820/5899 [01:40<00:22, 47.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  82% 4840/5899 [01:40<00:22, 48.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  82% 4860/5899 [01:40<00:21, 48.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  83% 4880/5899 [01:41<00:21, 48.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  83% 4900/5899 [01:41<00:20, 48.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  83% 4920/5899 [01:41<00:20, 48.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  84% 4940/5899 [01:41<00:19, 48.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  84% 4960/5899 [01:41<00:19, 48.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  84% 4980/5899 [01:41<00:18, 48.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  85% 5000/5899 [01:42<00:18, 48.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  85% 5020/5899 [01:42<00:17, 49.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  85% 5040/5899 [01:42<00:17, 49.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  86% 5060/5899 [01:42<00:17, 49.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  86% 5080/5899 [01:42<00:16, 49.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  86% 5100/5899 [01:42<00:16, 49.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  87% 5120/5899 [01:43<00:15, 49.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  87% 5140/5899 [01:43<00:15, 49.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  87% 5160/5899 [01:43<00:14, 49.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  88% 5180/5899 [01:43<00:14, 49.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  88% 5200/5899 [01:43<00:13, 50.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  88% 5220/5899 [01:44<00:13, 50.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  89% 5240/5899 [01:44<00:13, 50.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  89% 5260/5899 [01:44<00:12, 50.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  90% 5280/5899 [01:44<00:12, 50.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  90% 5300/5899 [01:44<00:11, 50.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  90% 5320/5899 [01:44<00:11, 50.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  91% 5340/5899 [01:45<00:10, 50.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  91% 5360/5899 [01:45<00:10, 50.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  91% 5380/5899 [01:45<00:10, 51.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  92% 5400/5899 [01:45<00:09, 51.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  92% 5420/5899 [01:45<00:09, 51.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  92% 5440/5899 [01:45<00:08, 51.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  93% 5460/5899 [01:46<00:08, 51.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  93% 5480/5899 [01:46<00:08, 51.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  93% 5500/5899 [01:46<00:07, 51.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  94% 5520/5899 [01:46<00:07, 51.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  94% 5540/5899 [01:46<00:06, 51.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  94% 5560/5899 [01:46<00:06, 51.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  95% 5580/5899 [01:47<00:06, 52.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  95% 5600/5899 [01:47<00:05, 52.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  95% 5620/5899 [01:47<00:05, 52.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  96% 5640/5899 [01:47<00:04, 52.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  96% 5660/5899 [01:47<00:04, 52.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  96% 5680/5899 [01:48<00:04, 52.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  97% 5700/5899 [01:48<00:03, 52.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  97% 5720/5899 [01:48<00:03, 52.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  97% 5740/5899 [01:48<00:03, 52.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  98% 5760/5899 [01:48<00:02, 52.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  98% 5780/5899 [01:48<00:02, 53.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  98% 5800/5899 [01:49<00:01, 53.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  99% 5820/5899 [01:49<00:01, 53.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  99% 5840/5899 [01:49<00:01, 53.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56:  99% 5860/5899 [01:49<00:00, 53.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56: 100% 5880/5899 [01:49<00:00, 53.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 56: 100% 5899/5899 [01:49<00:00, 53.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 57:  80% 4700/5899 [01:35<00:24, 49.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 57:  80% 4720/5899 [01:39<00:24, 47.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  80% 4740/5899 [01:39<00:24, 47.42it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  81% 4760/5899 [01:40<00:23, 47.53it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  81% 4780/5899 [01:40<00:23, 47.65it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  81% 4800/5899 [01:40<00:23, 47.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  82% 4820/5899 [01:40<00:22, 47.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  82% 4840/5899 [01:40<00:22, 47.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  82% 4860/5899 [01:41<00:21, 48.10it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  83% 4880/5899 [01:41<00:21, 48.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  83% 4900/5899 [01:41<00:20, 48.32it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  83% 4920/5899 [01:41<00:20, 48.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  84% 4940/5899 [01:41<00:19, 48.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  84% 4960/5899 [01:41<00:19, 48.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  84% 4980/5899 [01:42<00:18, 48.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  85% 5000/5899 [01:42<00:18, 48.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  85% 5020/5899 [01:42<00:17, 48.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  85% 5040/5899 [01:42<00:17, 49.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  86% 5060/5899 [01:42<00:17, 49.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  86% 5080/5899 [01:43<00:16, 49.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  86% 5100/5899 [01:43<00:16, 49.41it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  87% 5120/5899 [01:43<00:15, 49.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  87% 5140/5899 [01:43<00:15, 49.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  87% 5160/5899 [01:43<00:14, 49.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  88% 5180/5899 [01:43<00:14, 49.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  88% 5200/5899 [01:44<00:13, 49.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  88% 5220/5899 [01:44<00:13, 50.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  89% 5240/5899 [01:44<00:13, 50.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  89% 5260/5899 [01:44<00:12, 50.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  90% 5280/5899 [01:44<00:12, 50.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  90% 5300/5899 [01:44<00:11, 50.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  90% 5320/5899 [01:45<00:11, 50.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  91% 5340/5899 [01:45<00:11, 50.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  91% 5360/5899 [01:45<00:10, 50.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  91% 5380/5899 [01:45<00:10, 50.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  92% 5400/5899 [01:45<00:09, 51.03it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  92% 5420/5899 [01:45<00:09, 51.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  92% 5440/5899 [01:46<00:08, 51.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  93% 5460/5899 [01:46<00:08, 51.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  93% 5480/5899 [01:46<00:08, 51.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  93% 5500/5899 [01:46<00:07, 51.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  94% 5520/5899 [01:46<00:07, 51.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  94% 5540/5899 [01:47<00:06, 51.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  94% 5560/5899 [01:47<00:06, 51.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  95% 5580/5899 [01:47<00:06, 51.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  95% 5600/5899 [01:47<00:05, 52.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  95% 5620/5899 [01:47<00:05, 52.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  96% 5640/5899 [01:47<00:04, 52.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  96% 5660/5899 [01:48<00:04, 52.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  96% 5680/5899 [01:48<00:04, 52.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  97% 5700/5899 [01:48<00:03, 52.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  97% 5720/5899 [01:48<00:03, 52.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  97% 5740/5899 [01:48<00:03, 52.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  98% 5760/5899 [01:48<00:02, 52.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  98% 5780/5899 [01:49<00:02, 52.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  98% 5800/5899 [01:49<00:01, 53.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  99% 5820/5899 [01:49<00:01, 53.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  99% 5840/5899 [01:49<00:01, 53.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57:  99% 5860/5899 [01:49<00:00, 53.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57: 100% 5880/5899 [01:49<00:00, 53.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 57: 100% 5899/5899 [01:50<00:00, 53.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 58:  80% 4700/5899 [01:35<00:24, 49.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 58:  80% 4720/5899 [01:39<00:24, 47.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  80% 4740/5899 [01:40<00:24, 47.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  81% 4760/5899 [01:40<00:23, 47.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  81% 4780/5899 [01:40<00:23, 47.57it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  81% 4800/5899 [01:40<00:23, 47.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  82% 4820/5899 [01:40<00:22, 47.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  82% 4840/5899 [01:40<00:22, 47.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  82% 4860/5899 [01:41<00:21, 48.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  83% 4880/5899 [01:41<00:21, 48.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  83% 4900/5899 [01:41<00:20, 48.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  83% 4920/5899 [01:41<00:20, 48.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  84% 4940/5899 [01:41<00:19, 48.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  84% 4960/5899 [01:41<00:19, 48.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  84% 4980/5899 [01:42<00:18, 48.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  85% 5000/5899 [01:42<00:18, 48.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  85% 5020/5899 [01:42<00:17, 48.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  85% 5040/5899 [01:42<00:17, 49.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  86% 5060/5899 [01:42<00:17, 49.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  86% 5080/5899 [01:43<00:16, 49.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  86% 5100/5899 [01:43<00:16, 49.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  87% 5120/5899 [01:43<00:15, 49.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  87% 5140/5899 [01:43<00:15, 49.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  87% 5160/5899 [01:43<00:14, 49.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  88% 5180/5899 [01:43<00:14, 49.83it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  88% 5200/5899 [01:44<00:13, 49.94it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  88% 5220/5899 [01:44<00:13, 50.05it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  89% 5240/5899 [01:44<00:13, 50.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  89% 5260/5899 [01:44<00:12, 50.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  90% 5280/5899 [01:44<00:12, 50.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  90% 5300/5899 [01:44<00:11, 50.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  90% 5320/5899 [01:45<00:11, 50.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  91% 5340/5899 [01:45<00:11, 50.70it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  91% 5360/5899 [01:45<00:10, 50.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  91% 5380/5899 [01:45<00:10, 50.92it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  92% 5400/5899 [01:45<00:09, 51.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  92% 5420/5899 [01:46<00:09, 51.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  92% 5440/5899 [01:46<00:08, 51.24it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  93% 5460/5899 [01:46<00:08, 51.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  93% 5480/5899 [01:46<00:08, 51.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  93% 5500/5899 [01:46<00:07, 51.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  94% 5520/5899 [01:46<00:07, 51.67it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  94% 5540/5899 [01:46<00:06, 51.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  94% 5560/5899 [01:47<00:06, 51.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  95% 5580/5899 [01:47<00:06, 51.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  95% 5600/5899 [01:47<00:05, 52.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  95% 5620/5899 [01:47<00:05, 52.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  96% 5640/5899 [01:47<00:04, 52.30it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  96% 5660/5899 [01:48<00:04, 52.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  96% 5680/5899 [01:48<00:04, 52.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  97% 5700/5899 [01:48<00:03, 52.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  97% 5720/5899 [01:48<00:03, 52.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  97% 5740/5899 [01:48<00:03, 52.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  98% 5760/5899 [01:48<00:02, 52.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  98% 5780/5899 [01:49<00:02, 53.01it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  98% 5800/5899 [01:49<00:01, 53.11it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  99% 5820/5899 [01:49<00:01, 53.21it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  99% 5840/5899 [01:49<00:01, 53.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58:  99% 5860/5899 [01:49<00:00, 53.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58: 100% 5880/5899 [01:49<00:00, 53.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 58: 100% 5899/5899 [01:50<00:00, 53.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 59:  80% 4700/5899 [01:36<00:24, 48.84it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 59:  80% 4720/5899 [01:40<00:25, 46.92it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  80% 4740/5899 [01:40<00:24, 47.03it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  81% 4760/5899 [01:40<00:24, 47.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  81% 4780/5899 [01:41<00:23, 47.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  81% 4800/5899 [01:41<00:23, 47.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  82% 4820/5899 [01:41<00:22, 47.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  82% 4840/5899 [01:41<00:22, 47.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  82% 4860/5899 [01:41<00:21, 47.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  83% 4880/5899 [01:42<00:21, 47.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  83% 4900/5899 [01:42<00:20, 47.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  83% 4920/5899 [01:42<00:20, 48.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  84% 4940/5899 [01:42<00:19, 48.15it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  84% 4960/5899 [01:42<00:19, 48.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  84% 4980/5899 [01:42<00:18, 48.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  85% 5000/5899 [01:43<00:18, 48.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  85% 5020/5899 [01:43<00:18, 48.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  85% 5040/5899 [01:43<00:17, 48.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  86% 5060/5899 [01:43<00:17, 48.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  86% 5080/5899 [01:43<00:16, 48.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  86% 5100/5899 [01:44<00:16, 49.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  87% 5120/5899 [01:44<00:15, 49.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  87% 5140/5899 [01:44<00:15, 49.22it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  87% 5160/5899 [01:44<00:14, 49.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  88% 5180/5899 [01:44<00:14, 49.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  88% 5200/5899 [01:44<00:14, 49.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  88% 5220/5899 [01:45<00:13, 49.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  89% 5240/5899 [01:45<00:13, 49.75it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  89% 5260/5899 [01:45<00:12, 49.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  90% 5280/5899 [01:45<00:12, 49.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  90% 5300/5899 [01:45<00:11, 50.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  90% 5320/5899 [01:46<00:11, 50.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  91% 5340/5899 [01:46<00:11, 50.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  91% 5360/5899 [01:46<00:10, 50.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  91% 5380/5899 [01:46<00:10, 50.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  92% 5400/5899 [01:46<00:09, 50.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  92% 5420/5899 [01:46<00:09, 50.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  92% 5440/5899 [01:47<00:09, 50.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  93% 5460/5899 [01:47<00:08, 50.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  93% 5480/5899 [01:47<00:08, 50.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  93% 5500/5899 [01:47<00:07, 51.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  94% 5520/5899 [01:47<00:07, 51.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  94% 5540/5899 [01:48<00:07, 51.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  94% 5560/5899 [01:48<00:06, 51.38it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  95% 5580/5899 [01:48<00:06, 51.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  95% 5600/5899 [01:48<00:05, 51.59it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  95% 5620/5899 [01:48<00:05, 51.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  96% 5640/5899 [01:48<00:05, 51.79it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  96% 5660/5899 [01:49<00:04, 51.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  96% 5680/5899 [01:49<00:04, 51.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  97% 5700/5899 [01:49<00:03, 52.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  97% 5720/5899 [01:49<00:03, 52.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  97% 5740/5899 [01:49<00:03, 52.28it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  98% 5760/5899 [01:49<00:02, 52.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  98% 5780/5899 [01:50<00:02, 52.46it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  98% 5800/5899 [01:50<00:01, 52.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  99% 5820/5899 [01:50<00:01, 52.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  99% 5840/5899 [01:50<00:01, 52.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59:  99% 5860/5899 [01:50<00:00, 52.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59: 100% 5880/5899 [01:51<00:00, 52.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 59: 100% 5899/5899 [01:51<00:00, 53.00it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 60:  80% 4700/5899 [01:37<00:24, 48.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 60:  80% 4720/5899 [01:41<00:25, 46.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  80% 4740/5899 [01:41<00:24, 46.60it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  81% 4760/5899 [01:41<00:24, 46.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  81% 4780/5899 [01:42<00:23, 46.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  81% 4800/5899 [01:42<00:23, 46.92it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  82% 4820/5899 [01:42<00:22, 47.03it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  82% 4840/5899 [01:42<00:22, 47.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  82% 4860/5899 [01:42<00:21, 47.26it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  83% 4880/5899 [01:43<00:21, 47.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  83% 4900/5899 [01:43<00:21, 47.47it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  83% 4920/5899 [01:43<00:20, 47.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  84% 4940/5899 [01:43<00:20, 47.69it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  84% 4960/5899 [01:43<00:19, 47.80it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  84% 4980/5899 [01:43<00:19, 47.91it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  85% 5000/5899 [01:44<00:18, 48.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  85% 5020/5899 [01:44<00:18, 48.13it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  85% 5040/5899 [01:44<00:17, 48.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  86% 5060/5899 [01:44<00:17, 48.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  86% 5080/5899 [01:44<00:16, 48.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  86% 5100/5899 [01:45<00:16, 48.55it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  87% 5120/5899 [01:45<00:16, 48.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  87% 5140/5899 [01:45<00:15, 48.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  87% 5160/5899 [01:45<00:15, 48.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  88% 5180/5899 [01:45<00:14, 48.98it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  88% 5200/5899 [01:45<00:14, 49.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  88% 5220/5899 [01:46<00:13, 49.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  89% 5240/5899 [01:46<00:13, 49.31it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  89% 5260/5899 [01:46<00:12, 49.42it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  90% 5280/5899 [01:46<00:12, 49.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  90% 5300/5899 [01:46<00:12, 49.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  90% 5320/5899 [01:46<00:11, 49.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  91% 5340/5899 [01:47<00:11, 49.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  91% 5360/5899 [01:47<00:10, 49.95it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  91% 5380/5899 [01:47<00:10, 50.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  92% 5400/5899 [01:47<00:09, 50.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  92% 5420/5899 [01:47<00:09, 50.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  92% 5440/5899 [01:47<00:09, 50.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  93% 5460/5899 [01:48<00:08, 50.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  93% 5480/5899 [01:48<00:08, 50.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  93% 5500/5899 [01:48<00:07, 50.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  94% 5520/5899 [01:48<00:07, 50.79it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  94% 5540/5899 [01:48<00:07, 50.89it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  94% 5560/5899 [01:49<00:06, 50.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  95% 5580/5899 [01:49<00:06, 51.09it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  95% 5600/5899 [01:49<00:05, 51.20it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  95% 5620/5899 [01:49<00:05, 51.30it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  96% 5640/5899 [01:49<00:05, 51.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  96% 5660/5899 [01:49<00:04, 51.51it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  96% 5680/5899 [01:50<00:04, 51.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  97% 5700/5899 [01:50<00:03, 51.71it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  97% 5720/5899 [01:50<00:03, 51.81it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  97% 5740/5899 [01:50<00:03, 51.92it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  98% 5760/5899 [01:50<00:02, 52.02it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  98% 5780/5899 [01:50<00:02, 52.12it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  98% 5800/5899 [01:51<00:01, 52.23it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  99% 5820/5899 [01:51<00:01, 52.33it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  99% 5840/5899 [01:51<00:01, 52.43it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60:  99% 5860/5899 [01:51<00:00, 52.53it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60: 100% 5880/5899 [01:51<00:00, 52.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 60: 100% 5899/5899 [01:51<00:00, 52.73it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Epoch 61:  80% 4700/5899 [01:39<00:25, 47.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1180 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 61:  80% 4720/5899 [01:43<00:25, 45.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  80% 4740/5899 [01:43<00:25, 45.64it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  81% 4760/5899 [01:44<00:24, 45.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  81% 4780/5899 [01:44<00:24, 45.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  81% 4800/5899 [01:44<00:23, 45.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  82% 4820/5899 [01:44<00:23, 46.07it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  82% 4840/5899 [01:44<00:22, 46.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  82% 4860/5899 [01:44<00:22, 46.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  83% 4880/5899 [01:45<00:21, 46.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  83% 4900/5899 [01:45<00:21, 46.52it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  83% 4920/5899 [01:45<00:20, 46.63it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  84% 4940/5899 [01:45<00:20, 46.74it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  84% 4960/5899 [01:45<00:20, 46.85it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  84% 4980/5899 [01:46<00:19, 46.97it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  85% 5000/5899 [01:46<00:19, 47.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  85% 5020/5899 [01:46<00:18, 47.19it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  85% 5040/5899 [01:46<00:18, 47.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  86% 5060/5899 [01:46<00:17, 47.40it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  86% 5080/5899 [01:46<00:17, 47.50it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  86% 5100/5899 [01:47<00:16, 47.61it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  87% 5120/5899 [01:47<00:16, 47.72it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  87% 5140/5899 [01:47<00:15, 47.82it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  87% 5160/5899 [01:47<00:15, 47.93it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  88% 5180/5899 [01:47<00:14, 48.04it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  88% 5200/5899 [01:48<00:14, 48.14it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  88% 5220/5899 [01:48<00:14, 48.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  89% 5240/5899 [01:48<00:13, 48.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  89% 5260/5899 [01:48<00:13, 48.45it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  90% 5280/5899 [01:48<00:12, 48.56it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  90% 5300/5899 [01:48<00:12, 48.66it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  90% 5320/5899 [01:49<00:11, 48.76it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  91% 5340/5899 [01:49<00:11, 48.86it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  91% 5360/5899 [01:49<00:11, 48.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  91% 5380/5899 [01:49<00:10, 49.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  92% 5400/5899 [01:49<00:10, 49.17it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  92% 5420/5899 [01:50<00:09, 49.27it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  92% 5440/5899 [01:50<00:09, 49.37it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  93% 5460/5899 [01:50<00:08, 49.48it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  93% 5480/5899 [01:50<00:08, 49.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  93% 5500/5899 [01:50<00:08, 49.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  94% 5520/5899 [01:50<00:07, 49.78it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  94% 5540/5899 [01:51<00:07, 49.88it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  94% 5560/5899 [01:51<00:06, 49.99it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  95% 5580/5899 [01:51<00:06, 50.08it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  95% 5600/5899 [01:51<00:05, 50.18it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  95% 5620/5899 [01:51<00:05, 50.29it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  96% 5640/5899 [01:51<00:05, 50.39it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  96% 5660/5899 [01:52<00:04, 50.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  96% 5680/5899 [01:52<00:04, 50.58it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  97% 5700/5899 [01:52<00:03, 50.68it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  97% 5720/5899 [01:52<00:03, 50.77it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  97% 5740/5899 [01:52<00:03, 50.87it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  98% 5760/5899 [01:53<00:02, 50.96it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  98% 5780/5899 [01:53<00:02, 51.06it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  98% 5800/5899 [01:53<00:01, 51.16it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  99% 5820/5899 [01:53<00:01, 51.25it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  99% 5840/5899 [01:53<00:01, 51.35it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61:  99% 5860/5899 [01:53<00:00, 51.44it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61: 100% 5880/5899 [01:54<00:00, 51.54it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "Epoch 61: 100% 5899/5899 [01:54<00:00, 51.62it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "                                                                 \u001b[ADEBUG keys: ['train_loss', 'train_loss_step', 'val_loss', 'avg_val_loss', 'train_loss_epoch', 'train_contrast', 'train_penalty']\n",
            "Monitored metric avg_val_loss did not improve in the last 10 records. Best score: 4.337. Signaling Trainer to stop.\n",
            "Epoch 61: 100% 5899/5899 [01:54<00:00, 51.49it/s, loss=4.39, v_num=6, train_loss_step=4.390, val_loss=4.340, avg_val_loss=4.340, train_loss_epoch=4.390]\n",
            "trainning done\n",
            "\n",
            "-------------------------------Feature Extractin-------------------------------\n",
            "#--------------------------------------------Feature Extracting(pairs)------------------------------------------------------\n",
            "num of batches for all cells (not cell pairs): 144\n",
            "/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1979) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "  self.pid = os.fork()\n",
            "Shape of the feature representation generated by the base encoder: (37207, 64)\n",
            "end time: 1768840962.8641858\n",
            "Execution time: 1.91 hours\n"
          ]
        }
      ],
      "source": [
        "!python LCL_Main_Semi.py --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_train.h5ad\" \\\n",
        "                                              --testFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_test.h5ad\" \\\n",
        "                                              --batch_size 260 \\\n",
        "                                              --size_factor 0.45 \\\n",
        "                                              --unlabeled_per_batch 15 \\\n",
        "                                              --lambda_penalty 0.01 \\\n",
        "                                              --temperature 0.5 \\\n",
        "                                              --max_epoch 220\\\n",
        "                                              --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty\" \\\n",
        "                                              --train_test 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzXLxalfnumD"
      },
      "source": [
        "**5. extract the features**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python feature_extraction.py --help"
      ],
      "metadata": {
        "id": "d8HHCBIOWxgQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RL94aUsczXKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python feature_extraction.py \\\n",
        "  --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_train.h5ad\" \\\n",
        "  --batch_size 260 \\\n",
        "  --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty\" \\\n",
        "  --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\" \\\n",
        "  --out_file_name \"train_base_embed.npy\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRpHFU3vdOs_",
        "outputId": "0a8be26e-703d-4932-f4de-6af3121ea99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected input_dim = 2000, n_obs = 37207\n",
            "=== STATE_DICT LOAD REPORT (bare BaseEncoder_ProjHead_MLP) ===\n",
            "missing: 0\n",
            "unexpected: 0\n",
            "missing examples: []\n",
            "unexpected examples: []\n",
            "=============================================================\n",
            "Extracted base features shape: (37207, 64)\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/train_base_embed.npy\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/cell_ids_bs260.txt\n",
            "Done in 0.10 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab4a15a-0252-4af3-dbe7-831dacc193b0",
        "id": "s-Zog6SpsEJE"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected input_dim = 2000, n_obs = 3886\n",
            "=== STATE_DICT LOAD REPORT (bare BaseEncoder_ProjHead_MLP) ===\n",
            "missing: 0\n",
            "unexpected: 0\n",
            "missing examples: []\n",
            "unexpected examples: []\n",
            "=============================================================\n",
            "Extracted base features shape: (3886, 64)\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/test_base_embed.npy\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/cell_ids_bs260.txt\n",
            "Done in 0.01 minutes\n"
          ]
        }
      ],
      "source": [
        "!python feature_extraction.py \\\n",
        "  --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_test.h5ad\" \\\n",
        "  --batch_size 260 \\\n",
        "  --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty\" \\\n",
        "  --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\" \\\n",
        "  --out_file_name \"test_base_embed.npy\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python project_head_extraction.py \\\n",
        "  --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_train.h5ad\" \\\n",
        "  --batch_size 260 \\\n",
        "  --out_file_name 'train_proj_embed.npy' \\\n",
        "  --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty\" \\\n",
        "  --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzINhEkYmO__",
        "outputId": "3aad7ac5-781b-4403-ad94-2ac453948864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------- PROJECTION EMBEDDING EXTRACTION ---------------------\n",
            "inputFilePath: /content/drive/MyDrive/Colab Notebooks/data/Larry_train.h5ad\n",
            "resume_from_checkpoint: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\n",
            "inferred input_dim: 2000\n",
            "hidden_dims: [1024, 256, 64]\n",
            "embedding_size: 32\n",
            "batch_size (inference): 260\n",
            "=== STATE_DICT LOAD REPORT ===\n",
            "missing: 0\n",
            "unexpected: 0\n",
            "missing examples: []\n",
            "unexpected examples: []\n",
            "================================\n",
            "[OK] Loaded checkpoint: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/train_proj_embed.npy  (shape=(37207, 32))\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/cell_ids.txt  (n=37207)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python project_head_extraction.py \\\n",
        "  --inputFilePath \"/content/drive/MyDrive/Colab Notebooks/data/Larry_test.h5ad\" \\\n",
        "  --batch_size 260 \\\n",
        "  --out_file_name 'test_proj_embed.npy' \\\n",
        "  --output_dir \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty\" \\\n",
        "  --resume_from_checkpoint \"/content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_ablvvmcKY",
        "outputId": "6f7ce2bb-3bef-4c28-9a82-f6d39d070cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------- PROJECTION EMBEDDING EXTRACTION ---------------------\n",
            "inputFilePath: /content/drive/MyDrive/Colab Notebooks/data/Larry_test.h5ad\n",
            "resume_from_checkpoint: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\n",
            "inferred input_dim: 2000\n",
            "hidden_dims: [1024, 256, 64]\n",
            "embedding_size: 32\n",
            "batch_size (inference): 260\n",
            "=== STATE_DICT LOAD REPORT ===\n",
            "missing: 0\n",
            "unexpected: 0\n",
            "missing examples: []\n",
            "unexpected examples: []\n",
            "================================\n",
            "[OK] Loaded checkpoint: /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/saved_models/scContrastiveLearn_last.ckpt\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/test_proj_embed.npy  (shape=(3886, 32))\n",
            "[SAVED] /content/drive/MyDrive/Colab Notebooks/scCL/output/LCL_2025/Larry_top200/Larry_full_lambda0.01_unlab15_bs270_testAsPenalty/cell_ids.txt  (n=3886)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOzHpGtv6qWa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}